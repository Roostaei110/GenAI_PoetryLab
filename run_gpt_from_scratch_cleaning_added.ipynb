{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ax4CMTRg3EBB","outputId":"259961c0-8cf0-473b-9a83-d14c96f9fc27","executionInfo":{"status":"ok","timestamp":1723614014628,"user_tz":-210,"elapsed":53949,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"VTP7QzghXe6z","executionInfo":{"status":"ok","timestamp":1723617431990,"user_tz":-210,"elapsed":530,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[],"source":["# import libraries\n","import os\n","import math\n","import time\n","import inspect\n","from dataclasses import dataclass\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from transformers import AutoTokenizer\n","import hazm\n","%cd /content/drive/MyDrive/gpt/cleaning  # a path for cleaning functions\n","from main_clean import clean\n","import check"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"3reIGFswbaJZ","executionInfo":{"status":"ok","timestamp":1723617434297,"user_tz":-210,"elapsed":445,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[],"source":["# Defining tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n","special_tokens = {\"cls_token\": \"[CLS]\", \"additional_special_tokens\":[\"[BOM]\",\"[BOB]\",\"[SEP]\",\"<unk>\"],\"eos_token\":\"[EOS]\"}\n","tokenizer.add_special_tokens(special_tokens)\n","tokenizer.cls_token = \"[CLS]\"\n","tokenizer.bob_token =\"[BOB]\"\n","tokenizer.bom_token = \"[BOM]\"\n","tokenizer.eos_token =\"[EOS]\"\n","enc = tokenizer"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1d_K_bTXe60","outputId":"07470146-f05f-4dd9-e94d-3e37d845d030","executionInfo":{"status":"ok","timestamp":1723617448787,"user_tz":-210,"elapsed":420,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/gpt/cleaning\n"]}],"source":["# Special tokens will be added, input txt files will be aggregated to one and data will be tokenized...\n","data_root = \"/content/drive/MyDrive/gpt\" # a path for input, log and output files\n","poems_file_name = \"aggregated_data_file.txt\" # aggregated data\n","cls_token = '[CLS]'\n","bob_token = '[BOB]'\n","bom_token = '[BOM]'\n","eos_token = '[EOS]'\n","\n","def aggregate_data_files_and_add_special_tokens():\n","    _data_root = data_root + \"/input/\"\n","    all_files = [os.path.join(_data_root + f) for f in os.listdir(_data_root ) if f.endswith('.txt')]\n","    aggregated_text = ''\n","\n","    #cleanning\n","        # Clean all files and save them to the specified path\n","    for file in all_files:\n","        with open(file, 'r', encoding='utf-8') as f:\n","            text = f.readlines()  # Read the whole file\n","            cleaned_text = clean(text)  # Clean the text using the defined clean function\n","            print(cleaned_text)\n","        if isinstance(cleaned_text, list):\n","            cleaned_text = '\\n'.join(cleaned_text)\n","\n","        # Save the cleaned text to a new file in the cleaned_file_path\n","        cleaned_file_path = os.path.join('/content/drive/MyDrive/gpt/cleaninput', os.path.basename(file))\n","        with open(cleaned_file_path, 'w', encoding='utf-8') as cleaned_file:\n","            cleaned_file.write(cleaned_text)\n","\n","\n","    _clean_file_path = data_root + \"/cleaninput/\"\n","    all_clean_files = [os.path.join(_clean_file_path + f) for f in os.listdir(_clean_file_path ) if f.endswith('.txt')]\n","    aggregated_text = ''\n","# /**************************************\n","    for file in all_clean_files:\n","\n","        with open(file, 'r', encoding='utf-8') as f:\n","            text = f.readlines()[2:] # TODO\n","            is_beit = True\n","            poem_str = f'\\n{cls_token}'\n","\n","            for line in text:\n","                if line.strip() == '':\n","                    continue\n","                poem_str += f\"{bob_token + bom_token if is_beit else bom_token}{line}\"\n","                is_beit = not is_beit\n","        poem_str += f'{eos_token}'\n","\n","        aggregated_text += poem_str.strip()\n","\n","    output_file_path = data_root + \"/input/\" + poems_file_name # Set the path for the output file\n","    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n","        output_file.write(aggregated_text)\n","\n","    all_tokens = tokenizer.encode(aggregated_text, add_special_tokens=True)\n","    tokens_np = np.array(all_tokens, dtype=np.uint16)\n","    _data_root = data_root + '/input/'\n","    poems_file_path = os.path.join(_data_root + 'poems.npy')\n","    np.save(poems_file_path, tokens_np)\n","\n","    n = int(0.9 * len(all_tokens))\n","    train_data = tokens_np[:n]\n","    val_data = tokens_np[n:]\n","\n","\n","    train_file = os.path.join(_data_root, 'train.npy')\n","\n","    np.save(train_file, train_data)\n","    val_file = os.path.join(_data_root, 'val.npy')\n","    np.save(val_file, val_data)\n","\n","    return"]},{"cell_type":"markdown","metadata":{"id":"XMpfbSKubaJa"},"source":["![Attention is all you need. High quality image :)](Capture.JPG)"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"3UKd5mq33VFb","executionInfo":{"status":"ok","timestamp":1723617466214,"user_tz":-210,"elapsed":990,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[],"source":["# Defining Transformer model. This cell implements what the above image explains. source: <Attention is all you need>\n","class CausalSelfAttention(nn.Module): # Implements self-attention mechanism\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1 # standard deviation grows inside the residual stream. This line controls it.\n","        # regularization\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","\n","    def forward(self, x):\n","        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n","        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n","        qkv = self.c_attn(x)\n","        q, k, v = qkv.split(self.n_embd, dim=2)\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n","        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n","        # output projection\n","        y = self.c_proj(y)\n","        return y\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n","        self.gelu    = nn.GELU(approximate='tanh')\n","        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        return x\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embd)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embd)\n","        self.mlp = MLP(config)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x)) # self.ln_1(x) noramlization --> attention think about all tokens together. aggregated function\n","        x = x + self.mlp(self.ln_2(x)) # self.ln_2(x) --> multi linear perc(?) # think individually about tokens\n","        return x\n","\n","@dataclass\n","class GPTConfig:\n","    block_size: int = 1024 # max sequence length\n","    vocab_size: int =  25005 # tokenizer.vocab_size number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n","    n_layer: int = 6 # number of layers\n","    n_head: int = 6 # number of heads\n","    n_embd: int = 384 # embedding dimension\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embd), # token encoding the first box of the picture\n","            wpe = nn.Embedding(config.block_size, config.n_embd), # position encoding\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # total block in the picture of attention is all you need\n","            ln_f = nn.LayerNorm(config.n_embd), # linear part of picture\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # weight sharing scheme\n","        self.transformer.wte.weight = self.lm_head.weight\n","\n","        # init params\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):# if a layer is linear use normal distribution std is different\n","            std = 0.02\n","            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n","                std *= (2 * self.config.n_layer) ** -0.5\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        # idx is of shape (B, T)\n","        B, T = idx.size()\n","        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n","        # forward the token and posisition embeddings\n","        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n","        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n","        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n","        x = tok_emb + pos_emb\n","        # forward the blocks of the transformer\n","        for block in self.transformer.h:\n","            x = block(x)\n","        # forward the final layernorm and the classifier\n","        x = self.transformer.ln_f(x)\n","        logits = self.lm_head(x) # (B, T, vocab_size)\n","        loss = None\n","        if targets is not None: # if we have target in data we calculate loss as follows\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # flatten data by targets.view(-1)\n","        return logits, loss\n","\n","    @classmethod\n","    def from_pretrained(cls, model_type): # loading wieghts. this is a constructor or class method\n","        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n","        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n","        from transformers import GPT2LMHeadModel\n","        print(\"loading weights from pretrained gpt: %s\" % model_type)\n","\n","        # n_layer, n_head and n_embd are determined from model_type\n","        config_args = {\n","            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n","            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n","            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n","            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n","        }[model_type]\n","        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n","        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n","        # create a from-scratch initialized minGPT model\n","        config = GPTConfig(**config_args)\n","        model = GPT(config)\n","        sd = model.state_dict()\n","        sd_keys = sd.keys()\n","        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n","\n","        # init a huggingface/transformers model\n","        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n","        sd_hf = model_hf.state_dict()\n","\n","        # copy while ensuring all of the parameters are aligned and match in names and shapes\n","        sd_keys_hf = sd_hf.keys()\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n","        # this means that we have to transpose these weights when we import them\n","        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n","        for k in sd_keys_hf:\n","            if any(k.endswith(w) for w in transposed):\n","                # special treatment for the Conv1D weights we need to transpose\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t())\n","            else:\n","                # vanilla copy over the other parameters\n","                assert sd_hf[k].shape == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k])\n","\n","        return model\n","\n","    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n","        # start with all of the candidate parameters (that require grad)\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n","        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n","        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n","        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n","        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n","        optim_groups = [\n","            {'params': decay_params, 'weight_decay': weight_decay},\n","            {'params': nodecay_params, 'weight_decay': 0.0}\n","        ]\n","        num_decay_params = sum(p.numel() for p in decay_params)\n","        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n","\n","        if master_process:\n","            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n","            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n","        # Create AdamW optimizer and use the fused version if it is available\n","        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n","        use_fused = fused_available and device_type == \"cuda\"\n","        # use_fused = fused_available and device_type == \"mps\"\n","        if master_process:\n","            print(f\"using fused AdamW: {use_fused}\")\n","        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n","        return optimizer"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"FBF-J9ke3cVc","executionInfo":{"status":"ok","timestamp":1723617470664,"user_tz":-210,"elapsed":462,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[],"source":["\n","def load_tokens(filename):\n","    npt = np.load(filename)\n","    npt = npt.astype(np.int32) # added after video\n","    ptt = torch.tensor(npt, dtype=torch.long)\n","    return ptt\n","\n","class DataLoaderLite:\n","    def __init__(self, B, T, process_rank, num_processes, split):\n","        self.B = B\n","        self.T = T\n","        self.process_rank = process_rank\n","        self.num_processes = num_processes\n","        assert split in {'train', 'val'}\n","        data_root = \"/content/drive/MyDrive/gpt/input/\"\n","        # get the shard filenames\n","        shards = os.listdir(data_root)\n","        shards = [s for s in shards if split in s]\n","        shards = sorted(shards)\n","        shards = [os.path.join(data_root,  s) for s in shards]\n","        print(shards)\n","        self.shards = shards\n","        assert len(shards) > 0, f\"no shards found for split {split}\"\n","\n","        if master_process:\n","            print(f\"found {len(shards)} shards for split {split}\")\n","        self.reset()\n","\n","    def reset(self):\n","        # state, init at shard zero\n","        self.current_shard = 0\n","        self.tokens = load_tokens(self.shards[self.current_shard])\n","        self.current_position = self.B * self.T * self.process_rank\n","\n","    def next_batch(self):\n","        B, T = self.B, self.T\n","        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n","        x = (buf[:-1]).view(B, T) # inputs\n","        y = (buf[1:]).view(B, T) # targets\n","        # advance the position in the tensor\n","        self.current_position += B * T * self.num_processes\n","        # if loading the next batch would be out of bounds, advance to next shard\n","        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n","            self.current_shard = (self.current_shard + 1) % len(self.shards)\n","            self.tokens = load_tokens(self.shards[self.current_shard])\n","            self.current_position = B * T * self.process_rank\n","        return x, y\n","\n","# -----------------------------------------------------------------------------\n","# helper function for HellaSwag eval\n","# takes tokens, mask, and logits, returns the index of the completion with the lowest loss\n","\n","def get_most_likely_row(tokens, mask, logits):\n","    # evaluate the autoregressive loss at all positions\n","    shift_logits = (logits[..., :-1, :]).contiguous()\n","    shift_tokens = (tokens[..., 1:]).contiguous()\n","    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n","    flat_shift_tokens = shift_tokens.view(-1)\n","    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n","    shift_losses = shift_losses.view(tokens.size(0), -1)\n","    # now get the average loss just for the completion region (where mask == 1), in each row\n","    shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n","    masked_shift_losses = shift_losses * shift_mask\n","    # sum and divide by the number of 1s in the mask\n","    sum_loss = masked_shift_losses.sum(dim=1)\n","    avg_loss = sum_loss / shift_mask.sum(dim=1)\n","    # now we have a loss for each of the 4 completions\n","    # the one with the lowest loss should be the most likely\n","    pred_norm = avg_loss.argmin().item()\n","    return pred_norm"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"Hziv-34h2oyU","executionInfo":{"status":"ok","timestamp":1723617472817,"user_tz":-210,"elapsed":456,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[],"source":["# This cell generates poem and saves the trained model. Generated poems will be saved into a file.\n","def generate_poem(model, device, device_type, ddp_rank, phrase, num_return_sequences):\n","    result = 'result.txt'\n","    model.eval()\n","    max_length = 32\n","    tokens = enc.encode(phrase)\n","    tokens = torch.tensor(tokens, dtype=torch.long)\n","    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n","    xgen = tokens.to(device)\n","    sample_rng = torch.Generator(device=device)\n","    sample_rng.manual_seed(42 + ddp_rank)\n","\n","    generated_poems = []  # List to store generated poems\n","\n","    while xgen.size(1) < max_length:\n","        # forward the model to get the logits\n","        with torch.no_grad():\n","            with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n","                logits, loss = model(xgen) # (B, T, vocab_size)\n","            # take the logits at the last position\n","            logits = logits[:, -1, :] # (B, vocab_size)\n","            # get the probabilities\n","            probs = F.softmax(logits, dim=-1)\n","            # do top-k sampling of 50 (huggingface pipeline default)\n","            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # return top k high probability\n","            # select a token from the top-k probabilities\n","            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n","            # gather the corresponding indices\n","            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n","            # append to the sequence\n","            xgen = torch.cat((xgen, xcol), dim=1)\n","\n","    # collect and save the generated text\n","    for i in range(num_return_sequences):\n","        tokens = xgen[i, :max_length].tolist()\n","        decoded = enc.decode(tokens)\n","        print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n","        generated_poems.append(decoded)\n","\n","    # Save the model state (optional)\n","    torch.save(model.state_dict(), data_root + '/saved_model.pth')\n","\n","    # Save the generated poems to a file\n","    output_file = data_root + \"/\" + result\n","    with open(output_file, \"a\") as out_f:\n","        for i, poem in enumerate(generated_poems):\n","            out_f.write(f\"sample {i}: {poem}\\n\")\n","\n","    return generated_poems  # Return the list of generated poems\n"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1-pDqAe1szPoHxItzXpvhaxIUFronr0e1"},"id":"Mt0h7QuU3kKy","outputId":"71ccd13a-efb1-4f92-9a84-f894b4100094","executionInfo":{"status":"ok","timestamp":1723617571224,"user_tz":-210,"elapsed":96815,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Loading data, split data for train and validation, Training model\n","\n","from torch.distributed import init_process_group, destroy_process_group\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","import torch.distributed as dist\n","\n","# set up DDP (distributed data parallel).\n","# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n","ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n","print(ddp, \"DDP\")\n","if ddp:\n","    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n","    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n","    init_process_group(backend='nccl')\n","    ddp_rank = int(os.environ['RANK'])\n","    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n","    ddp_world_size = int(os.environ['WORLD_SIZE'])\n","    device = f'cuda:{ddp_local_rank}'\n","    torch.cuda.set_device(device)\n","    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n","else:\n","    # vanilla, non-DDP run\n","    ddp_rank = 0\n","    ddp_local_rank = 0\n","    ddp_world_size = 1\n","    master_process = True\n","    # attempt to autodetect device\n","    device = \"cpu\"\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","        device = \"mps\"\n","    print(f\"using device: {device}\")\n","\n","# added after video, pytorch can be serious about it's device vs. device_type distinction\n","device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(1337)\n","\n","# enc = tiktoken.get_encoding(\"gpt2\")\n","\n","total_batch_size = 8192 #524288 # 2**19, ~0.5M, in number of tokens\n","B = 8 # micro batch size\n","T = 1024 # sequence length\n","assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n","grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n","if master_process:\n","    print(f\"total desired batch size: {total_batch_size}\")\n","    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n","\n","\n","import random\n","\n","# all_files = [os.path.join(data_root, f) for f in os.listdir(data_root) if f.endswith('.txt')]\n","# random.shuffle(all_files)  # Shuffle to ensure random split\n","\n","# split_index = int(0.9 * len(all_files))  # 90% for training, 10% for validation\n","# train_files = all_files[:split_index]\n","# val_files = all_files[split_index:]\n","\n","# train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=train_files)\n","# val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=val_files)\n","# new method\n","\n","\n","aggregate_data_files_and_add_special_tokens()\n","\n","train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n","val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n","\n","\n","# # Shuffle lines to ensure randomness\n","# random.shuffle(all_lines)\n","\n","# # Split the lines into training and validation sets\n","# split_index = int(0.9 * len(all_lines))  # 90% for training, 10% for validation\n","# train_lines = all_lines[:split_index]\n","# val_lines = all_lines[split_index:]\n","# with open(os.path.join(data_root, 'train.txt'), 'w', encoding='utf-8') as train_file:\n","#     train_file.writelines(train_lines)\n","\n","# with open(os.path.join(data_root, 'val.txt'), 'w', encoding='utf-8') as val_file:\n","#     val_file.writelines(val_lines)\n","\n","# train_file_path = os.path.join(data_root, 'train.txt')\n","# val_file_path = os.path.join(data_root, 'val.txt')\n","# print(train_file_path)\n","# print(val_file_path)\n","# train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=[train_file_path])\n","# val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=[val_file_path])\n","\n","print(\"data is ready....\")\n","\n"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8IgdTXEiGX9H","outputId":"3180c740-377a-49ed-8fc3-f106c7285348","executionInfo":{"status":"error","timestamp":1723617919255,"user_tz":-210,"elapsed":331813,"user":{"displayName":"GenAI","userId":"05659125202882086658"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","step  1475 | loss: 1.783694 | lr 5.9763e-04 | norm: 0.9695 | dt: 96.97ms | tok/sec: 84477.80\n","start training the model\n","step 0 of training\n","step  1476 | loss: 1.819891 | lr 5.9763e-04 | norm: 0.8937 | dt: 96.94ms | tok/sec: 84504.60\n","start training the model\n","step 0 of training\n","step  1477 | loss: 1.799419 | lr 5.9762e-04 | norm: 0.6291 | dt: 96.20ms | tok/sec: 85155.72\n","start training the model\n","step 0 of training\n","step  1478 | loss: 1.814622 | lr 5.9761e-04 | norm: 0.8922 | dt: 96.97ms | tok/sec: 84483.40\n","start training the model\n","step 0 of training\n","step  1479 | loss: 1.756219 | lr 5.9761e-04 | norm: 0.5824 | dt: 96.72ms | tok/sec: 84701.25\n","start training the model\n","step 0 of training\n","step  1480 | loss: 1.779356 | lr 5.9760e-04 | norm: 0.5671 | dt: 96.02ms | tok/sec: 85311.55\n","start training the model\n","step 0 of training\n","step  1481 | loss: 1.808132 | lr 5.9760e-04 | norm: 1.1601 | dt: 96.55ms | tok/sec: 84849.75\n","start training the model\n","step 0 of training\n","step  1482 | loss: 2.686668 | lr 5.9759e-04 | norm: 1.1576 | dt: 96.66ms | tok/sec: 84752.64\n","start training the model\n","step 0 of training\n","step  1483 | loss: 6.010792 | lr 5.9758e-04 | norm: 2.3077 | dt: 96.99ms | tok/sec: 84462.43\n","start training the model\n","step 0 of training\n","step  1484 | loss: 5.478638 | lr 5.9758e-04 | norm: 1.5652 | dt: 95.92ms | tok/sec: 85405.06\n","start training the model\n","step 0 of training\n","step  1485 | loss: 5.503784 | lr 5.9757e-04 | norm: 0.8937 | dt: 96.82ms | tok/sec: 84613.02\n","start training the model\n","step 0 of training\n","step  1486 | loss: 5.425215 | lr 5.9757e-04 | norm: 0.8642 | dt: 96.60ms | tok/sec: 84805.57\n","start training the model\n","step 0 of training\n","step  1487 | loss: 5.380530 | lr 5.9756e-04 | norm: 0.8821 | dt: 96.61ms | tok/sec: 84794.68\n","start training the model\n","step 0 of training\n","step  1488 | loss: 5.392511 | lr 5.9755e-04 | norm: 0.8153 | dt: 96.92ms | tok/sec: 84527.47\n","start training the model\n","step 0 of training\n","step  1489 | loss: 5.442175 | lr 5.9755e-04 | norm: 0.7580 | dt: 96.46ms | tok/sec: 84924.41\n","start training the model\n","step 0 of training\n","step  1490 | loss: 5.336172 | lr 5.9754e-04 | norm: 0.7602 | dt: 96.96ms | tok/sec: 84490.68\n","start training the model\n","step 0 of training\n","step  1491 | loss: 5.487784 | lr 5.9753e-04 | norm: 0.7061 | dt: 96.62ms | tok/sec: 84781.92\n","start training the model\n","step 0 of training\n","step  1492 | loss: 5.514430 | lr 5.9753e-04 | norm: 0.7024 | dt: 96.70ms | tok/sec: 84711.69\n","start training the model\n","step 0 of training\n","step  1493 | loss: 5.473631 | lr 5.9752e-04 | norm: 0.6288 | dt: 96.47ms | tok/sec: 84915.39\n","start training the model\n","step 0 of training\n","step  1494 | loss: 5.431854 | lr 5.9752e-04 | norm: 0.5919 | dt: 96.59ms | tok/sec: 84816.24\n","start training the model\n","step 0 of training\n","step  1495 | loss: 5.517634 | lr 5.9751e-04 | norm: 0.8170 | dt: 96.90ms | tok/sec: 84541.40\n","start training the model\n","step 0 of training\n","step  1496 | loss: 5.572045 | lr 5.9750e-04 | norm: 0.7792 | dt: 96.71ms | tok/sec: 84709.39\n","start training the model\n","step 0 of training\n","step  1497 | loss: 5.500305 | lr 5.9750e-04 | norm: 0.7980 | dt: 96.93ms | tok/sec: 84514.78\n","start training the model\n","step 0 of training\n","step  1498 | loss: 5.446836 | lr 5.9749e-04 | norm: 0.6733 | dt: 95.38ms | tok/sec: 85888.61\n","start training the model\n","step 0 of training\n","step  1499 | loss: 5.470588 | lr 5.9748e-04 | norm: 0.6982 | dt: 95.71ms | tok/sec: 85592.92\n","validation loss: 5.8959\n","rank 0 sample 0: [CLS] دردم از یار است و درمان نیز هم[SEP] بی تو ای است[BOM] بر منست کز غم از فراق[BOM] زی تو[BOB][BOM] وز دل به هر\n","rank 0 sample 1: [CLS] دردم از یار است و درمان نیز هم[SEP] با این چشم[BOB][BOM] مگر که مرا در میان توست هر چه باشد مگر که تو[BOM] ای که ای که\n","rank 0 sample 2: [CLS] دردم از یار است و درمان نیز هم[SEP] در این دو روزی اگر گر دست است[BOB][BOM] ن در سر[BOB][BOM] چون شمع ور[BOM] که\n","rank 0 sample 3: [CLS] دردم از یار است و درمان نیز هم[SEP] را بود[BOB][BOM] که دری بود[BOB][BOM] در دست خدای رای این کهی[BOB][BOM] که\n","start training the model\n","step 0 of training\n","step  1500 | loss: 5.330729 | lr 5.9748e-04 | norm: 0.7975 | dt: 1348.77ms | tok/sec: 6073.67\n","start training the model\n","step 0 of training\n","step  1501 | loss: 5.778272 | lr 5.9747e-04 | norm: 1.7045 | dt: 96.19ms | tok/sec: 85161.00\n","start training the model\n","step 0 of training\n","step  1502 | loss: 5.468915 | lr 5.9747e-04 | norm: 0.8620 | dt: 96.14ms | tok/sec: 85210.00\n","start training the model\n","step 0 of training\n","step  1503 | loss: 5.288746 | lr 5.9746e-04 | norm: 0.7435 | dt: 96.63ms | tok/sec: 84777.74\n","start training the model\n","step 0 of training\n","step  1504 | loss: 5.419433 | lr 5.9745e-04 | norm: 0.7264 | dt: 96.67ms | tok/sec: 84740.73\n","start training the model\n","step 0 of training\n","step  1505 | loss: 5.437256 | lr 5.9745e-04 | norm: 0.8215 | dt: 97.20ms | tok/sec: 84282.18\n","start training the model\n","step 0 of training\n","step  1506 | loss: 5.336680 | lr 5.9744e-04 | norm: 0.7703 | dt: 96.65ms | tok/sec: 84759.54\n","start training the model\n","step 0 of training\n","step  1507 | loss: 5.313444 | lr 5.9743e-04 | norm: 0.7187 | dt: 97.11ms | tok/sec: 84359.16\n","start training the model\n","step 0 of training\n","step  1508 | loss: 5.305305 | lr 5.9743e-04 | norm: 0.7412 | dt: 96.78ms | tok/sec: 84647.20\n","start training the model\n","step 0 of training\n","step  1509 | loss: 5.426160 | lr 5.9742e-04 | norm: 0.6842 | dt: 97.15ms | tok/sec: 84323.14\n","start training the model\n","step 0 of training\n","step  1510 | loss: 5.520591 | lr 5.9741e-04 | norm: 0.8246 | dt: 95.73ms | tok/sec: 85573.52\n","start training the model\n","step 0 of training\n","step  1511 | loss: 5.638368 | lr 5.9741e-04 | norm: 0.9270 | dt: 100.95ms | tok/sec: 81148.51\n","start training the model\n","step 0 of training\n","step  1512 | loss: 5.564781 | lr 5.9740e-04 | norm: 1.0222 | dt: 95.41ms | tok/sec: 85865.21\n","start training the model\n","step 0 of training\n","step  1513 | loss: 5.611180 | lr 5.9740e-04 | norm: 0.9528 | dt: 95.84ms | tok/sec: 85479.22\n","start training the model\n","step 0 of training\n","step  1514 | loss: 5.648364 | lr 5.9739e-04 | norm: 1.1930 | dt: 97.17ms | tok/sec: 84309.89\n","start training the model\n","step 0 of training\n","step  1515 | loss: 5.342131 | lr 5.9738e-04 | norm: 1.1978 | dt: 97.68ms | tok/sec: 83865.40\n","start training the model\n","step 0 of training\n","step  1516 | loss: 5.384137 | lr 5.9738e-04 | norm: 1.3322 | dt: 97.79ms | tok/sec: 83769.51\n","start training the model\n","step 0 of training\n","step  1517 | loss: 5.246860 | lr 5.9737e-04 | norm: 1.2002 | dt: 97.01ms | tok/sec: 84445.20\n","start training the model\n","step 0 of training\n","step  1518 | loss: 5.425664 | lr 5.9736e-04 | norm: 0.7974 | dt: 97.95ms | tok/sec: 83637.78\n","start training the model\n","step 0 of training\n","step  1519 | loss: 5.260186 | lr 5.9736e-04 | norm: 1.0743 | dt: 97.55ms | tok/sec: 83979.16\n","start training the model\n","step 0 of training\n","step  1520 | loss: 5.366056 | lr 5.9735e-04 | norm: 0.8011 | dt: 97.96ms | tok/sec: 83625.16\n","start training the model\n","step 0 of training\n","step  1521 | loss: 5.351674 | lr 5.9734e-04 | norm: 0.9990 | dt: 96.84ms | tok/sec: 84594.06\n","start training the model\n","step 0 of training\n","step  1522 | loss: 5.215362 | lr 5.9734e-04 | norm: 0.8705 | dt: 97.07ms | tok/sec: 84390.24\n","start training the model\n","step 0 of training\n","step  1523 | loss: 5.249146 | lr 5.9733e-04 | norm: 0.8055 | dt: 96.85ms | tok/sec: 84584.48\n","start training the model\n","step 0 of training\n","step  1524 | loss: 5.264877 | lr 5.9732e-04 | norm: 0.8737 | dt: 96.69ms | tok/sec: 84720.46\n","start training the model\n","step 0 of training\n","step  1525 | loss: 5.292178 | lr 5.9732e-04 | norm: 0.7001 | dt: 97.29ms | tok/sec: 84201.42\n","start training the model\n","step 0 of training\n","step  1526 | loss: 5.287514 | lr 5.9731e-04 | norm: 0.7176 | dt: 96.44ms | tok/sec: 84946.67\n","start training the model\n","step 0 of training\n","step  1527 | loss: 5.459915 | lr 5.9730e-04 | norm: 0.8507 | dt: 96.84ms | tok/sec: 84592.81\n","start training the model\n","step 0 of training\n","step  1528 | loss: 5.395696 | lr 5.9730e-04 | norm: 0.6514 | dt: 96.66ms | tok/sec: 84747.42\n","start training the model\n","step 0 of training\n","step  1529 | loss: 5.551306 | lr 5.9729e-04 | norm: 0.6916 | dt: 96.49ms | tok/sec: 84897.14\n","start training the model\n","step 0 of training\n","step  1530 | loss: 5.427829 | lr 5.9729e-04 | norm: 0.7137 | dt: 96.98ms | tok/sec: 84475.10\n","start training the model\n","step 0 of training\n","step  1531 | loss: 5.577426 | lr 5.9728e-04 | norm: 0.9628 | dt: 96.39ms | tok/sec: 84984.49\n","start training the model\n","step 0 of training\n","step  1532 | loss: 5.617178 | lr 5.9727e-04 | norm: 0.7292 | dt: 95.63ms | tok/sec: 85660.35\n","start training the model\n","step 0 of training\n","step  1533 | loss: 5.438130 | lr 5.9727e-04 | norm: 1.0696 | dt: 96.45ms | tok/sec: 84933.02\n","start training the model\n","step 0 of training\n","step  1534 | loss: 5.398274 | lr 5.9726e-04 | norm: 0.8793 | dt: 97.44ms | tok/sec: 84075.74\n","start training the model\n","step 0 of training\n","step  1535 | loss: 5.426744 | lr 5.9725e-04 | norm: 0.9097 | dt: 96.91ms | tok/sec: 84534.12\n","start training the model\n","step 0 of training\n","step  1536 | loss: 5.472507 | lr 5.9725e-04 | norm: 0.8815 | dt: 96.89ms | tok/sec: 84550.14\n","start training the model\n","step 0 of training\n","step  1537 | loss: 5.546227 | lr 5.9724e-04 | norm: 0.9496 | dt: 97.24ms | tok/sec: 84242.30\n","start training the model\n","step 0 of training\n","step  1538 | loss: 5.513462 | lr 5.9723e-04 | norm: 0.8706 | dt: 96.90ms | tok/sec: 84541.61\n","start training the model\n","step 0 of training\n","step  1539 | loss: 5.451936 | lr 5.9723e-04 | norm: 0.8782 | dt: 98.71ms | tok/sec: 82993.33\n","start training the model\n","step 0 of training\n","step  1540 | loss: 5.312387 | lr 5.9722e-04 | norm: 0.9062 | dt: 97.55ms | tok/sec: 83979.57\n","start training the model\n","step 0 of training\n","step  1541 | loss: 5.322048 | lr 5.9721e-04 | norm: 0.9695 | dt: 97.70ms | tok/sec: 83848.00\n","start training the model\n","step 0 of training\n","step  1542 | loss: 5.269877 | lr 5.9721e-04 | norm: 1.0066 | dt: 97.01ms | tok/sec: 84446.24\n","start training the model\n","step 0 of training\n","step  1543 | loss: 5.204061 | lr 5.9720e-04 | norm: 0.8843 | dt: 97.41ms | tok/sec: 84099.00\n","start training the model\n","step 0 of training\n","step  1544 | loss: 6.083894 | lr 5.9719e-04 | norm: 1.4553 | dt: 96.88ms | tok/sec: 84553.88\n","start training the model\n","step 0 of training\n","step  1545 | loss: 6.136003 | lr 5.9719e-04 | norm: 1.1470 | dt: 96.32ms | tok/sec: 85046.12\n","start training the model\n","step 0 of training\n","step  1546 | loss: 6.013976 | lr 5.9718e-04 | norm: 0.9608 | dt: 96.68ms | tok/sec: 84733.00\n","start training the model\n","step 0 of training\n","step  1547 | loss: 5.952459 | lr 5.9717e-04 | norm: 0.8947 | dt: 96.28ms | tok/sec: 85081.08\n","start training the model\n","step 0 of training\n","step  1548 | loss: 5.887620 | lr 5.9717e-04 | norm: 0.8040 | dt: 97.30ms | tok/sec: 84197.09\n","start training the model\n","step 0 of training\n","step  1549 | loss: 5.990613 | lr 5.9716e-04 | norm: 0.7642 | dt: 96.44ms | tok/sec: 84948.35\n","start training the model\n","step 0 of training\n","step  1550 | loss: 5.829156 | lr 5.9715e-04 | norm: 0.8363 | dt: 96.46ms | tok/sec: 84926.93\n","start training the model\n","step 0 of training\n","step  1551 | loss: 5.814228 | lr 5.9715e-04 | norm: 0.8224 | dt: 96.33ms | tok/sec: 85044.23\n","start training the model\n","step 0 of training\n","step  1552 | loss: 5.793714 | lr 5.9714e-04 | norm: 0.8540 | dt: 97.13ms | tok/sec: 84337.62\n","start training the model\n","step 0 of training\n","step  1553 | loss: 5.885151 | lr 5.9713e-04 | norm: 0.9325 | dt: 95.96ms | tok/sec: 85365.17\n","start training the model\n","step 0 of training\n","step  1554 | loss: 5.821045 | lr 5.9713e-04 | norm: 0.8510 | dt: 96.27ms | tok/sec: 85093.93\n","start training the model\n","step 0 of training\n","step  1555 | loss: 5.867795 | lr 5.9712e-04 | norm: 0.7874 | dt: 97.02ms | tok/sec: 84437.31\n","start training the model\n","step 0 of training\n","step  1556 | loss: 5.820987 | lr 5.9711e-04 | norm: 0.8431 | dt: 96.33ms | tok/sec: 85040.02\n","start training the model\n","step 0 of training\n","step  1557 | loss: 5.796625 | lr 5.9711e-04 | norm: 0.9562 | dt: 96.87ms | tok/sec: 84569.28\n","start training the model\n","step 0 of training\n","step  1558 | loss: 5.667568 | lr 5.9710e-04 | norm: 0.9806 | dt: 96.46ms | tok/sec: 84925.04\n","start training the model\n","step 0 of training\n","step  1559 | loss: 5.760098 | lr 5.9709e-04 | norm: 0.8295 | dt: 96.22ms | tok/sec: 85139.90\n","start training the model\n","step 0 of training\n","step  1560 | loss: 5.743969 | lr 5.9709e-04 | norm: 0.9995 | dt: 96.64ms | tok/sec: 84766.44\n","start training the model\n","step 0 of training\n","step  1561 | loss: 5.604359 | lr 5.9708e-04 | norm: 1.1310 | dt: 96.75ms | tok/sec: 84673.69\n","start training the model\n","step 0 of training\n","step  1562 | loss: 6.046851 | lr 5.9707e-04 | norm: 2.0073 | dt: 96.77ms | tok/sec: 84657.84\n","start training the model\n","step 0 of training\n","step  1563 | loss: 5.875617 | lr 5.9707e-04 | norm: 2.8646 | dt: 96.52ms | tok/sec: 84876.58\n","start training the model\n","step 0 of training\n","step  1564 | loss: 5.172402 | lr 5.9706e-04 | norm: 2.8747 | dt: 96.85ms | tok/sec: 84584.90\n","start training the model\n","step 0 of training\n","step  1565 | loss: 5.507024 | lr 5.9705e-04 | norm: 2.5058 | dt: 96.35ms | tok/sec: 85023.18\n","start training the model\n","step 0 of training\n","step  1566 | loss: 4.690351 | lr 5.9705e-04 | norm: 1.9838 | dt: 96.32ms | tok/sec: 85048.86\n","start training the model\n","step 0 of training\n","step  1567 | loss: 4.710456 | lr 5.9704e-04 | norm: 1.6086 | dt: 96.06ms | tok/sec: 85282.12\n","start training the model\n","step 0 of training\n","step  1568 | loss: 4.548773 | lr 5.9703e-04 | norm: 1.1861 | dt: 96.45ms | tok/sec: 84935.33\n","start training the model\n","step 0 of training\n","step  1569 | loss: 5.092536 | lr 5.9702e-04 | norm: 1.1727 | dt: 95.57ms | tok/sec: 85719.98\n","start training the model\n","step 0 of training\n","step  1570 | loss: 4.747208 | lr 5.9702e-04 | norm: 1.1314 | dt: 95.67ms | tok/sec: 85625.98\n","start training the model\n","step 0 of training\n","step  1571 | loss: 3.935190 | lr 5.9701e-04 | norm: 1.0145 | dt: 95.79ms | tok/sec: 85521.34\n","start training the model\n","step 0 of training\n","step  1572 | loss: 2.244403 | lr 5.9700e-04 | norm: 1.4198 | dt: 96.60ms | tok/sec: 84800.75\n","start training the model\n","step 0 of training\n","step  1573 | loss: 2.114947 | lr 5.9700e-04 | norm: 1.2019 | dt: 95.52ms | tok/sec: 85760.41\n","start training the model\n","step 0 of training\n","step  1574 | loss: 2.028718 | lr 5.9699e-04 | norm: 0.9419 | dt: 95.98ms | tok/sec: 85350.12\n","start training the model\n","step 0 of training\n","step  1575 | loss: 2.093288 | lr 5.9698e-04 | norm: 1.1310 | dt: 95.68ms | tok/sec: 85618.94\n","start training the model\n","step 0 of training\n","step  1576 | loss: 2.171660 | lr 5.9698e-04 | norm: 0.8887 | dt: 95.52ms | tok/sec: 85760.41\n","start training the model\n","step 0 of training\n","step  1577 | loss: 2.008395 | lr 5.9697e-04 | norm: 0.7431 | dt: 95.61ms | tok/sec: 85681.07\n","start training the model\n","step 0 of training\n","step  1578 | loss: 2.207323 | lr 5.9696e-04 | norm: 0.7445 | dt: 95.72ms | tok/sec: 85586.53\n","start training the model\n","step 0 of training\n","step  1579 | loss: 2.098419 | lr 5.9696e-04 | norm: 0.7675 | dt: 96.43ms | tok/sec: 84951.29\n","start training the model\n","step 0 of training\n","step  1580 | loss: 1.802295 | lr 5.9695e-04 | norm: 0.7571 | dt: 96.39ms | tok/sec: 84987.01\n","start training the model\n","step 0 of training\n","step  1581 | loss: 1.783513 | lr 5.9694e-04 | norm: 0.7799 | dt: 96.63ms | tok/sec: 84776.90\n","start training the model\n","step 0 of training\n","step  1582 | loss: 2.064423 | lr 5.9694e-04 | norm: 0.7366 | dt: 96.54ms | tok/sec: 84852.48\n","start training the model\n","step 0 of training\n","step  1583 | loss: 2.226820 | lr 5.9693e-04 | norm: 0.8500 | dt: 96.89ms | tok/sec: 84549.30\n","start training the model\n","step 0 of training\n","step  1584 | loss: 2.297428 | lr 5.9692e-04 | norm: 1.2350 | dt: 97.12ms | tok/sec: 84352.53\n","start training the model\n","step 0 of training\n","step  1585 | loss: 2.047592 | lr 5.9691e-04 | norm: 0.8156 | dt: 96.11ms | tok/sec: 85234.31\n","start training the model\n","step 0 of training\n","step  1586 | loss: 2.088023 | lr 5.9691e-04 | norm: 1.3813 | dt: 96.54ms | tok/sec: 84859.60\n","start training the model\n","step 0 of training\n","step  1587 | loss: 2.078081 | lr 5.9690e-04 | norm: 0.8698 | dt: 96.79ms | tok/sec: 84637.19\n","start training the model\n","step 0 of training\n","step  1588 | loss: 2.107543 | lr 5.9689e-04 | norm: 1.4576 | dt: 95.19ms | tok/sec: 86057.48\n","start training the model\n","step 0 of training\n","step  1589 | loss: 2.000463 | lr 5.9689e-04 | norm: 1.3802 | dt: 96.19ms | tok/sec: 85166.07\n","start training the model\n","step 0 of training\n","step  1590 | loss: 1.963639 | lr 5.9688e-04 | norm: 0.6970 | dt: 122.84ms | tok/sec: 66685.70\n","start training the model\n","step 0 of training\n","step  1591 | loss: 2.008404 | lr 5.9687e-04 | norm: 1.1794 | dt: 96.05ms | tok/sec: 85292.28\n","start training the model\n","step 0 of training\n","step  1592 | loss: 1.974553 | lr 5.9687e-04 | norm: 0.5533 | dt: 96.69ms | tok/sec: 84720.46\n","start training the model\n","step 0 of training\n","step  1593 | loss: 1.972784 | lr 5.9686e-04 | norm: 1.0334 | dt: 97.83ms | tok/sec: 83735.01\n","start training the model\n","step 0 of training\n","step  1594 | loss: 1.902595 | lr 5.9685e-04 | norm: 0.7965 | dt: 96.80ms | tok/sec: 84625.73\n","start training the model\n","step 0 of training\n","step  1595 | loss: 1.856416 | lr 5.9684e-04 | norm: 0.7308 | dt: 97.64ms | tok/sec: 83897.14\n","start training the model\n","step 0 of training\n","step  1596 | loss: 1.989289 | lr 5.9684e-04 | norm: 0.8227 | dt: 96.55ms | tok/sec: 84845.77\n","start training the model\n","step 0 of training\n","step  1597 | loss: 1.911671 | lr 5.9683e-04 | norm: 0.8522 | dt: 96.89ms | tok/sec: 84548.06\n","start training the model\n","step 0 of training\n","step  1598 | loss: 1.943851 | lr 5.9682e-04 | norm: 0.6779 | dt: 96.08ms | tok/sec: 85259.90\n","start training the model\n","step 0 of training\n","step  1599 | loss: 1.928294 | lr 5.9682e-04 | norm: 0.7383 | dt: 96.77ms | tok/sec: 84650.96\n","start training the model\n","step 0 of training\n","step  1600 | loss: 1.917628 | lr 5.9681e-04 | norm: 0.6665 | dt: 97.26ms | tok/sec: 84225.78\n","start training the model\n","step 0 of training\n","step  1601 | loss: 1.761679 | lr 5.9680e-04 | norm: 0.7075 | dt: 96.79ms | tok/sec: 84634.90\n","start training the model\n","step 0 of training\n","step  1602 | loss: 1.793303 | lr 5.9680e-04 | norm: 0.7726 | dt: 97.14ms | tok/sec: 84331.62\n","start training the model\n","step 0 of training\n","step  1603 | loss: 1.775991 | lr 5.9679e-04 | norm: 0.7137 | dt: 96.75ms | tok/sec: 84668.27\n","start training the model\n","step 0 of training\n","step  1604 | loss: 1.788695 | lr 5.9678e-04 | norm: 0.5899 | dt: 97.08ms | tok/sec: 84387.96\n","start training the model\n","step 0 of training\n","step  1605 | loss: 1.735515 | lr 5.9677e-04 | norm: 0.7225 | dt: 96.51ms | tok/sec: 84880.15\n","start training the model\n","step 0 of training\n","step  1606 | loss: 1.753032 | lr 5.9677e-04 | norm: 0.5184 | dt: 96.78ms | tok/sec: 84649.91\n","start training the model\n","step 0 of training\n","step  1607 | loss: 1.784666 | lr 5.9676e-04 | norm: 0.7394 | dt: 96.96ms | tok/sec: 84487.56\n","start training the model\n","step 0 of training\n","step  1608 | loss: 2.652081 | lr 5.9675e-04 | norm: 0.7430 | dt: 96.75ms | tok/sec: 84669.73\n","start training the model\n","step 0 of training\n","step  1609 | loss: 6.021269 | lr 5.9675e-04 | norm: 2.4594 | dt: 96.82ms | tok/sec: 84610.31\n","start training the model\n","step 0 of training\n","step  1610 | loss: 5.491629 | lr 5.9674e-04 | norm: 2.0343 | dt: 95.66ms | tok/sec: 85636.23\n","start training the model\n","step 0 of training\n","step  1611 | loss: 5.435845 | lr 5.9673e-04 | norm: 0.8929 | dt: 96.77ms | tok/sec: 84650.96\n","start training the model\n","step 0 of training\n","step  1612 | loss: 5.365744 | lr 5.9672e-04 | norm: 0.9566 | dt: 96.48ms | tok/sec: 84904.90\n","start training the model\n","step 0 of training\n","step  1613 | loss: 5.312560 | lr 5.9672e-04 | norm: 0.9449 | dt: 96.33ms | tok/sec: 85040.86\n","start training the model\n","step 0 of training\n","step  1614 | loss: 5.324836 | lr 5.9671e-04 | norm: 0.8385 | dt: 96.01ms | tok/sec: 85322.14\n","start training the model\n","step 0 of training\n","step  1615 | loss: 5.363970 | lr 5.9670e-04 | norm: 0.7976 | dt: 96.58ms | tok/sec: 84819.80\n","start training the model\n","step 0 of training\n","step  1616 | loss: 5.267528 | lr 5.9669e-04 | norm: 0.8377 | dt: 97.22ms | tok/sec: 84261.31\n","start training the model\n","step 0 of training\n","step  1617 | loss: 5.427878 | lr 5.9669e-04 | norm: 0.7455 | dt: 96.41ms | tok/sec: 84971.88\n","start training the model\n","step 0 of training\n","step  1618 | loss: 5.457068 | lr 5.9668e-04 | norm: 0.7560 | dt: 96.71ms | tok/sec: 84702.92\n","start training the model\n","step 0 of training\n","step  1619 | loss: 5.413655 | lr 5.9667e-04 | norm: 0.6838 | dt: 96.56ms | tok/sec: 84837.18\n","start training the model\n","step 0 of training\n","step  1620 | loss: 5.378501 | lr 5.9667e-04 | norm: 0.6667 | dt: 96.28ms | tok/sec: 85088.66\n","start training the model\n","step 0 of training\n","step  1621 | loss: 5.467877 | lr 5.9666e-04 | norm: 0.8911 | dt: 96.91ms | tok/sec: 84532.46\n","start training the model\n","step 0 of training\n","step  1622 | loss: 5.530368 | lr 5.9665e-04 | norm: 0.7382 | dt: 96.53ms | tok/sec: 84864.42\n","start training the model\n","step 0 of training\n","step  1623 | loss: 5.451688 | lr 5.9664e-04 | norm: 1.0381 | dt: 96.29ms | tok/sec: 85074.13\n","start training the model\n","step 0 of training\n","step  1624 | loss: 5.413581 | lr 5.9664e-04 | norm: 0.7888 | dt: 96.54ms | tok/sec: 84855.41\n","start training the model\n","step 0 of training\n","step  1625 | loss: 5.432950 | lr 5.9663e-04 | norm: 0.5870 | dt: 95.33ms | tok/sec: 85930.07\n","start training the model\n","step 0 of training\n","step  1626 | loss: 5.318799 | lr 5.9662e-04 | norm: 1.0303 | dt: 96.59ms | tok/sec: 84812.47\n","start training the model\n","step 0 of training\n","step  1627 | loss: 5.725085 | lr 5.9662e-04 | norm: 1.3889 | dt: 97.55ms | tok/sec: 83975.67\n","start training the model\n","step 0 of training\n","step  1628 | loss: 5.423011 | lr 5.9661e-04 | norm: 1.2515 | dt: 97.27ms | tok/sec: 84216.28\n","start training the model\n","step 0 of training\n","step  1629 | loss: 5.269948 | lr 5.9660e-04 | norm: 0.8438 | dt: 97.74ms | tok/sec: 83815.89\n","start training the model\n","step 0 of training\n","step  1630 | loss: 5.403569 | lr 5.9659e-04 | norm: 0.8148 | dt: 97.85ms | tok/sec: 83718.28\n","start training the model\n","step 0 of training\n","step  1631 | loss: 5.405388 | lr 5.9659e-04 | norm: 1.0249 | dt: 97.29ms | tok/sec: 84202.46\n","start training the model\n","step 0 of training\n","step  1632 | loss: 5.280595 | lr 5.9658e-04 | norm: 0.7225 | dt: 97.42ms | tok/sec: 84091.18\n","start training the model\n","step 0 of training\n","step  1633 | loss: 5.265224 | lr 5.9657e-04 | norm: 0.7288 | dt: 96.86ms | tok/sec: 84579.07\n","start training the model\n","step 0 of training\n","step  1634 | loss: 5.271088 | lr 5.9656e-04 | norm: 0.7798 | dt: 96.99ms | tok/sec: 84458.28\n","start training the model\n","step 0 of training\n","step  1635 | loss: 5.363939 | lr 5.9656e-04 | norm: 0.7072 | dt: 96.21ms | tok/sec: 85143.06\n","start training the model\n","step 0 of training\n","step  1636 | loss: 5.459139 | lr 5.9655e-04 | norm: 0.8469 | dt: 97.19ms | tok/sec: 84292.31\n","start training the model\n","step 0 of training\n","step  1637 | loss: 5.552257 | lr 5.9654e-04 | norm: 0.8081 | dt: 100.11ms | tok/sec: 81832.09\n","start training the model\n","step 0 of training\n","step  1638 | loss: 5.512039 | lr 5.9653e-04 | norm: 1.1437 | dt: 96.40ms | tok/sec: 84979.44\n","start training the model\n","step 0 of training\n","step  1639 | loss: 5.575120 | lr 5.9653e-04 | norm: 1.1137 | dt: 95.59ms | tok/sec: 85699.02\n","start training the model\n","step 0 of training\n","step  1640 | loss: 5.594272 | lr 5.9652e-04 | norm: 0.8867 | dt: 95.82ms | tok/sec: 85494.10\n","start training the model\n","step 0 of training\n","step  1641 | loss: 5.340613 | lr 5.9651e-04 | norm: 1.3106 | dt: 96.64ms | tok/sec: 84772.51\n","start training the model\n","step 0 of training\n","step  1642 | loss: 5.327465 | lr 5.9651e-04 | norm: 1.2183 | dt: 96.42ms | tok/sec: 84962.84\n","start training the model\n","step 0 of training\n","step  1643 | loss: 5.177936 | lr 5.9650e-04 | norm: 1.0027 | dt: 96.78ms | tok/sec: 84645.95\n","start training the model\n","step 0 of training\n","step  1644 | loss: 5.386898 | lr 5.9649e-04 | norm: 0.8065 | dt: 96.85ms | tok/sec: 84588.02\n","start training the model\n","step 0 of training\n","step  1645 | loss: 5.196130 | lr 5.9648e-04 | norm: 0.9034 | dt: 97.15ms | tok/sec: 84324.58\n","start training the model\n","step 0 of training\n","step  1646 | loss: 5.336166 | lr 5.9648e-04 | norm: 0.8825 | dt: 96.39ms | tok/sec: 84987.22\n","start training the model\n","step 0 of training\n","step  1647 | loss: 5.295236 | lr 5.9647e-04 | norm: 0.8989 | dt: 96.98ms | tok/sec: 84469.07\n","start training the model\n","step 0 of training\n","step  1648 | loss: 5.153401 | lr 5.9646e-04 | norm: 0.8219 | dt: 96.45ms | tok/sec: 84933.23\n","start training the model\n","step 0 of training\n","step  1649 | loss: 5.186251 | lr 5.9645e-04 | norm: 0.8182 | dt: 96.40ms | tok/sec: 84982.18\n","start training the model\n","step 0 of training\n","step  1650 | loss: 5.194595 | lr 5.9645e-04 | norm: 0.8409 | dt: 97.13ms | tok/sec: 84339.49\n","start training the model\n","step 0 of training\n","step  1651 | loss: 5.241465 | lr 5.9644e-04 | norm: 0.7414 | dt: 96.47ms | tok/sec: 84921.06\n","start training the model\n","step 0 of training\n","step  1652 | loss: 5.229828 | lr 5.9643e-04 | norm: 0.6344 | dt: 96.59ms | tok/sec: 84809.54\n","start training the model\n","step 0 of training\n","step  1653 | loss: 5.407584 | lr 5.9642e-04 | norm: 0.7595 | dt: 96.56ms | tok/sec: 84839.49\n","start training the model\n","step 0 of training\n","step  1654 | loss: 5.354857 | lr 5.9642e-04 | norm: 0.8517 | dt: 96.87ms | tok/sec: 84569.28\n","start training the model\n","step 0 of training\n","step  1655 | loss: 5.500507 | lr 5.9641e-04 | norm: 0.8411 | dt: 97.68ms | tok/sec: 83863.76\n","start training the model\n","step 0 of training\n","step  1656 | loss: 5.370955 | lr 5.9640e-04 | norm: 0.7625 | dt: 96.25ms | tok/sec: 85115.01\n","start training the model\n","step 0 of training\n","step  1657 | loss: 5.495800 | lr 5.9639e-04 | norm: 1.0600 | dt: 97.18ms | tok/sec: 84299.76\n","start training the model\n","step 0 of training\n","step  1658 | loss: 5.546105 | lr 5.9639e-04 | norm: 0.7618 | dt: 96.54ms | tok/sec: 84852.69\n","start training the model\n","step 0 of training\n","step  1659 | loss: 5.352543 | lr 5.9638e-04 | norm: 1.0677 | dt: 96.41ms | tok/sec: 84969.99\n","start training the model\n","step 0 of training\n","step  1660 | loss: 5.328168 | lr 5.9637e-04 | norm: 1.0524 | dt: 97.00ms | tok/sec: 84456.62\n","start training the model\n","step 0 of training\n","step  1661 | loss: 5.341278 | lr 5.9636e-04 | norm: 0.9987 | dt: 96.56ms | tok/sec: 84840.12\n","start training the model\n","step 0 of training\n","step  1662 | loss: 5.393302 | lr 5.9636e-04 | norm: 0.9302 | dt: 95.67ms | tok/sec: 85629.40\n","start training the model\n","step 0 of training\n","step  1663 | loss: 5.447124 | lr 5.9635e-04 | norm: 0.8664 | dt: 96.13ms | tok/sec: 85219.51\n","start training the model\n","step 0 of training\n","step  1664 | loss: 5.396069 | lr 5.9634e-04 | norm: 0.8208 | dt: 98.55ms | tok/sec: 83123.84\n","start training the model\n","step 0 of training\n","step  1665 | loss: 5.344704 | lr 5.9633e-04 | norm: 0.7831 | dt: 97.51ms | tok/sec: 84013.04\n","start training the model\n","step 0 of training\n","step  1666 | loss: 5.200337 | lr 5.9633e-04 | norm: 0.8629 | dt: 98.27ms | tok/sec: 83358.78\n","start training the model\n","step 0 of training\n","step  1667 | loss: 5.209068 | lr 5.9632e-04 | norm: 0.8068 | dt: 97.31ms | tok/sec: 84182.44\n","start training the model\n","step 0 of training\n","step  1668 | loss: 5.196658 | lr 5.9631e-04 | norm: 0.9674 | dt: 98.29ms | tok/sec: 83342.81\n","start training the model\n","step 0 of training\n","step  1669 | loss: 5.125379 | lr 5.9630e-04 | norm: 0.7966 | dt: 97.21ms | tok/sec: 84274.74\n","start training the model\n","step 0 of training\n","step  1670 | loss: 6.063162 | lr 5.9629e-04 | norm: 1.2775 | dt: 97.31ms | tok/sec: 84184.30\n","start training the model\n","step 0 of training\n","step  1671 | loss: 6.085610 | lr 5.9629e-04 | norm: 1.1629 | dt: 96.90ms | tok/sec: 84543.90\n","start training the model\n","step 0 of training\n","step  1672 | loss: 5.950553 | lr 5.9628e-04 | norm: 0.9001 | dt: 97.60ms | tok/sec: 83931.57\n","start training the model\n","step 0 of training\n","step  1673 | loss: 5.884624 | lr 5.9627e-04 | norm: 0.8208 | dt: 96.46ms | tok/sec: 84925.88\n","start training the model\n","step 0 of training\n","step  1674 | loss: 5.847236 | lr 5.9626e-04 | norm: 1.1901 | dt: 96.72ms | tok/sec: 84699.16\n","start training the model\n","step 0 of training\n","step  1675 | loss: 5.927829 | lr 5.9626e-04 | norm: 0.7684 | dt: 96.97ms | tok/sec: 84479.25\n","start training the model\n","step 0 of training\n","step  1676 | loss: 5.766398 | lr 5.9625e-04 | norm: 1.1150 | dt: 96.37ms | tok/sec: 85005.30\n","start training the model\n","step 0 of training\n","step  1677 | loss: 5.733871 | lr 5.9624e-04 | norm: 0.8996 | dt: 97.00ms | tok/sec: 84457.24\n","start training the model\n","step 0 of training\n","step  1678 | loss: 5.745849 | lr 5.9623e-04 | norm: 0.7358 | dt: 96.02ms | tok/sec: 85315.36\n","start training the model\n","step 0 of training\n","step  1679 | loss: 5.821502 | lr 5.9623e-04 | norm: 0.9015 | dt: 96.85ms | tok/sec: 84585.31\n","start training the model\n","step 0 of training\n","step  1680 | loss: 5.758071 | lr 5.9622e-04 | norm: 0.7688 | dt: 96.94ms | tok/sec: 84506.05\n","start training the model\n","step 0 of training\n","step  1681 | loss: 5.785974 | lr 5.9621e-04 | norm: 0.6302 | dt: 95.96ms | tok/sec: 85368.14\n","start training the model\n","step 0 of training\n","step  1682 | loss: 5.754199 | lr 5.9620e-04 | norm: 0.7760 | dt: 96.68ms | tok/sec: 84733.83\n","start training the model\n","step 0 of training\n","step  1683 | loss: 5.759415 | lr 5.9619e-04 | norm: 1.0261 | dt: 97.17ms | tok/sec: 84308.65\n","start training the model\n","step 0 of training\n","step  1684 | loss: 5.612724 | lr 5.9619e-04 | norm: 0.8921 | dt: 97.49ms | tok/sec: 84026.60\n","start training the model\n","step 0 of training\n","step  1685 | loss: 5.732492 | lr 5.9618e-04 | norm: 0.8455 | dt: 97.15ms | tok/sec: 84321.89\n","start training the model\n","step 0 of training\n","step  1686 | loss: 5.687497 | lr 5.9617e-04 | norm: 0.7376 | dt: 98.42ms | tok/sec: 83233.38\n","start training the model\n","step 0 of training\n","step  1687 | loss: 5.577268 | lr 5.9616e-04 | norm: 0.8909 | dt: 97.17ms | tok/sec: 84307.62\n","start training the model\n","step 0 of training\n","step  1688 | loss: 6.000622 | lr 5.9616e-04 | norm: 1.5725 | dt: 98.36ms | tok/sec: 83288.06\n","start training the model\n","step 0 of training\n","step  1689 | loss: 5.819468 | lr 5.9615e-04 | norm: 2.7094 | dt: 96.69ms | tok/sec: 84726.94\n","start training the model\n","step 0 of training\n","step  1690 | loss: 5.147147 | lr 5.9614e-04 | norm: 2.7506 | dt: 97.12ms | tok/sec: 84348.18\n","start training the model\n","step 0 of training\n","step  1691 | loss: 5.441852 | lr 5.9613e-04 | norm: 1.6974 | dt: 96.66ms | tok/sec: 84746.79\n","start training the model\n","step 0 of training\n","step  1692 | loss: 4.603373 | lr 5.9613e-04 | norm: 1.7723 | dt: 96.98ms | tok/sec: 84470.53\n","start training the model\n","step 0 of training\n","step  1693 | loss: 4.644214 | lr 5.9612e-04 | norm: 2.0165 | dt: 96.74ms | tok/sec: 84681.42\n","start training the model\n","step 0 of training\n","step  1694 | loss: 4.479786 | lr 5.9611e-04 | norm: 1.4150 | dt: 96.39ms | tok/sec: 84986.59\n","start training the model\n","step 0 of training\n","step  1695 | loss: 5.089005 | lr 5.9610e-04 | norm: 2.6332 | dt: 96.90ms | tok/sec: 84536.82\n","start training the model\n","step 0 of training\n","step  1696 | loss: 4.725373 | lr 5.9609e-04 | norm: 1.5333 | dt: 97.01ms | tok/sec: 84447.69\n","start training the model\n","step 0 of training\n","step  1697 | loss: 3.955689 | lr 5.9609e-04 | norm: 1.5647 | dt: 96.72ms | tok/sec: 84698.95\n","start training the model\n","step 0 of training\n","step  1698 | loss: 2.317053 | lr 5.9608e-04 | norm: 1.8939 | dt: 96.57ms | tok/sec: 84830.90\n","start training the model\n","step 0 of training\n","step  1699 | loss: 2.178557 | lr 5.9607e-04 | norm: 1.3594 | dt: 96.05ms | tok/sec: 85286.35\n","start training the model\n","step 0 of training\n","step  1700 | loss: 2.042091 | lr 5.9606e-04 | norm: 1.0342 | dt: 96.19ms | tok/sec: 85163.53\n","start training the model\n","step 0 of training\n","step  1701 | loss: 2.101268 | lr 5.9605e-04 | norm: 1.2030 | dt: 96.11ms | tok/sec: 85233.88\n","start training the model\n","step 0 of training\n","step  1702 | loss: 2.163610 | lr 5.9605e-04 | norm: 0.8954 | dt: 96.54ms | tok/sec: 84855.83\n","start training the model\n","step 0 of training\n","step  1703 | loss: 1.990297 | lr 5.9604e-04 | norm: 0.7553 | dt: 96.45ms | tok/sec: 84938.90\n","start training the model\n","step 0 of training\n","step  1704 | loss: 2.210639 | lr 5.9603e-04 | norm: 0.7594 | dt: 96.38ms | tok/sec: 84997.52\n","start training the model\n","step 0 of training\n","step  1705 | loss: 2.076204 | lr 5.9602e-04 | norm: 0.7716 | dt: 97.06ms | tok/sec: 84399.77\n","start training the model\n","step 0 of training\n","step  1706 | loss: 1.794909 | lr 5.9602e-04 | norm: 0.9507 | dt: 96.41ms | tok/sec: 84974.82\n","start training the model\n","step 0 of training\n","step  1707 | loss: 1.767811 | lr 5.9601e-04 | norm: 0.8182 | dt: 97.83ms | tok/sec: 83736.03\n","start training the model\n","step 0 of training\n","step  1708 | loss: 2.032990 | lr 5.9600e-04 | norm: 0.6846 | dt: 96.26ms | tok/sec: 85101.52\n","start training the model\n","step 0 of training\n","step  1709 | loss: 2.191135 | lr 5.9599e-04 | norm: 0.7184 | dt: 96.75ms | tok/sec: 84671.61\n","start training the model\n","step 0 of training\n","step  1710 | loss: 2.299491 | lr 5.9598e-04 | norm: 1.1601 | dt: 96.47ms | tok/sec: 84914.76\n","start training the model\n","step 0 of training\n","step  1711 | loss: 2.034247 | lr 5.9598e-04 | norm: 0.9145 | dt: 96.29ms | tok/sec: 85076.65\n","start training the model\n","step 0 of training\n","step  1712 | loss: 2.069886 | lr 5.9597e-04 | norm: 0.7250 | dt: 96.94ms | tok/sec: 84508.75\n","start training the model\n","step 0 of training\n","step  1713 | loss: 2.060627 | lr 5.9596e-04 | norm: 0.7649 | dt: 96.31ms | tok/sec: 85060.65\n","start training the model\n","step 0 of training\n","step  1714 | loss: 2.087091 | lr 5.9595e-04 | norm: 0.6234 | dt: 97.13ms | tok/sec: 84340.52\n","start training the model\n","step 0 of training\n","step  1715 | loss: 1.979092 | lr 5.9594e-04 | norm: 0.7755 | dt: 96.50ms | tok/sec: 84886.86\n","start training the model\n","step 0 of training\n","step  1716 | loss: 1.948697 | lr 5.9594e-04 | norm: 0.7511 | dt: 96.47ms | tok/sec: 84916.44\n","start training the model\n","step 0 of training\n","step  1717 | loss: 1.974869 | lr 5.9593e-04 | norm: 0.5784 | dt: 97.06ms | tok/sec: 84405.37\n","start training the model\n","step 0 of training\n","step  1718 | loss: 1.954332 | lr 5.9592e-04 | norm: 0.6848 | dt: 95.57ms | tok/sec: 85720.19\n","start training the model\n","step 0 of training\n","step  1719 | loss: 1.933152 | lr 5.9591e-04 | norm: 0.7044 | dt: 96.72ms | tok/sec: 84697.91\n","start training the model\n","step 0 of training\n","step  1720 | loss: 1.865221 | lr 5.9590e-04 | norm: 0.5399 | dt: 96.77ms | tok/sec: 84654.08\n","start training the model\n","step 0 of training\n","step  1721 | loss: 1.830968 | lr 5.9590e-04 | norm: 0.4648 | dt: 97.83ms | tok/sec: 83737.05\n","start training the model\n","step 0 of training\n","step  1722 | loss: 1.952553 | lr 5.9589e-04 | norm: 0.5215 | dt: 98.06ms | tok/sec: 83538.95\n","start training the model\n","step 0 of training\n","step  1723 | loss: 1.875736 | lr 5.9588e-04 | norm: 0.5515 | dt: 98.08ms | tok/sec: 83520.68\n","start training the model\n","step 0 of training\n","step  1724 | loss: 1.921425 | lr 5.9587e-04 | norm: 0.6867 | dt: 97.76ms | tok/sec: 83799.74\n","start training the model\n","step 0 of training\n","step  1725 | loss: 1.895844 | lr 5.9586e-04 | norm: 0.5809 | dt: 98.00ms | tok/sec: 83592.00\n","start training the model\n","step 0 of training\n","step  1726 | loss: 1.889957 | lr 5.9586e-04 | norm: 0.7051 | dt: 96.94ms | tok/sec: 84502.52\n","start training the model\n","step 0 of training\n","step  1727 | loss: 1.747684 | lr 5.9585e-04 | norm: 0.7573 | dt: 97.99ms | tok/sec: 83599.93\n","start training the model\n","step 0 of training\n","step  1728 | loss: 1.783596 | lr 5.9584e-04 | norm: 0.8513 | dt: 96.50ms | tok/sec: 84893.36\n","start training the model\n","step 0 of training\n","step  1729 | loss: 1.762867 | lr 5.9583e-04 | norm: 0.6613 | dt: 96.65ms | tok/sec: 84761.21\n","start training the model\n","step 0 of training\n","step  1730 | loss: 1.775315 | lr 5.9582e-04 | norm: 1.0180 | dt: 96.95ms | tok/sec: 84496.49\n","start training the model\n","step 0 of training\n","step  1731 | loss: 1.720407 | lr 5.9582e-04 | norm: 1.2307 | dt: 96.69ms | tok/sec: 84725.06\n","start training the model\n","step 0 of training\n","step  1732 | loss: 1.738596 | lr 5.9581e-04 | norm: 0.9937 | dt: 97.35ms | tok/sec: 84150.08\n","start training the model\n","step 0 of training\n","step  1733 | loss: 1.763803 | lr 5.9580e-04 | norm: 0.8957 | dt: 96.38ms | tok/sec: 84993.74\n","start training the model\n","step 0 of training\n","step  1734 | loss: 2.629594 | lr 5.9579e-04 | norm: 1.2560 | dt: 96.98ms | tok/sec: 84467.20\n","start training the model\n","step 0 of training\n","step  1735 | loss: 5.994197 | lr 5.9578e-04 | norm: 2.4967 | dt: 96.89ms | tok/sec: 84550.35\n","start training the model\n","step 0 of training\n","step  1736 | loss: 5.481300 | lr 5.9577e-04 | norm: 1.8729 | dt: 97.83ms | tok/sec: 83739.29\n","start training the model\n","step 0 of training\n","step  1737 | loss: 5.427936 | lr 5.9577e-04 | norm: 1.2112 | dt: 95.65ms | tok/sec: 85649.46\n","start training the model\n","step 0 of training\n","step  1738 | loss: 5.350453 | lr 5.9576e-04 | norm: 1.1093 | dt: 97.06ms | tok/sec: 84401.85\n","start training the model\n","step 0 of training\n","step  1739 | loss: 5.288795 | lr 5.9575e-04 | norm: 1.0111 | dt: 97.55ms | tok/sec: 83974.03\n","start training the model\n","step 0 of training\n","step  1740 | loss: 5.311920 | lr 5.9574e-04 | norm: 1.0166 | dt: 96.86ms | tok/sec: 84578.23\n","start training the model\n","step 0 of training\n","step  1741 | loss: 5.343995 | lr 5.9573e-04 | norm: 0.9075 | dt: 97.14ms | tok/sec: 84333.28\n","start training the model\n","step 0 of training\n","step  1742 | loss: 5.224503 | lr 5.9573e-04 | norm: 0.7437 | dt: 97.16ms | tok/sec: 84318.38\n","start training the model\n","step 0 of training\n","step  1743 | loss: 5.395639 | lr 5.9572e-04 | norm: 0.8827 | dt: 96.95ms | tok/sec: 84493.38\n","start training the model\n","step 0 of training\n","step  1744 | loss: 5.427755 | lr 5.9571e-04 | norm: 0.9073 | dt: 97.29ms | tok/sec: 84199.77\n","start training the model\n","step 0 of training\n","step  1745 | loss: 5.360125 | lr 5.9570e-04 | norm: 0.6219 | dt: 96.75ms | tok/sec: 84667.64\n","start training the model\n","step 0 of training\n","step  1746 | loss: 5.330066 | lr 5.9569e-04 | norm: 0.6482 | dt: 96.84ms | tok/sec: 84594.48\n","start training the model\n","step 0 of training\n","step  1747 | loss: 5.400287 | lr 5.9568e-04 | norm: 0.9332 | dt: 96.87ms | tok/sec: 84567.41\n","start training the model\n","step 0 of training\n","step  1748 | loss: 5.474453 | lr 5.9568e-04 | norm: 0.7614 | dt: 96.65ms | tok/sec: 84762.68\n","start training the model\n","step 0 of training\n","step  1749 | loss: 5.403416 | lr 5.9567e-04 | norm: 0.7292 | dt: 96.94ms | tok/sec: 84508.75\n","validation loss: 5.9568\n","rank 0 sample 0: [CLS] دردم از یار است و درمان نیز هم[SEP] با من، دل[BOB][BOM] گر عظمم، زار من اینست، ای بادش در آن چه\n","rank 0 sample 1: [CLS] دردم از یار است و درمان نیز هم[SEP] باش[BOM] گو ن نیست[BOB][BOM] که از خویشتن ز آب حیات ما در دل از خود به در دل\n","rank 0 sample 2: [CLS] دردم از یار است و درمان نیز هم[SEP] و سیم چو روی خاک خاک فلک و مه که ما را ز دست به لب لب چون سرو و گر به\n","rank 0 sample 3: [CLS] دردم از یار است و درمان نیز هم[SEP] است[BOM] که از این است[BOM] می روی لب به کام باد از سر و شب از دل نازک در جان\n","start training the model\n","step 0 of training\n","step  1750 | loss: 5.377280 | lr 5.9566e-04 | norm: 0.7699 | dt: 1285.15ms | tok/sec: 6374.37\n","start training the model\n","step 0 of training\n","step  1751 | loss: 5.422521 | lr 5.9565e-04 | norm: 0.7936 | dt: 95.64ms | tok/sec: 85658.22\n","start training the model\n","step 0 of training\n","step  1752 | loss: 5.298458 | lr 5.9564e-04 | norm: 0.8710 | dt: 95.00ms | tok/sec: 86233.72\n","start training the model\n","step 0 of training\n","step  1753 | loss: 5.678263 | lr 5.9564e-04 | norm: 1.6856 | dt: 97.05ms | tok/sec: 84413.46\n","start training the model\n","step 0 of training\n","step  1754 | loss: 5.395528 | lr 5.9563e-04 | norm: 0.9305 | dt: 99.57ms | tok/sec: 82270.20\n","start training the model\n","step 0 of training\n","step  1755 | loss: 5.248009 | lr 5.9562e-04 | norm: 0.8934 | dt: 97.31ms | tok/sec: 84184.92\n","start training the model\n","step 0 of training\n","step  1756 | loss: 5.369211 | lr 5.9561e-04 | norm: 0.9153 | dt: 97.74ms | tok/sec: 83815.28\n","start training the model\n","step 0 of training\n","step  1757 | loss: 5.372655 | lr 5.9560e-04 | norm: 0.8685 | dt: 96.78ms | tok/sec: 84649.08\n","start training the model\n","step 0 of training\n","step  1758 | loss: 5.243284 | lr 5.9559e-04 | norm: 0.9089 | dt: 96.33ms | tok/sec: 85045.28\n","start training the model\n","step 0 of training\n","step  1759 | loss: 5.237893 | lr 5.9559e-04 | norm: 0.8334 | dt: 97.16ms | tok/sec: 84315.48\n","start training the model\n","step 0 of training\n","step  1760 | loss: 5.218041 | lr 5.9558e-04 | norm: 0.7963 | dt: 96.90ms | tok/sec: 84539.94\n","start training the model\n","step 0 of training\n","step  1761 | loss: 5.314239 | lr 5.9557e-04 | norm: 0.6782 | dt: 97.47ms | tok/sec: 84050.45\n","start training the model\n","step 0 of training\n","step  1762 | loss: 5.414323 | lr 5.9556e-04 | norm: 0.8318 | dt: 96.83ms | tok/sec: 84598.02\n","start training the model\n","step 0 of training\n","step  1763 | loss: 5.478551 | lr 5.9555e-04 | norm: 0.7601 | dt: 100.74ms | tok/sec: 81320.01\n","start training the model\n","step 0 of training\n","step  1764 | loss: 5.447902 | lr 5.9554e-04 | norm: 1.2587 | dt: 95.95ms | tok/sec: 85373.66\n","start training the model\n","step 0 of training\n","step  1765 | loss: 5.546966 | lr 5.9554e-04 | norm: 1.0838 | dt: 96.60ms | tok/sec: 84803.68\n","start training the model\n","step 0 of training\n","step  1766 | loss: 5.542099 | lr 5.9553e-04 | norm: 1.0976 | dt: 96.53ms | tok/sec: 84868.41\n","start training the model\n","step 0 of training\n","step  1767 | loss: 5.291172 | lr 5.9552e-04 | norm: 0.9605 | dt: 96.38ms | tok/sec: 84992.69\n","start training the model\n","step 0 of training\n","step  1768 | loss: 5.267829 | lr 5.9551e-04 | norm: 1.1004 | dt: 96.63ms | tok/sec: 84778.36\n","start training the model\n","step 0 of training\n","step  1769 | loss: 5.110974 | lr 5.9550e-04 | norm: 1.2086 | dt: 96.13ms | tok/sec: 85218.66\n","start training the model\n","step 0 of training\n","step  1770 | loss: 5.312987 | lr 5.9549e-04 | norm: 0.8646 | dt: 96.23ms | tok/sec: 85125.13\n","start training the model\n","step 0 of training\n","step  1771 | loss: 5.135158 | lr 5.9549e-04 | norm: 1.0896 | dt: 96.95ms | tok/sec: 84493.58\n","start training the model\n","step 0 of training\n","step  1772 | loss: 5.268789 | lr 5.9548e-04 | norm: 1.0349 | dt: 95.88ms | tok/sec: 85437.13\n","start training the model\n","step 0 of training\n","step  1773 | loss: 5.242705 | lr 5.9547e-04 | norm: 1.0104 | dt: 96.92ms | tok/sec: 84527.26\n","start training the model\n","step 0 of training\n","step  1774 | loss: 5.099965 | lr 5.9546e-04 | norm: 0.9711 | dt: 96.34ms | tok/sec: 85030.55\n","start training the model\n","step 0 of training\n","step  1775 | loss: 5.131168 | lr 5.9545e-04 | norm: 0.8339 | dt: 96.41ms | tok/sec: 84969.15\n","start training the model\n","step 0 of training\n","step  1776 | loss: 5.150036 | lr 5.9544e-04 | norm: 0.7952 | dt: 97.05ms | tok/sec: 84407.65\n","start training the model\n","step 0 of training\n","step  1777 | loss: 5.194993 | lr 5.9543e-04 | norm: 0.8711 | dt: 96.41ms | tok/sec: 84969.78\n","start training the model\n","step 0 of training\n","step  1778 | loss: 5.179415 | lr 5.9543e-04 | norm: 0.7784 | dt: 97.70ms | tok/sec: 83850.46\n","start training the model\n","step 0 of training\n","step  1779 | loss: 5.357666 | lr 5.9542e-04 | norm: 0.9193 | dt: 96.20ms | tok/sec: 85154.67\n","start training the model\n","step 0 of training\n","step  1780 | loss: 5.293300 | lr 5.9541e-04 | norm: 0.8912 | dt: 97.16ms | tok/sec: 84311.76\n","start training the model\n","step 0 of training\n","step  1781 | loss: 5.413003 | lr 5.9540e-04 | norm: 0.7608 | dt: 96.44ms | tok/sec: 84947.51\n","start training the model\n","step 0 of training\n","step  1782 | loss: 5.301568 | lr 5.9539e-04 | norm: 0.7431 | dt: 96.56ms | tok/sec: 84842.21\n","start training the model\n","step 0 of training\n","step  1783 | loss: 5.428624 | lr 5.9538e-04 | norm: 0.9078 | dt: 96.98ms | tok/sec: 84474.06\n","start training the model\n","step 0 of training\n","step  1784 | loss: 5.487507 | lr 5.9538e-04 | norm: 0.7570 | dt: 96.79ms | tok/sec: 84641.16\n","start training the model\n","step 0 of training\n","step  1785 | loss: 5.287700 | lr 5.9537e-04 | norm: 1.2510 | dt: 96.00ms | tok/sec: 85332.52\n","start training the model\n","step 0 of training\n","step  1786 | loss: 5.273028 | lr 5.9536e-04 | norm: 0.9327 | dt: 96.59ms | tok/sec: 84809.54\n","start training the model\n","step 0 of training\n","step  1787 | loss: 5.253697 | lr 5.9535e-04 | norm: 0.8201 | dt: 96.44ms | tok/sec: 84946.88\n","start training the model\n","step 0 of training\n","step  1788 | loss: 5.335449 | lr 5.9534e-04 | norm: 0.7983 | dt: 95.53ms | tok/sec: 85748.86\n","start training the model\n","step 0 of training\n","step  1789 | loss: 5.382536 | lr 5.9533e-04 | norm: 0.9495 | dt: 95.37ms | tok/sec: 85898.92\n","start training the model\n","step 0 of training\n","step  1790 | loss: 5.328783 | lr 5.9532e-04 | norm: 0.8173 | dt: 96.66ms | tok/sec: 84752.02\n","start training the model\n","step 0 of training\n","step  1791 | loss: 5.269764 | lr 5.9532e-04 | norm: 0.7782 | dt: 97.30ms | tok/sec: 84195.65\n","start training the model\n","step 0 of training\n","step  1792 | loss: 5.119375 | lr 5.9531e-04 | norm: 0.8290 | dt: 97.72ms | tok/sec: 83834.91\n","start training the model\n","step 0 of training\n","step  1793 | loss: 5.139703 | lr 5.9530e-04 | norm: 0.8994 | dt: 97.20ms | tok/sec: 84281.56\n","start training the model\n","step 0 of training\n","step  1794 | loss: 5.125323 | lr 5.9529e-04 | norm: 0.7673 | dt: 98.55ms | tok/sec: 83121.83\n","start training the model\n","step 0 of training\n","step  1795 | loss: 5.049070 | lr 5.9528e-04 | norm: 0.7136 | dt: 97.14ms | tok/sec: 84330.17\n","start training the model\n","step 0 of training\n","step  1796 | loss: 6.028005 | lr 5.9527e-04 | norm: 1.2188 | dt: 97.39ms | tok/sec: 84115.67\n","start training the model\n","step 0 of training\n","step  1797 | loss: 6.065878 | lr 5.9526e-04 | norm: 0.9775 | dt: 96.74ms | tok/sec: 84683.29\n","start training the model\n","step 0 of training\n","step  1798 | loss: 5.916983 | lr 5.9526e-04 | norm: 0.9026 | dt: 96.63ms | tok/sec: 84780.46\n","start training the model\n","step 0 of training\n","step  1799 | loss: 5.853429 | lr 5.9525e-04 | norm: 1.0216 | dt: 97.14ms | tok/sec: 84331.62\n","start training the model\n","step 0 of training\n","step  1800 | loss: 5.822729 | lr 5.9524e-04 | norm: 1.0535 | dt: 96.14ms | tok/sec: 85213.38\n","start training the model\n","step 0 of training\n","step  1801 | loss: 5.910287 | lr 5.9523e-04 | norm: 1.0462 | dt: 96.92ms | tok/sec: 84526.63\n","start training the model\n","step 0 of training\n","step  1802 | loss: 5.727262 | lr 5.9522e-04 | norm: 0.9891 | dt: 96.93ms | tok/sec: 84511.04\n","start training the model\n","step 0 of training\n","step  1803 | loss: 5.715014 | lr 5.9521e-04 | norm: 1.0783 | dt: 96.98ms | tok/sec: 84468.04\n","start training the model\n","step 0 of training\n","step  1804 | loss: 5.706448 | lr 5.9520e-04 | norm: 0.8564 | dt: 96.81ms | tok/sec: 84616.77\n","start training the model\n","step 0 of training\n","step  1805 | loss: 5.763305 | lr 5.9519e-04 | norm: 0.9949 | dt: 96.94ms | tok/sec: 84504.81\n","start training the model\n","step 0 of training\n","step  1806 | loss: 5.726099 | lr 5.9519e-04 | norm: 1.3218 | dt: 97.70ms | tok/sec: 83844.73\n","start training the model\n","step 0 of training\n","step  1807 | loss: 5.760067 | lr 5.9518e-04 | norm: 1.2341 | dt: 96.21ms | tok/sec: 85151.08\n","start training the model\n","step 0 of training\n","step  1808 | loss: 5.694368 | lr 5.9517e-04 | norm: 0.9548 | dt: 96.29ms | tok/sec: 85079.81\n","start training the model\n","step 0 of training\n","step  1809 | loss: 5.667940 | lr 5.9516e-04 | norm: 1.0460 | dt: 96.73ms | tok/sec: 84691.02\n","start training the model\n","step 0 of training\n","step  1810 | loss: 5.524578 | lr 5.9515e-04 | norm: 1.1454 | dt: 96.77ms | tok/sec: 84650.12\n","start training the model\n","step 0 of training\n","step  1811 | loss: 5.635942 | lr 5.9514e-04 | norm: 0.9833 | dt: 96.72ms | tok/sec: 84697.91\n","start training the model\n","step 0 of training\n","step  1812 | loss: 5.612893 | lr 5.9513e-04 | norm: 0.9844 | dt: 96.79ms | tok/sec: 84633.86\n","start training the model\n","step 0 of training\n","step  1813 | loss: 5.497113 | lr 5.9513e-04 | norm: 1.2010 | dt: 98.11ms | tok/sec: 83496.52\n","start training the model\n","step 0 of training\n","step  1814 | loss: 5.891544 | lr 5.9512e-04 | norm: 1.8127 | dt: 96.67ms | tok/sec: 84741.15\n","start training the model\n","step 0 of training\n","step  1815 | loss: 5.728288 | lr 5.9511e-04 | norm: 2.3438 | dt: 96.61ms | tok/sec: 84795.52\n","start training the model\n","step 0 of training\n","step  1816 | loss: 5.067598 | lr 5.9510e-04 | norm: 2.6454 | dt: 96.09ms | tok/sec: 85256.72\n","start training the model\n","step 0 of training\n","step  1817 | loss: 5.302170 | lr 5.9509e-04 | norm: 1.4791 | dt: 96.31ms | tok/sec: 85061.07\n","start training the model\n","step 0 of training\n","step  1818 | loss: 4.532225 | lr 5.9508e-04 | norm: 1.8298 | dt: 96.23ms | tok/sec: 85125.34\n","start training the model\n","step 0 of training\n","step  1819 | loss: 4.610053 | lr 5.9507e-04 | norm: 2.2794 | dt: 96.31ms | tok/sec: 85055.80\n","start training the model\n","step 0 of training\n","step  1820 | loss: 4.410752 | lr 5.9506e-04 | norm: 1.3289 | dt: 96.96ms | tok/sec: 84491.71\n","start training the model\n","step 0 of training\n","step  1821 | loss: 5.056407 | lr 5.9506e-04 | norm: 1.7525 | dt: 96.39ms | tok/sec: 84991.43\n","start training the model\n","step 0 of training\n","step  1822 | loss: 4.649328 | lr 5.9505e-04 | norm: 1.1906 | dt: 96.64ms | tok/sec: 84767.07\n","start training the model\n","step 0 of training\n","step  1823 | loss: 3.845010 | lr 5.9504e-04 | norm: 1.0256 | dt: 96.72ms | tok/sec: 84694.77\n","start training the model\n","step 0 of training\n","step  1824 | loss: 2.199335 | lr 5.9503e-04 | norm: 1.3807 | dt: 96.76ms | tok/sec: 84664.93\n","start training the model\n","step 0 of training\n","step  1825 | loss: 2.115267 | lr 5.9502e-04 | norm: 1.7633 | dt: 96.99ms | tok/sec: 84463.47\n","start training the model\n","step 0 of training\n","step  1826 | loss: 2.016796 | lr 5.9501e-04 | norm: 1.5538 | dt: 95.85ms | tok/sec: 85469.43\n","start training the model\n","step 0 of training\n","step  1827 | loss: 2.064658 | lr 5.9500e-04 | norm: 1.2706 | dt: 96.66ms | tok/sec: 84752.85\n","start training the model\n","step 0 of training\n","step  1828 | loss: 2.128831 | lr 5.9499e-04 | norm: 0.9131 | dt: 96.93ms | tok/sec: 84511.46\n","start training the model\n","step 0 of training\n","step  1829 | loss: 1.979991 | lr 5.9498e-04 | norm: 0.8906 | dt: 97.15ms | tok/sec: 84327.07\n","start training the model\n","step 0 of training\n","step  1830 | loss: 2.179698 | lr 5.9498e-04 | norm: 0.9414 | dt: 97.25ms | tok/sec: 84235.07\n","start training the model\n","step 0 of training\n","step  1831 | loss: 2.050569 | lr 5.9497e-04 | norm: 0.9949 | dt: 97.57ms | tok/sec: 83959.67\n","start training the model\n","step 0 of training\n","step  1832 | loss: 1.761572 | lr 5.9496e-04 | norm: 0.7458 | dt: 96.28ms | tok/sec: 85081.29\n","start training the model\n","step 0 of training\n","step  1833 | loss: 1.746824 | lr 5.9495e-04 | norm: 0.7000 | dt: 96.91ms | tok/sec: 84529.75\n","start training the model\n","step 0 of training\n","step  1834 | loss: 2.012737 | lr 5.9494e-04 | norm: 0.7702 | dt: 96.91ms | tok/sec: 84535.78\n","start training the model\n","step 0 of training\n","step  1835 | loss: 2.163588 | lr 5.9493e-04 | norm: 0.7930 | dt: 96.56ms | tok/sec: 84834.67\n","start training the model\n","step 0 of training\n","step  1836 | loss: 2.262876 | lr 5.9492e-04 | norm: 1.3542 | dt: 96.76ms | tok/sec: 84661.59\n","start training the model\n","step 0 of training\n","step  1837 | loss: 2.006658 | lr 5.9491e-04 | norm: 0.8198 | dt: 96.46ms | tok/sec: 84927.35\n","start training the model\n","step 0 of training\n","step  1838 | loss: 2.044667 | lr 5.9490e-04 | norm: 0.8402 | dt: 96.78ms | tok/sec: 84648.66\n","start training the model\n","step 0 of training\n","step  1839 | loss: 2.030786 | lr 5.9490e-04 | norm: 0.6851 | dt: 96.62ms | tok/sec: 84787.99\n","start training the model\n","step 0 of training\n","step  1840 | loss: 2.054713 | lr 5.9489e-04 | norm: 1.0190 | dt: 97.01ms | tok/sec: 84447.48\n","start training the model\n","step 0 of training\n","step  1841 | loss: 1.947065 | lr 5.9488e-04 | norm: 0.6241 | dt: 96.95ms | tok/sec: 84494.21\n","start training the model\n","step 0 of training\n","step  1842 | loss: 1.939007 | lr 5.9487e-04 | norm: 0.9055 | dt: 96.72ms | tok/sec: 84699.58\n","start training the model\n","step 0 of training\n","step  1843 | loss: 1.956130 | lr 5.9486e-04 | norm: 0.5267 | dt: 96.64ms | tok/sec: 84764.77\n","start training the model\n","step 0 of training\n","step  1844 | loss: 1.937222 | lr 5.9485e-04 | norm: 0.7638 | dt: 96.55ms | tok/sec: 84845.98\n","start training the model\n","step 0 of training\n","step  1845 | loss: 1.913647 | lr 5.9484e-04 | norm: 0.7044 | dt: 96.36ms | tok/sec: 85012.45\n","start training the model\n","step 0 of training\n","step  1846 | loss: 1.843772 | lr 5.9483e-04 | norm: 0.7422 | dt: 97.67ms | tok/sec: 83872.97\n","start training the model\n","step 0 of training\n","step  1847 | loss: 1.811654 | lr 5.9482e-04 | norm: 0.6043 | dt: 98.79ms | tok/sec: 82921.23\n","start training the model\n","step 0 of training\n","step  1848 | loss: 1.930220 | lr 5.9481e-04 | norm: 0.5875 | dt: 97.68ms | tok/sec: 83866.22\n","start training the model\n","step 0 of training\n","step  1849 | loss: 1.846295 | lr 5.9481e-04 | norm: 0.8260 | dt: 97.78ms | tok/sec: 83782.99\n","start training the model\n","step 0 of training\n","step  1850 | loss: 1.891939 | lr 5.9480e-04 | norm: 1.0892 | dt: 96.84ms | tok/sec: 84592.81\n","start training the model\n","step 0 of training\n","step  1851 | loss: 1.873409 | lr 5.9479e-04 | norm: 0.8284 | dt: 97.48ms | tok/sec: 84040.37\n","start training the model\n","step 0 of training\n","step  1852 | loss: 1.868106 | lr 5.9478e-04 | norm: 0.7438 | dt: 97.11ms | tok/sec: 84359.99\n","start training the model\n","step 0 of training\n","step  1853 | loss: 1.715588 | lr 5.9477e-04 | norm: 0.8585 | dt: 97.55ms | tok/sec: 83981.63\n","start training the model\n","step 0 of training\n","step  1854 | loss: 1.763508 | lr 5.9476e-04 | norm: 0.9901 | dt: 96.49ms | tok/sec: 84898.18\n","start training the model\n","step 0 of training\n","step  1855 | loss: 1.732352 | lr 5.9475e-04 | norm: 0.7506 | dt: 96.98ms | tok/sec: 84467.20\n","start training the model\n","step 0 of training\n","step  1856 | loss: 1.748049 | lr 5.9474e-04 | norm: 1.0091 | dt: 96.07ms | tok/sec: 85273.23\n","start training the model\n","step 0 of training\n","step  1857 | loss: 1.699632 | lr 5.9473e-04 | norm: 1.0040 | dt: 96.23ms | tok/sec: 85132.94\n","start training the model\n","step 0 of training\n","step  1858 | loss: 1.700132 | lr 5.9472e-04 | norm: 0.7536 | dt: 96.45ms | tok/sec: 84936.59\n","start training the model\n","step 0 of training\n","step  1859 | loss: 1.741254 | lr 5.9472e-04 | norm: 1.6908 | dt: 96.50ms | tok/sec: 84887.07\n","start training the model\n","step 0 of training\n","step  1860 | loss: 2.611211 | lr 5.9471e-04 | norm: 1.3817 | dt: 96.76ms | tok/sec: 84662.22\n","start training the model\n","step 0 of training\n","step  1861 | loss: 6.019192 | lr 5.9470e-04 | norm: 2.7980 | dt: 96.31ms | tok/sec: 85062.12\n","start training the model\n","step 0 of training\n","step  1862 | loss: 5.490786 | lr 5.9469e-04 | norm: 2.2301 | dt: 96.68ms | tok/sec: 84736.55\n","start training the model\n","step 0 of training\n","step  1863 | loss: 5.420162 | lr 5.9468e-04 | norm: 1.4598 | dt: 97.03ms | tok/sec: 84426.32\n","start training the model\n","step 0 of training\n","step  1864 | loss: 5.326970 | lr 5.9467e-04 | norm: 0.9781 | dt: 96.55ms | tok/sec: 84845.14\n","start training the model\n","step 0 of training\n","step  1865 | loss: 5.253566 | lr 5.9466e-04 | norm: 0.8572 | dt: 97.41ms | tok/sec: 84102.08\n","start training the model\n","step 0 of training\n","step  1866 | loss: 5.242148 | lr 5.9465e-04 | norm: 0.7173 | dt: 96.91ms | tok/sec: 84534.12\n","start training the model\n","step 0 of training\n","step  1867 | loss: 5.278723 | lr 5.9464e-04 | norm: 0.6813 | dt: 96.85ms | tok/sec: 84586.56\n","start training the model\n","step 0 of training\n","step  1868 | loss: 5.182795 | lr 5.9463e-04 | norm: 0.7475 | dt: 96.74ms | tok/sec: 84680.37\n","start training the model\n","step 0 of training\n","step  1869 | loss: 5.338628 | lr 5.9462e-04 | norm: 0.7326 | dt: 97.12ms | tok/sec: 84352.32\n","start training the model\n","step 0 of training\n","step  1870 | loss: 5.366877 | lr 5.9461e-04 | norm: 0.7669 | dt: 97.01ms | tok/sec: 84446.44\n","start training the model\n","step 0 of training\n","step  1871 | loss: 5.304438 | lr 5.9461e-04 | norm: 0.6561 | dt: 96.47ms | tok/sec: 84919.38\n","start training the model\n","step 0 of training\n","step  1872 | loss: 5.265100 | lr 5.9460e-04 | norm: 0.7062 | dt: 96.77ms | tok/sec: 84651.37\n","start training the model\n","step 0 of training\n","step  1873 | loss: 5.303502 | lr 5.9459e-04 | norm: 0.7977 | dt: 96.53ms | tok/sec: 84866.73\n","start training the model\n","step 0 of training\n","step  1874 | loss: 5.381928 | lr 5.9458e-04 | norm: 0.6250 | dt: 96.71ms | tok/sec: 84710.85\n","start training the model\n","step 0 of training\n","step  1875 | loss: 5.292454 | lr 5.9457e-04 | norm: 0.7111 | dt: 96.94ms | tok/sec: 84506.68\n","start training the model\n","step 0 of training\n","step  1876 | loss: 5.268413 | lr 5.9456e-04 | norm: 0.6498 | dt: 96.90ms | tok/sec: 84542.86\n","start training the model\n","step 0 of training\n","step  1877 | loss: 5.304702 | lr 5.9455e-04 | norm: 0.7049 | dt: 97.12ms | tok/sec: 84352.95\n","start training the model\n","step 0 of training\n","step  1878 | loss: 5.165967 | lr 5.9454e-04 | norm: 0.7287 | dt: 96.67ms | tok/sec: 84743.65\n","start training the model\n","step 0 of training\n","step  1879 | loss: 5.554496 | lr 5.9453e-04 | norm: 1.9688 | dt: 96.77ms | tok/sec: 84658.67\n","start training the model\n","step 0 of training\n","step  1880 | loss: 5.265218 | lr 5.9452e-04 | norm: 1.0205 | dt: 96.16ms | tok/sec: 85193.94\n","start training the model\n","step 0 of training\n","step  1881 | loss: 5.160302 | lr 5.9451e-04 | norm: 0.8842 | dt: 96.22ms | tok/sec: 85142.64\n","start training the model\n","step 0 of training\n","step  1882 | loss: 5.292113 | lr 5.9450e-04 | norm: 1.0093 | dt: 96.48ms | tok/sec: 84908.46\n","start training the model\n","step 0 of training\n","step  1883 | loss: 5.284632 | lr 5.9449e-04 | norm: 0.8139 | dt: 97.33ms | tok/sec: 84163.27\n","start training the model\n","step 0 of training\n","step  1884 | loss: 5.181108 | lr 5.9449e-04 | norm: 0.7062 | dt: 98.90ms | tok/sec: 82831.28\n","start training the model\n","step 0 of training\n","step  1885 | loss: 5.173945 | lr 5.9448e-04 | norm: 0.9795 | dt: 97.33ms | tok/sec: 84168.83\n","start training the model\n","step 0 of training\n","step  1886 | loss: 5.161181 | lr 5.9447e-04 | norm: 0.8010 | dt: 97.92ms | tok/sec: 83657.94\n","start training the model\n","step 0 of training\n","step  1887 | loss: 5.256028 | lr 5.9446e-04 | norm: 0.7485 | dt: 96.87ms | tok/sec: 84566.78\n","start training the model\n","step 0 of training\n","step  1888 | loss: 5.354711 | lr 5.9445e-04 | norm: 0.7673 | dt: 97.74ms | tok/sec: 83812.01\n","start training the model\n","step 0 of training\n","step  1889 | loss: 5.433019 | lr 5.9444e-04 | norm: 0.9218 | dt: 100.40ms | tok/sec: 81595.78\n","start training the model\n","step 0 of training\n","step  1890 | loss: 5.405169 | lr 5.9443e-04 | norm: 1.4468 | dt: 96.21ms | tok/sec: 85150.87\n","start training the model\n","step 0 of training\n","step  1891 | loss: 5.497810 | lr 5.9442e-04 | norm: 1.1602 | dt: 95.88ms | tok/sec: 85442.23\n","start training the model\n","step 0 of training\n","step  1892 | loss: 5.467312 | lr 5.9441e-04 | norm: 1.0601 | dt: 95.92ms | tok/sec: 85407.61\n","start training the model\n","step 0 of training\n","step  1893 | loss: 5.222138 | lr 5.9440e-04 | norm: 1.1365 | dt: 95.92ms | tok/sec: 85403.15\n","start training the model\n","step 0 of training\n","step  1894 | loss: 5.174566 | lr 5.9439e-04 | norm: 1.0137 | dt: 95.48ms | tok/sec: 85798.32\n","start training the model\n","step 0 of training\n","step  1895 | loss: 5.056952 | lr 5.9438e-04 | norm: 1.1808 | dt: 96.16ms | tok/sec: 85195.63\n","start training the model\n","step 0 of training\n","step  1896 | loss: 5.281301 | lr 5.9437e-04 | norm: 0.9441 | dt: 96.19ms | tok/sec: 85162.06\n","start training the model\n","step 0 of training\n","step  1897 | loss: 5.100870 | lr 5.9436e-04 | norm: 1.3893 | dt: 96.94ms | tok/sec: 84506.88\n","start training the model\n","step 0 of training\n","step  1898 | loss: 5.206838 | lr 5.9435e-04 | norm: 0.8803 | dt: 95.79ms | tok/sec: 85517.30\n","start training the model\n","step 0 of training\n","step  1899 | loss: 5.167136 | lr 5.9435e-04 | norm: 0.8484 | dt: 95.64ms | tok/sec: 85653.95\n","start training the model\n","step 0 of training\n","step  1900 | loss: 5.046126 | lr 5.9434e-04 | norm: 0.8840 | dt: 96.88ms | tok/sec: 84556.59\n","start training the model\n","step 0 of training\n","step  1901 | loss: 5.063134 | lr 5.9433e-04 | norm: 0.7334 | dt: 96.45ms | tok/sec: 84930.92\n","start training the model\n","step 0 of training\n","step  1902 | loss: 5.101250 | lr 5.9432e-04 | norm: 0.8900 | dt: 96.74ms | tok/sec: 84679.95\n","start training the model\n","step 0 of training\n","step  1903 | loss: 5.160649 | lr 5.9431e-04 | norm: 0.8813 | dt: 95.98ms | tok/sec: 85355.42\n","start training the model\n","step 0 of training\n","step  1904 | loss: 5.127274 | lr 5.9430e-04 | norm: 0.7206 | dt: 96.02ms | tok/sec: 85313.24\n","start training the model\n","step 0 of training\n","step  1905 | loss: 5.314935 | lr 5.9429e-04 | norm: 0.9264 | dt: 96.52ms | tok/sec: 84875.95\n","start training the model\n","step 0 of training\n","step  1906 | loss: 5.238314 | lr 5.9428e-04 | norm: 0.7611 | dt: 96.21ms | tok/sec: 85145.38\n","start training the model\n","step 0 of training\n","step  1907 | loss: 5.378704 | lr 5.9427e-04 | norm: 0.8495 | dt: 97.60ms | tok/sec: 83933.01\n","start training the model\n","step 0 of training\n","step  1908 | loss: 5.264888 | lr 5.9426e-04 | norm: 1.0424 | dt: 96.17ms | tok/sec: 85181.27\n","start training the model\n","step 0 of training\n","step  1909 | loss: 5.359471 | lr 5.9425e-04 | norm: 0.8465 | dt: 96.33ms | tok/sec: 85044.86\n","start training the model\n","step 0 of training\n","step  1910 | loss: 5.428718 | lr 5.9424e-04 | norm: 0.7800 | dt: 96.86ms | tok/sec: 84574.49\n","start training the model\n","step 0 of training\n","step  1911 | loss: 5.265516 | lr 5.9423e-04 | norm: 1.0518 | dt: 96.48ms | tok/sec: 84908.46\n","start training the model\n","step 0 of training\n","step  1912 | loss: 5.240167 | lr 5.9422e-04 | norm: 0.9637 | dt: 97.11ms | tok/sec: 84354.19\n","start training the model\n","step 0 of training\n","step  1913 | loss: 5.223249 | lr 5.9421e-04 | norm: 0.9397 | dt: 96.52ms | tok/sec: 84869.25\n","start training the model\n","step 0 of training\n","step  1914 | loss: 5.295597 | lr 5.9420e-04 | norm: 0.9898 | dt: 96.54ms | tok/sec: 84854.78\n","start training the model\n","step 0 of training\n","step  1915 | loss: 5.318517 | lr 5.9419e-04 | norm: 0.9330 | dt: 97.01ms | tok/sec: 84446.03\n","start training the model\n","step 0 of training\n","step  1916 | loss: 5.277357 | lr 5.9418e-04 | norm: 0.8509 | dt: 96.62ms | tok/sec: 84785.06\n","start training the model\n","step 0 of training\n","step  1917 | loss: 5.266720 | lr 5.9418e-04 | norm: 1.0837 | dt: 97.20ms | tok/sec: 84282.18\n","start training the model\n","step 0 of training\n","step  1918 | loss: 5.113728 | lr 5.9417e-04 | norm: 1.0557 | dt: 96.56ms | tok/sec: 84841.79\n","start training the model\n","step 0 of training\n","step  1919 | loss: 5.073208 | lr 5.9416e-04 | norm: 0.8774 | dt: 95.39ms | tok/sec: 85877.88\n","start training the model\n","step 0 of training\n","step  1920 | loss: 5.071898 | lr 5.9415e-04 | norm: 0.8674 | dt: 96.17ms | tok/sec: 85179.37\n","start training the model\n","step 0 of training\n","step  1921 | loss: 4.997380 | lr 5.9414e-04 | norm: 0.7950 | dt: 96.07ms | tok/sec: 85274.29\n","start training the model\n","step 0 of training\n","step  1922 | loss: 5.936450 | lr 5.9413e-04 | norm: 1.2531 | dt: 95.93ms | tok/sec: 85391.27\n","start training the model\n","step 0 of training\n","step  1923 | loss: 6.011024 | lr 5.9412e-04 | norm: 1.0640 | dt: 95.61ms | tok/sec: 85685.13\n","start training the model\n","step 0 of training\n","step  1924 | loss: 5.882367 | lr 5.9411e-04 | norm: 1.0389 | dt: 95.89ms | tok/sec: 85427.57\n","start training the model\n","step 0 of training\n","step  1925 | loss: 5.814527 | lr 5.9410e-04 | norm: 1.0357 | dt: 95.67ms | tok/sec: 85629.83\n","start training the model\n","step 0 of training\n","step  1926 | loss: 5.738087 | lr 5.9409e-04 | norm: 0.9929 | dt: 95.74ms | tok/sec: 85561.38\n","start training the model\n","step 0 of training\n","step  1927 | loss: 5.828747 | lr 5.9408e-04 | norm: 0.8160 | dt: 96.70ms | tok/sec: 84719.00\n","start training the model\n","step 0 of training\n","step  1928 | loss: 5.648813 | lr 5.9407e-04 | norm: 1.0209 | dt: 95.35ms | tok/sec: 85912.45\n","start training the model\n","step 0 of training\n","step  1929 | loss: 5.642222 | lr 5.9406e-04 | norm: 0.7586 | dt: 96.21ms | tok/sec: 85148.76\n","start training the model\n","step 0 of training\n","step  1930 | loss: 5.633432 | lr 5.9405e-04 | norm: 0.7122 | dt: 96.33ms | tok/sec: 85039.81\n","start training the model\n","step 0 of training\n","step  1931 | loss: 5.727127 | lr 5.9404e-04 | norm: 1.1490 | dt: 95.90ms | tok/sec: 85419.29\n","start training the model\n","step 0 of training\n","step  1932 | loss: 5.706762 | lr 5.9403e-04 | norm: 0.8255 | dt: 96.88ms | tok/sec: 84559.08\n","start training the model\n","step 0 of training\n","step  1933 | loss: 5.725366 | lr 5.9402e-04 | norm: 0.9064 | dt: 96.63ms | tok/sec: 84774.39\n","start training the model\n","step 0 of training\n","step  1934 | loss: 5.648505 | lr 5.9401e-04 | norm: 1.0756 | dt: 97.17ms | tok/sec: 84305.76\n","start training the model\n","step 0 of training\n","step  1935 | loss: 5.612405 | lr 5.9400e-04 | norm: 0.9623 | dt: 96.58ms | tok/sec: 84823.36\n","start training the model\n","step 0 of training\n","step  1936 | loss: 5.496834 | lr 5.9399e-04 | norm: 1.2062 | dt: 96.56ms | tok/sec: 84840.75\n","start training the model\n","step 0 of training\n","step  1937 | loss: 5.580121 | lr 5.9398e-04 | norm: 1.0965 | dt: 96.77ms | tok/sec: 84650.54\n","start training the model\n","step 0 of training\n","step  1938 | loss: 5.548794 | lr 5.9397e-04 | norm: 1.0056 | dt: 95.74ms | tok/sec: 85565.64\n","start training the model\n","step 0 of training\n","step  1939 | loss: 5.468590 | lr 5.9396e-04 | norm: 1.2788 | dt: 96.48ms | tok/sec: 84904.48\n","start training the model\n","step 0 of training\n","step  1940 | loss: 5.855989 | lr 5.9395e-04 | norm: 2.0733 | dt: 96.28ms | tok/sec: 85081.71\n","start training the model\n","step 0 of training\n","step  1941 | loss: 5.592971 | lr 5.9394e-04 | norm: 2.3420 | dt: 97.05ms | tok/sec: 84407.03\n","start training the model\n","step 0 of training\n","step  1942 | loss: 5.000884 | lr 5.9393e-04 | norm: 2.6209 | dt: 96.20ms | tok/sec: 85152.98\n","start training the model\n","step 0 of training\n","step  1943 | loss: 5.260689 | lr 5.9392e-04 | norm: 1.7253 | dt: 96.61ms | tok/sec: 84795.94\n","start training the model\n","step 0 of training\n","step  1944 | loss: 4.512098 | lr 5.9391e-04 | norm: 1.3320 | dt: 96.25ms | tok/sec: 85114.59\n","start training the model\n","step 0 of training\n","step  1945 | loss: 4.578549 | lr 5.9391e-04 | norm: 1.8686 | dt: 96.22ms | tok/sec: 85135.68\n","start training the model\n","step 0 of training\n","step  1946 | loss: 4.405294 | lr 5.9390e-04 | norm: 1.7542 | dt: 96.57ms | tok/sec: 84831.74\n","start training the model\n","step 0 of training\n","step  1947 | loss: 4.972007 | lr 5.9389e-04 | norm: 1.5249 | dt: 96.91ms | tok/sec: 84529.34\n","start training the model\n","step 0 of training\n","step  1948 | loss: 4.571781 | lr 5.9388e-04 | norm: 1.3471 | dt: 96.68ms | tok/sec: 84731.12\n","start training the model\n","step 0 of training\n","step  1949 | loss: 3.790479 | lr 5.9387e-04 | norm: 1.4005 | dt: 96.33ms | tok/sec: 85041.28\n","start training the model\n","step 0 of training\n","step  1950 | loss: 2.152328 | lr 5.9386e-04 | norm: 1.4219 | dt: 96.55ms | tok/sec: 84848.50\n","start training the model\n","step 0 of training\n","step  1951 | loss: 2.063656 | lr 5.9385e-04 | norm: 0.8900 | dt: 96.77ms | tok/sec: 84658.05\n","start training the model\n","step 0 of training\n","step  1952 | loss: 1.983855 | lr 5.9384e-04 | norm: 1.5391 | dt: 96.34ms | tok/sec: 85033.91\n","start training the model\n","step 0 of training\n","step  1953 | loss: 2.023252 | lr 5.9383e-04 | norm: 0.9984 | dt: 96.53ms | tok/sec: 84863.59\n","start training the model\n","step 0 of training\n","step  1954 | loss: 2.097156 | lr 5.9382e-04 | norm: 0.8079 | dt: 96.68ms | tok/sec: 84730.28\n","start training the model\n","step 0 of training\n","step  1955 | loss: 1.947493 | lr 5.9381e-04 | norm: 0.7563 | dt: 96.95ms | tok/sec: 84492.96\n","start training the model\n","step 0 of training\n","step  1956 | loss: 2.147156 | lr 5.9380e-04 | norm: 0.8342 | dt: 97.75ms | tok/sec: 83809.55\n","start training the model\n","step 0 of training\n","step  1957 | loss: 2.023447 | lr 5.9379e-04 | norm: 0.6942 | dt: 96.05ms | tok/sec: 85292.49\n","start training the model\n","step 0 of training\n","step  1958 | loss: 1.751996 | lr 5.9378e-04 | norm: 0.8374 | dt: 96.80ms | tok/sec: 84628.65\n","start training the model\n","step 0 of training\n","step  1959 | loss: 1.726412 | lr 5.9377e-04 | norm: 0.8651 | dt: 96.88ms | tok/sec: 84562.21\n","start training the model\n","step 0 of training\n","step  1960 | loss: 1.983613 | lr 5.9376e-04 | norm: 0.7077 | dt: 96.08ms | tok/sec: 85265.19\n","start training the model\n","step 0 of training\n","step  1961 | loss: 2.124527 | lr 5.9375e-04 | norm: 0.7694 | dt: 96.85ms | tok/sec: 84587.19\n","start training the model\n","step 0 of training\n","step  1962 | loss: 2.224992 | lr 5.9374e-04 | norm: 1.0728 | dt: 96.61ms | tok/sec: 84792.80\n","start training the model\n","step 0 of training\n","step  1963 | loss: 1.969906 | lr 5.9373e-04 | norm: 0.8551 | dt: 96.83ms | tok/sec: 84603.02\n","start training the model\n","step 0 of training\n","step  1964 | loss: 1.997711 | lr 5.9372e-04 | norm: 0.7471 | dt: 96.80ms | tok/sec: 84628.65\n","start training the model\n","step 0 of training\n","step  1965 | loss: 1.990325 | lr 5.9371e-04 | norm: 0.7518 | dt: 96.62ms | tok/sec: 84783.80\n","start training the model\n","step 0 of training\n","step  1966 | loss: 2.010517 | lr 5.9370e-04 | norm: 0.7628 | dt: 97.25ms | tok/sec: 84239.61\n","start training the model\n","step 0 of training\n","step  1967 | loss: 1.905298 | lr 5.9369e-04 | norm: 0.7081 | dt: 96.54ms | tok/sec: 84852.48\n","start training the model\n","step 0 of training\n","step  1968 | loss: 1.891914 | lr 5.9368e-04 | norm: 0.7356 | dt: 97.16ms | tok/sec: 84315.07\n","start training the model\n","step 0 of training\n","step  1969 | loss: 1.912784 | lr 5.9367e-04 | norm: 0.7679 | dt: 96.66ms | tok/sec: 84746.79\n","start training the model\n","step 0 of training\n","step  1970 | loss: 1.895724 | lr 5.9366e-04 | norm: 0.9110 | dt: 96.68ms | tok/sec: 84733.21\n","start training the model\n","step 0 of training\n","step  1971 | loss: 1.878237 | lr 5.9365e-04 | norm: 1.2331 | dt: 96.86ms | tok/sec: 84577.40\n","start training the model\n","step 0 of training\n","step  1972 | loss: 1.814505 | lr 5.9364e-04 | norm: 0.9449 | dt: 97.05ms | tok/sec: 84410.55\n","start training the model\n","step 0 of training\n","step  1973 | loss: 1.783863 | lr 5.9363e-04 | norm: 0.8403 | dt: 97.87ms | tok/sec: 83706.85\n","start training the model\n","step 0 of training\n","step  1974 | loss: 1.934852 | lr 5.9362e-04 | norm: 1.7138 | dt: 96.55ms | tok/sec: 84845.14\n","start training the model\n","step 0 of training\n","step  1975 | loss: 1.824337 | lr 5.9361e-04 | norm: 1.0025 | dt: 95.95ms | tok/sec: 85380.87\n","start training the model\n","step 0 of training\n","step  1976 | loss: 1.883480 | lr 5.9360e-04 | norm: 1.5620 | dt: 96.61ms | tok/sec: 84797.61\n","start training the model\n","step 0 of training\n","step  1977 | loss: 1.846371 | lr 5.9359e-04 | norm: 0.9813 | dt: 97.15ms | tok/sec: 84326.65\n","start training the model\n","step 0 of training\n","step  1978 | loss: 1.853453 | lr 5.9358e-04 | norm: 1.3905 | dt: 96.40ms | tok/sec: 84979.02\n","start training the model\n","step 0 of training\n","step  1979 | loss: 1.710099 | lr 5.9357e-04 | norm: 1.3803 | dt: 96.46ms | tok/sec: 84922.31\n","start training the model\n","step 0 of training\n","step  1980 | loss: 1.750823 | lr 5.9356e-04 | norm: 0.8580 | dt: 96.35ms | tok/sec: 85018.97\n","start training the model\n","step 0 of training\n","step  1981 | loss: 1.732256 | lr 5.9355e-04 | norm: 1.3758 | dt: 95.46ms | tok/sec: 85814.60\n","start training the model\n","step 0 of training\n","step  1982 | loss: 1.729059 | lr 5.9354e-04 | norm: 0.9062 | dt: 95.52ms | tok/sec: 85762.34\n","start training the model\n","step 0 of training\n","step  1983 | loss: 1.693263 | lr 5.9353e-04 | norm: 1.5715 | dt: 95.51ms | tok/sec: 85770.69\n","start training the model\n","step 0 of training\n","step  1984 | loss: 1.695602 | lr 5.9352e-04 | norm: 1.3895 | dt: 95.55ms | tok/sec: 85734.74\n","start training the model\n","step 0 of training\n","step  1985 | loss: 1.736073 | lr 5.9351e-04 | norm: 1.1733 | dt: 96.52ms | tok/sec: 84876.16\n","start training the model\n","step 0 of training\n","step  1986 | loss: 2.587011 | lr 5.9350e-04 | norm: 1.1637 | dt: 95.88ms | tok/sec: 85443.08\n","start training the model\n","step 0 of training\n","step  1987 | loss: 5.938550 | lr 5.9349e-04 | norm: 2.8215 | dt: 96.54ms | tok/sec: 84853.74\n","start training the model\n","step 0 of training\n","step  1988 | loss: 5.383139 | lr 5.9348e-04 | norm: 1.3566 | dt: 96.17ms | tok/sec: 85182.11\n","start training the model\n","step 0 of training\n","step  1989 | loss: 5.383519 | lr 5.9347e-04 | norm: 1.2546 | dt: 96.59ms | tok/sec: 84815.82\n","start training the model\n","step 0 of training\n","step  1990 | loss: 5.267302 | lr 5.9346e-04 | norm: 1.0782 | dt: 96.86ms | tok/sec: 84573.03\n","start training the model\n","step 0 of training\n","step  1991 | loss: 5.190203 | lr 5.9345e-04 | norm: 0.9352 | dt: 96.36ms | tok/sec: 85015.82\n","start training the model\n","step 0 of training\n","step  1992 | loss: 5.179408 | lr 5.9344e-04 | norm: 0.7246 | dt: 96.80ms | tok/sec: 84631.98\n","start training the model\n","step 0 of training\n","step  1993 | loss: 5.209803 | lr 5.9343e-04 | norm: 0.7788 | dt: 96.62ms | tok/sec: 84781.50\n","start training the model\n","step 0 of training\n","step  1994 | loss: 5.115071 | lr 5.9342e-04 | norm: 0.8270 | dt: 96.32ms | tok/sec: 85049.28\n","start training the model\n","step 0 of training\n","step  1995 | loss: 5.263830 | lr 5.9341e-04 | norm: 0.8223 | dt: 97.00ms | tok/sec: 84456.20\n","start training the model\n","step 0 of training\n","step  1996 | loss: 5.300756 | lr 5.9340e-04 | norm: 0.7924 | dt: 96.11ms | tok/sec: 85237.69\n","start training the model\n","step 0 of training\n","step  1997 | loss: 5.248806 | lr 5.9339e-04 | norm: 0.7334 | dt: 97.36ms | tok/sec: 84139.57\n","start training the model\n","step 0 of training\n","step  1998 | loss: 5.206388 | lr 5.9338e-04 | norm: 0.7879 | dt: 96.52ms | tok/sec: 84870.71\n","start training the model\n","step 0 of training\n","step  1999 | loss: 5.239415 | lr 5.9337e-04 | norm: 0.8369 | dt: 96.42ms | tok/sec: 84957.80\n","validation loss: 5.9392\n","rank 0 sample 0: [CLS] دردم از یار است و درمان نیز هم[SEP] به سر می زند[BOB][BOM] بر او باد خوش[BOB][BOM] گر همه آن چه می دهد[BOM] هر دم تو\n","rank 0 sample 1: [CLS] دردم از یار است و درمان نیز هم[SEP] همه چون توان کرد[BOM] آن را سر به[BOM] صوفی بیار[BOM] از جان در میخانه ت می به سر\n","rank 0 sample 2: [CLS] دردم از یار است و درمان نیز هم[SEP] این که باشد و فضل حق نتوان[BOB][BOM] تو نیز که ز در[BOM] که این نکته ن سر زلف\n","rank 0 sample 3: [CLS] دردم از یار است و درمان نیز هم[SEP] نه است[BOM] از این که به[BOB][BOM] نقش تو می بر خود ز تو سرو روان می زد دل و\n","start training the model\n","step 0 of training\n","step  2000 | loss: 5.331433 | lr 5.9336e-04 | norm: 0.6380 | dt: 1320.90ms | tok/sec: 6201.81\n","start training the model\n","step 0 of training\n","step  2001 | loss: 5.244688 | lr 5.9335e-04 | norm: 0.7828 | dt: 95.66ms | tok/sec: 85633.03\n","start training the model\n","step 0 of training\n","step  2002 | loss: 5.217940 | lr 5.9334e-04 | norm: 0.6791 | dt: 95.98ms | tok/sec: 85350.54\n","start training the model\n","step 0 of training\n","step  2003 | loss: 5.258813 | lr 5.9333e-04 | norm: 0.7199 | dt: 96.38ms | tok/sec: 84995.84\n","start training the model\n","step 0 of training\n","step  2004 | loss: 5.112418 | lr 5.9332e-04 | norm: 0.7466 | dt: 96.24ms | tok/sec: 85119.44\n","start training the model\n","step 0 of training\n","step  2005 | loss: 5.491765 | lr 5.9331e-04 | norm: 1.5110 | dt: 96.51ms | tok/sec: 84885.39\n","start training the model\n","step 0 of training\n","step  2006 | loss: 5.215445 | lr 5.9330e-04 | norm: 1.2973 | dt: 95.84ms | tok/sec: 85476.03\n","start training the model\n","step 0 of training\n","step  2007 | loss: 5.113942 | lr 5.9329e-04 | norm: 0.9421 | dt: 97.19ms | tok/sec: 84291.07\n","start training the model\n","step 0 of training\n","step  2008 | loss: 5.237538 | lr 5.9328e-04 | norm: 0.8037 | dt: 96.14ms | tok/sec: 85206.40\n","start training the model\n","step 0 of training\n","step  2009 | loss: 5.250732 | lr 5.9326e-04 | norm: 0.9363 | dt: 94.09ms | tok/sec: 87070.00\n","start training the model\n","step 0 of training\n","step  2010 | loss: 5.128709 | lr 5.9325e-04 | norm: 0.7555 | dt: 95.38ms | tok/sec: 85884.96\n","start training the model\n","step 0 of training\n","step  2011 | loss: 5.099291 | lr 5.9324e-04 | norm: 0.6535 | dt: 95.84ms | tok/sec: 85479.00\n","start training the model\n","step 0 of training\n","step  2012 | loss: 5.086418 | lr 5.9323e-04 | norm: 0.7445 | dt: 96.44ms | tok/sec: 84942.05\n","start training the model\n","step 0 of training\n","step  2013 | loss: 5.186775 | lr 5.9322e-04 | norm: 0.7914 | dt: 96.82ms | tok/sec: 84612.39\n","start training the model\n","step 0 of training\n","step  2014 | loss: 5.292264 | lr 5.9321e-04 | norm: 0.7359 | dt: 96.22ms | tok/sec: 85140.53\n","start training the model\n","step 0 of training\n","step  2015 | loss: 5.363104 | lr 5.9320e-04 | norm: 0.7764 | dt: 100.21ms | tok/sec: 81744.48\n","start training the model\n","step 0 of training\n","step  2016 | loss: 5.335725 | lr 5.9319e-04 | norm: 1.5494 | dt: 95.79ms | tok/sec: 85517.94\n","start training the model\n","step 0 of training\n","step  2017 | loss: 5.429834 | lr 5.9318e-04 | norm: 1.1793 | dt: 95.58ms | tok/sec: 85706.72\n","start training the model\n","step 0 of training\n","step  2018 | loss: 5.420481 | lr 5.9317e-04 | norm: 1.2759 | dt: 95.92ms | tok/sec: 85401.24\n","start training the model\n","step 0 of training\n","step  2019 | loss: 5.181903 | lr 5.9316e-04 | norm: 1.1068 | dt: 96.05ms | tok/sec: 85287.41\n","start training the model\n","step 0 of training\n","step  2020 | loss: 5.131044 | lr 5.9315e-04 | norm: 0.8692 | dt: 96.12ms | tok/sec: 85226.48\n","start training the model\n","step 0 of training\n","step  2021 | loss: 4.992585 | lr 5.9314e-04 | norm: 1.1240 | dt: 96.68ms | tok/sec: 84735.50\n","start training the model\n","step 0 of training\n","step  2022 | loss: 5.219087 | lr 5.9313e-04 | norm: 1.0072 | dt: 96.45ms | tok/sec: 84936.38\n","start training the model\n","step 0 of training\n","step  2023 | loss: 5.052222 | lr 5.9312e-04 | norm: 1.0837 | dt: 97.12ms | tok/sec: 84349.01\n","start training the model\n","step 0 of training\n","step  2024 | loss: 5.163505 | lr 5.9311e-04 | norm: 0.9036 | dt: 96.31ms | tok/sec: 85061.91\n","start training the model\n","step 0 of training\n","step  2025 | loss: 5.124419 | lr 5.9310e-04 | norm: 1.0426 | dt: 96.48ms | tok/sec: 84911.19\n","start training the model\n","step 0 of training\n","step  2026 | loss: 4.967232 | lr 5.9309e-04 | norm: 0.8524 | dt: 96.90ms | tok/sec: 84544.31\n","start training the model\n","step 0 of training\n","step  2027 | loss: 5.020610 | lr 5.9308e-04 | norm: 1.0380 | dt: 95.16ms | tok/sec: 86089.82\n","start training the model\n","step 0 of training\n","step  2028 | loss: 5.036903 | lr 5.9307e-04 | norm: 0.8773 | dt: 95.90ms | tok/sec: 85422.26\n","start training the model\n","step 0 of training\n","step  2029 | loss: 5.110380 | lr 5.9306e-04 | norm: 0.9094 | dt: 95.73ms | tok/sec: 85571.18\n","start training the model\n","step 0 of training\n","step  2030 | loss: 5.089025 | lr 5.9305e-04 | norm: 0.7620 | dt: 97.08ms | tok/sec: 84382.98\n","start training the model\n","step 0 of training\n","step  2031 | loss: 5.251299 | lr 5.9304e-04 | norm: 0.7139 | dt: 96.27ms | tok/sec: 85096.88\n","start training the model\n","step 0 of training\n","step  2032 | loss: 5.191303 | lr 5.9303e-04 | norm: 0.7076 | dt: 96.09ms | tok/sec: 85253.34\n","start training the model\n","step 0 of training\n","step  2033 | loss: 5.313009 | lr 5.9302e-04 | norm: 0.7023 | dt: 95.56ms | tok/sec: 85727.46\n","start training the model\n","step 0 of training\n","step  2034 | loss: 5.200881 | lr 5.9301e-04 | norm: 0.7531 | dt: 95.43ms | tok/sec: 85840.33\n","start training the model\n","step 0 of training\n","step  2035 | loss: 5.300372 | lr 5.9300e-04 | norm: 0.9377 | dt: 96.48ms | tok/sec: 84910.98\n","start training the model\n","step 0 of training\n","step  2036 | loss: 5.375384 | lr 5.9299e-04 | norm: 0.7889 | dt: 95.69ms | tok/sec: 85610.20\n","start training the model\n","step 0 of training\n","step  2037 | loss: 5.210514 | lr 5.9298e-04 | norm: 1.1944 | dt: 95.78ms | tok/sec: 85531.99\n","start training the model\n","step 0 of training\n","step  2038 | loss: 5.165700 | lr 5.9296e-04 | norm: 0.9076 | dt: 95.99ms | tok/sec: 85339.31\n","start training the model\n","step 0 of training\n","step  2039 | loss: 5.150095 | lr 5.9295e-04 | norm: 0.7589 | dt: 96.33ms | tok/sec: 85037.49\n","start training the model\n","step 0 of training\n","step  2040 | loss: 5.238367 | lr 5.9294e-04 | norm: 0.9732 | dt: 96.77ms | tok/sec: 84653.46\n","start training the model\n","step 0 of training\n","step  2041 | loss: 5.250712 | lr 5.9293e-04 | norm: 1.0188 | dt: 97.01ms | tok/sec: 84446.65\n","start training the model\n","step 0 of training\n","step  2042 | loss: 5.211224 | lr 5.9292e-04 | norm: 0.9085 | dt: 97.07ms | tok/sec: 84392.93\n","start training the model\n","step 0 of training\n","step  2043 | loss: 5.184304 | lr 5.9291e-04 | norm: 0.8742 | dt: 96.06ms | tok/sec: 85282.12\n","start training the model\n","step 0 of training\n","step  2044 | loss: 5.038527 | lr 5.9290e-04 | norm: 1.0540 | dt: 97.04ms | tok/sec: 84421.13\n","start training the model\n","step 0 of training\n","step  2045 | loss: 4.994751 | lr 5.9289e-04 | norm: 0.9252 | dt: 97.01ms | tok/sec: 84444.37\n","start training the model\n","step 0 of training\n","step  2046 | loss: 5.010559 | lr 5.9288e-04 | norm: 0.7737 | dt: 95.70ms | tok/sec: 85599.96\n","start training the model\n","step 0 of training\n","step  2047 | loss: 4.948186 | lr 5.9287e-04 | norm: 0.8082 | dt: 96.48ms | tok/sec: 84904.48\n","start training the model\n","step 0 of training\n","step  2048 | loss: 5.866872 | lr 5.9286e-04 | norm: 1.2220 | dt: 96.64ms | tok/sec: 84766.44\n","start training the model\n","step 0 of training\n","step  2049 | loss: 5.926495 | lr 5.9285e-04 | norm: 1.0879 | dt: 96.57ms | tok/sec: 84830.48\n","start training the model\n","step 0 of training\n","step  2050 | loss: 5.768029 | lr 5.9284e-04 | norm: 0.8146 | dt: 96.40ms | tok/sec: 84975.45\n","start training the model\n","step 0 of training\n","step  2051 | loss: 5.730948 | lr 5.9283e-04 | norm: 1.0942 | dt: 96.75ms | tok/sec: 84674.95\n","start training the model\n","step 0 of training\n","step  2052 | loss: 5.664257 | lr 5.9282e-04 | norm: 1.1442 | dt: 96.50ms | tok/sec: 84887.49\n","start training the model\n","step 0 of training\n","step  2053 | loss: 5.731483 | lr 5.9281e-04 | norm: 0.8668 | dt: 96.30ms | tok/sec: 85065.49\n","start training the model\n","step 0 of training\n","step  2054 | loss: 5.574586 | lr 5.9280e-04 | norm: 0.9955 | dt: 96.83ms | tok/sec: 84599.06\n","start training the model\n","step 0 of training\n","step  2055 | loss: 5.564282 | lr 5.9279e-04 | norm: 0.8147 | dt: 96.72ms | tok/sec: 84697.07\n","start training the model\n","step 0 of training\n","step  2056 | loss: 5.569548 | lr 5.9277e-04 | norm: 0.7584 | dt: 97.07ms | tok/sec: 84392.72\n","start training the model\n","step 0 of training\n","step  2057 | loss: 5.661407 | lr 5.9276e-04 | norm: 1.1643 | dt: 97.24ms | tok/sec: 84242.92\n","start training the model\n","step 0 of training\n","step  2058 | loss: 5.611335 | lr 5.9275e-04 | norm: 0.7820 | dt: 96.52ms | tok/sec: 84871.55\n","start training the model\n","step 0 of training\n","step  2059 | loss: 5.635202 | lr 5.9274e-04 | norm: 0.8760 | dt: 97.47ms | tok/sec: 84049.62\n","start training the model\n","step 0 of training\n","step  2060 | loss: 5.583640 | lr 5.9273e-04 | norm: 0.9287 | dt: 96.33ms | tok/sec: 85036.65\n","start training the model\n","step 0 of training\n","step  2061 | loss: 5.558116 | lr 5.9272e-04 | norm: 0.9721 | dt: 96.51ms | tok/sec: 84878.47\n","start training the model\n","step 0 of training\n","step  2062 | loss: 5.443688 | lr 5.9271e-04 | norm: 0.9373 | dt: 96.82ms | tok/sec: 84611.56\n","start training the model\n","step 0 of training\n","step  2063 | loss: 5.527403 | lr 5.9270e-04 | norm: 0.9484 | dt: 96.47ms | tok/sec: 84917.07\n","start training the model\n","step 0 of training\n","step  2064 | loss: 5.487059 | lr 5.9269e-04 | norm: 1.2357 | dt: 95.75ms | tok/sec: 85557.76\n","start training the model\n","step 0 of training\n","step  2065 | loss: 5.434360 | lr 5.9268e-04 | norm: 0.9745 | dt: 97.22ms | tok/sec: 84259.65\n","start training the model\n","step 0 of training\n","step  2066 | loss: 5.786242 | lr 5.9267e-04 | norm: 1.9491 | dt: 98.52ms | tok/sec: 83149.79\n","start training the model\n","step 0 of training\n","step  2067 | loss: 5.538722 | lr 5.9266e-04 | norm: 2.2615 | dt: 97.17ms | tok/sec: 84305.96\n","start training the model\n","step 0 of training\n","step  2068 | loss: 4.981998 | lr 5.9265e-04 | norm: 2.9685 | dt: 97.43ms | tok/sec: 84080.89\n","start training the model\n","step 0 of training\n","step  2069 | loss: 5.143127 | lr 5.9264e-04 | norm: 1.5943 | dt: 98.39ms | tok/sec: 83261.21\n","start training the model\n","step 0 of training\n","step  2070 | loss: 4.386007 | lr 5.9263e-04 | norm: 1.1749 | dt: 97.38ms | tok/sec: 84122.47\n","start training the model\n","step 0 of training\n","step  2071 | loss: 4.500787 | lr 5.9261e-04 | norm: 1.9303 | dt: 96.51ms | tok/sec: 84880.78\n","start training the model\n","step 0 of training\n","step  2072 | loss: 4.372391 | lr 5.9260e-04 | norm: 2.2240 | dt: 96.74ms | tok/sec: 84682.88\n","start training the model\n","step 0 of training\n","step  2073 | loss: 4.866015 | lr 5.9259e-04 | norm: 1.4000 | dt: 98.63ms | tok/sec: 83056.73\n","start training the model\n","step 0 of training\n","step  2074 | loss: 4.533451 | lr 5.9258e-04 | norm: 1.7714 | dt: 96.36ms | tok/sec: 85010.77\n","start training the model\n","step 0 of training\n","step  2075 | loss: 3.797533 | lr 5.9257e-04 | norm: 1.4931 | dt: 97.71ms | tok/sec: 83842.48\n","start training the model\n","step 0 of training\n","step  2076 | loss: 2.226027 | lr 5.9256e-04 | norm: 1.9799 | dt: 96.24ms | tok/sec: 85116.91\n","start training the model\n","step 0 of training\n","step  2077 | loss: 2.072469 | lr 5.9255e-04 | norm: 1.3330 | dt: 97.36ms | tok/sec: 84141.42\n","start training the model\n","step 0 of training\n","step  2078 | loss: 1.971734 | lr 5.9254e-04 | norm: 1.0216 | dt: 95.79ms | tok/sec: 85524.53\n","start training the model\n","step 0 of training\n","step  2079 | loss: 2.013494 | lr 5.9253e-04 | norm: 1.2633 | dt: 96.07ms | tok/sec: 85275.13\n","start training the model\n","step 0 of training\n","step  2080 | loss: 2.076410 | lr 5.9252e-04 | norm: 0.9979 | dt: 96.95ms | tok/sec: 84498.57\n","start training the model\n","step 0 of training\n","step  2081 | loss: 1.926153 | lr 5.9251e-04 | norm: 1.0240 | dt: 96.33ms | tok/sec: 85041.70\n","start training the model\n","step 0 of training\n","step  2082 | loss: 2.129448 | lr 5.9250e-04 | norm: 0.9017 | dt: 98.33ms | tok/sec: 83314.51\n","start training the model\n","step 0 of training\n","step  2083 | loss: 1.997378 | lr 5.9249e-04 | norm: 1.3044 | dt: 95.96ms | tok/sec: 85371.75\n","start training the model\n","step 0 of training\n","step  2084 | loss: 1.722733 | lr 5.9247e-04 | norm: 1.0005 | dt: 97.04ms | tok/sec: 84420.92\n","start training the model\n","step 0 of training\n","step  2085 | loss: 1.702303 | lr 5.9246e-04 | norm: 1.1051 | dt: 97.35ms | tok/sec: 84148.02\n","start training the model\n","step 0 of training\n","step  2086 | loss: 1.975977 | lr 5.9245e-04 | norm: 1.0182 | dt: 96.59ms | tok/sec: 84811.01\n","start training the model\n","step 0 of training\n","step  2087 | loss: 2.116961 | lr 5.9244e-04 | norm: 1.0314 | dt: 96.20ms | tok/sec: 85157.41\n","start training the model\n","step 0 of training\n","step  2088 | loss: 2.229094 | lr 5.9243e-04 | norm: 1.7285 | dt: 97.32ms | tok/sec: 84177.50\n","start training the model\n","step 0 of training\n","step  2089 | loss: 1.968292 | lr 5.9242e-04 | norm: 0.7948 | dt: 96.23ms | tok/sec: 85133.15\n","start training the model\n","step 0 of training\n","step  2090 | loss: 2.004321 | lr 5.9241e-04 | norm: 1.0367 | dt: 97.23ms | tok/sec: 84253.66\n","start training the model\n","step 0 of training\n","step  2091 | loss: 1.998083 | lr 5.9240e-04 | norm: 1.1454 | dt: 96.51ms | tok/sec: 84884.34\n","start training the model\n","step 0 of training\n","step  2092 | loss: 2.010490 | lr 5.9239e-04 | norm: 0.7136 | dt: 97.08ms | tok/sec: 84383.61\n","start training the model\n","step 0 of training\n","step  2093 | loss: 1.899651 | lr 5.9238e-04 | norm: 0.9155 | dt: 96.63ms | tok/sec: 84775.02\n","start training the model\n","step 0 of training\n","step  2094 | loss: 1.902667 | lr 5.9237e-04 | norm: 1.1032 | dt: 95.64ms | tok/sec: 85652.67\n","start training the model\n","step 0 of training\n","step  2095 | loss: 1.897820 | lr 5.9236e-04 | norm: 0.6472 | dt: 95.99ms | tok/sec: 85343.97\n","start training the model\n","step 0 of training\n","step  2096 | loss: 1.879680 | lr 5.9234e-04 | norm: 0.9595 | dt: 96.26ms | tok/sec: 85102.57\n","start training the model\n","step 0 of training\n","step  2097 | loss: 1.866479 | lr 5.9233e-04 | norm: 0.7111 | dt: 97.18ms | tok/sec: 84293.76\n","start training the model\n","step 0 of training\n","step  2098 | loss: 1.791413 | lr 5.9232e-04 | norm: 0.7061 | dt: 96.04ms | tok/sec: 85295.67\n","start training the model\n","step 0 of training\n","step  2099 | loss: 1.771848 | lr 5.9231e-04 | norm: 0.9435 | dt: 96.95ms | tok/sec: 84494.00\n","start training the model\n","step 0 of training\n","step  2100 | loss: 1.907597 | lr 5.9230e-04 | norm: 0.5118 | dt: 96.82ms | tok/sec: 84609.89\n","start training the model\n","step 0 of training\n","step  2101 | loss: 1.803130 | lr 5.9229e-04 | norm: 0.8478 | dt: 96.00ms | tok/sec: 85332.10\n","start training the model\n","step 0 of training\n","step  2102 | loss: 1.875899 | lr 5.9228e-04 | norm: 0.9328 | dt: 96.84ms | tok/sec: 84593.43\n","start training the model\n","step 0 of training\n","step  2103 | loss: 1.816752 | lr 5.9227e-04 | norm: 0.4944 | dt: 96.87ms | tok/sec: 84563.04\n","start training the model\n","step 0 of training\n","step  2104 | loss: 1.829977 | lr 5.9226e-04 | norm: 0.9489 | dt: 97.03ms | tok/sec: 84430.67\n","start training the model\n","step 0 of training\n","step  2105 | loss: 1.690684 | lr 5.9225e-04 | norm: 0.8106 | dt: 97.40ms | tok/sec: 84106.41\n","start training the model\n","step 0 of training\n","step  2106 | loss: 1.713858 | lr 5.9223e-04 | norm: 0.9316 | dt: 97.81ms | tok/sec: 83757.05\n","start training the model\n","step 0 of training\n","step  2107 | loss: 1.705820 | lr 5.9222e-04 | norm: 0.9571 | dt: 97.70ms | tok/sec: 83848.21\n","start training the model\n","step 0 of training\n","step  2108 | loss: 1.703410 | lr 5.9221e-04 | norm: 0.9231 | dt: 96.68ms | tok/sec: 84735.29\n","start training the model\n","step 0 of training\n","step  2109 | loss: 1.667638 | lr 5.9220e-04 | norm: 1.2908 | dt: 96.92ms | tok/sec: 84525.39\n","start training the model\n","step 0 of training\n","step  2110 | loss: 1.666666 | lr 5.9219e-04 | norm: 0.8211 | dt: 96.90ms | tok/sec: 84542.65\n","start training the model\n","step 0 of training\n","step  2111 | loss: 1.704022 | lr 5.9218e-04 | norm: 1.1029 | dt: 96.66ms | tok/sec: 84754.11\n","start training the model\n","step 0 of training\n","step  2112 | loss: 2.540253 | lr 5.9217e-04 | norm: 0.9548 | dt: 97.42ms | tok/sec: 84089.73\n","start training the model\n","step 0 of training\n","step  2113 | loss: 5.882372 | lr 5.9216e-04 | norm: 2.8633 | dt: 96.43ms | tok/sec: 84957.17\n","start training the model\n","step 0 of training\n","step  2114 | loss: 5.321865 | lr 5.9215e-04 | norm: 1.8725 | dt: 97.10ms | tok/sec: 84363.92\n","start training the model\n","step 0 of training\n","step  2115 | loss: 5.286430 | lr 5.9214e-04 | norm: 1.1688 | dt: 96.08ms | tok/sec: 85263.07\n","start training the model\n","step 0 of training\n","step  2116 | loss: 5.185864 | lr 5.9212e-04 | norm: 1.0098 | dt: 95.95ms | tok/sec: 85381.51\n","start training the model\n","step 0 of training\n","step  2117 | loss: 5.109752 | lr 5.9211e-04 | norm: 0.9287 | dt: 96.70ms | tok/sec: 84715.24\n","start training the model\n","step 0 of training\n","step  2118 | loss: 5.096989 | lr 5.9210e-04 | norm: 0.8664 | dt: 96.00ms | tok/sec: 85331.47\n","start training the model\n","step 0 of training\n","step  2119 | loss: 5.135162 | lr 5.9209e-04 | norm: 0.8723 | dt: 96.93ms | tok/sec: 84512.70\n","start training the model\n","step 0 of training\n","step  2120 | loss: 5.043314 | lr 5.9208e-04 | norm: 0.8627 | dt: 95.96ms | tok/sec: 85366.66\n","start training the model\n","step 0 of training\n","step  2121 | loss: 5.201188 | lr 5.9207e-04 | norm: 0.7644 | dt: 96.64ms | tok/sec: 84770.63\n","start training the model\n","step 0 of training\n","step  2122 | loss: 5.237628 | lr 5.9206e-04 | norm: 0.9054 | dt: 96.26ms | tok/sec: 85098.99\n","start training the model\n","step 0 of training\n","step  2123 | loss: 5.195697 | lr 5.9205e-04 | norm: 0.7627 | dt: 95.65ms | tok/sec: 85642.42\n","start training the model\n","step 0 of training\n","step  2124 | loss: 5.151701 | lr 5.9204e-04 | norm: 0.6442 | dt: 96.25ms | tok/sec: 85109.11\n","start training the model\n","step 0 of training\n","step  2125 | loss: 5.202853 | lr 5.9202e-04 | norm: 0.9113 | dt: 96.35ms | tok/sec: 85021.71\n","start training the model\n","step 0 of training\n","step  2126 | loss: 5.294872 | lr 5.9201e-04 | norm: 0.7778 | dt: 95.86ms | tok/sec: 85457.32\n","start training the model\n","step 0 of training\n","step  2127 | loss: 5.187263 | lr 5.9200e-04 | norm: 0.8552 | dt: 97.27ms | tok/sec: 84218.14\n","start training the model\n","step 0 of training\n","step  2128 | loss: 5.168098 | lr 5.9199e-04 | norm: 0.7944 | dt: 96.10ms | tok/sec: 85242.76\n","start training the model\n","step 0 of training\n","step  2129 | loss: 5.185533 | lr 5.9198e-04 | norm: 0.7711 | dt: 97.00ms | tok/sec: 84454.75\n","start training the model\n","step 0 of training\n","step  2130 | loss: 5.041471 | lr 5.9197e-04 | norm: 1.0210 | dt: 96.00ms | tok/sec: 85331.04\n","start training the model\n","step 0 of training\n","step  2131 | loss: 5.389015 | lr 5.9196e-04 | norm: 1.2014 | dt: 96.73ms | tok/sec: 84685.17\n","start training the model\n","step 0 of training\n","step  2132 | loss: 5.118804 | lr 5.9195e-04 | norm: 1.1301 | dt: 96.52ms | tok/sec: 84869.46\n","start training the model\n","step 0 of training\n","step  2133 | loss: 5.014724 | lr 5.9194e-04 | norm: 0.7467 | dt: 96.39ms | tok/sec: 84990.79\n","start training the model\n","step 0 of training\n","step  2134 | loss: 5.175623 | lr 5.9192e-04 | norm: 0.8301 | dt: 97.04ms | tok/sec: 84422.79\n","start training the model\n","step 0 of training\n","step  2135 | loss: 5.209870 | lr 5.9191e-04 | norm: 0.8607 | dt: 96.84ms | tok/sec: 84597.18\n","start training the model\n","step 0 of training\n","step  2136 | loss: 5.096571 | lr 5.9190e-04 | norm: 0.7537 | dt: 97.54ms | tok/sec: 83985.53\n","start training the model\n","step 0 of training\n","step  2137 | loss: 5.054981 | lr 5.9189e-04 | norm: 0.6815 | dt: 96.60ms | tok/sec: 84805.57\n","start training the model\n","step 0 of training\n","step  2138 | loss: 5.017491 | lr 5.9188e-04 | norm: 0.7959 | dt: 95.47ms | tok/sec: 85803.46\n","start training the model\n","step 0 of training\n","step  2139 | loss: 5.123307 | lr 5.9187e-04 | norm: 0.8176 | dt: 96.91ms | tok/sec: 84533.50\n","start training the model\n","step 0 of training\n","step  2140 | loss: 5.235777 | lr 5.9186e-04 | norm: 0.8418 | dt: 97.01ms | tok/sec: 84448.31\n","start training the model\n","step 0 of training\n","step  2141 | loss: 5.298578 | lr 5.9185e-04 | norm: 1.2475 | dt: 101.10ms | tok/sec: 81029.47\n","start training the model\n","step 0 of training\n","step  2142 | loss: 5.301207 | lr 5.9183e-04 | norm: 1.2941 | dt: 97.27ms | tok/sec: 84221.24\n","start training the model\n","step 0 of training\n","step  2143 | loss: 5.355964 | lr 5.9182e-04 | norm: 1.0752 | dt: 97.18ms | tok/sec: 84294.38\n","start training the model\n","step 0 of training\n","step  2144 | loss: 5.374914 | lr 5.9181e-04 | norm: 1.1734 | dt: 97.74ms | tok/sec: 83816.71\n","start training the model\n","step 0 of training\n","step  2145 | loss: 5.103146 | lr 5.9180e-04 | norm: 1.0817 | dt: 97.15ms | tok/sec: 84319.82\n","start training the model\n","step 0 of training\n","step  2146 | loss: 5.087794 | lr 5.9179e-04 | norm: 1.0580 | dt: 96.53ms | tok/sec: 84864.21\n","start training the model\n","step 0 of training\n","step  2147 | loss: 4.934203 | lr 5.9178e-04 | norm: 1.0905 | dt: 96.37ms | tok/sec: 85004.67\n","start training the model\n","step 0 of training\n","step  2148 | loss: 5.136990 | lr 5.9177e-04 | norm: 0.8133 | dt: 96.78ms | tok/sec: 84642.61\n","start training the model\n","step 0 of training\n","step  2149 | loss: 4.960196 | lr 5.9176e-04 | norm: 0.9046 | dt: 97.51ms | tok/sec: 84014.28\n","start training the model\n","step 0 of training\n","step  2150 | loss: 5.090490 | lr 5.9174e-04 | norm: 1.0000 | dt: 96.78ms | tok/sec: 84642.82\n","start training the model\n","step 0 of training\n","step  2151 | loss: 5.059459 | lr 5.9173e-04 | norm: 0.8453 | dt: 97.12ms | tok/sec: 84351.08\n","start training the model\n","step 0 of training\n","step  2152 | loss: 4.926445 | lr 5.9172e-04 | norm: 0.9725 | dt: 96.89ms | tok/sec: 84550.97\n","start training the model\n","step 0 of training\n","step  2153 | loss: 4.970869 | lr 5.9171e-04 | norm: 0.8992 | dt: 97.55ms | tok/sec: 83973.21\n","start training the model\n","step 0 of training\n","step  2154 | loss: 4.965044 | lr 5.9170e-04 | norm: 0.8828 | dt: 95.73ms | tok/sec: 85570.54\n","start training the model\n","step 0 of training\n","step  2155 | loss: 5.021236 | lr 5.9169e-04 | norm: 0.8927 | dt: 96.52ms | tok/sec: 84870.50\n","start training the model\n","step 0 of training\n","step  2156 | loss: 5.039933 | lr 5.9168e-04 | norm: 0.9185 | dt: 96.41ms | tok/sec: 84966.62\n","start training the model\n","step 0 of training\n","step  2157 | loss: 5.226035 | lr 5.9166e-04 | norm: 0.9241 | dt: 95.99ms | tok/sec: 85339.73\n","start training the model\n","step 0 of training\n","step  2158 | loss: 5.150403 | lr 5.9165e-04 | norm: 0.6640 | dt: 96.55ms | tok/sec: 84849.75\n","start training the model\n","step 0 of training\n","step  2159 | loss: 5.266072 | lr 5.9164e-04 | norm: 0.8387 | dt: 96.54ms | tok/sec: 84852.48\n","start training the model\n","step 0 of training\n","step  2160 | loss: 5.158508 | lr 5.9163e-04 | norm: 0.7986 | dt: 96.84ms | tok/sec: 84594.68\n","start training the model\n","step 0 of training\n","step  2161 | loss: 5.235664 | lr 5.9162e-04 | norm: 0.9390 | dt: 95.90ms | tok/sec: 85421.84\n","start training the model\n","step 0 of training\n","step  2162 | loss: 5.310233 | lr 5.9161e-04 | norm: 0.9029 | dt: 96.66ms | tok/sec: 84752.64\n","start training the model\n","step 0 of training\n","step  2163 | loss: 5.137984 | lr 5.9160e-04 | norm: 0.9770 | dt: 96.42ms | tok/sec: 84962.00\n","start training the model\n","step 0 of training\n","step  2164 | loss: 5.078299 | lr 5.9158e-04 | norm: 0.9436 | dt: 96.41ms | tok/sec: 84974.61\n","start training the model\n","step 0 of training\n","step  2165 | loss: 5.068505 | lr 5.9157e-04 | norm: 0.8379 | dt: 96.92ms | tok/sec: 84521.02\n","start training the model\n","step 0 of training\n","step  2166 | loss: 5.164566 | lr 5.9156e-04 | norm: 1.1536 | dt: 96.77ms | tok/sec: 84657.63\n","start training the model\n","step 0 of training\n","step  2167 | loss: 5.137913 | lr 5.9155e-04 | norm: 0.9499 | dt: 97.62ms | tok/sec: 83915.79\n","start training the model\n","step 0 of training\n","step  2168 | loss: 5.096067 | lr 5.9154e-04 | norm: 0.8124 | dt: 96.11ms | tok/sec: 85237.48\n","start training the model\n","step 0 of training\n","step  2169 | loss: 5.064552 | lr 5.9153e-04 | norm: 0.8139 | dt: 96.76ms | tok/sec: 84664.51\n","start training the model\n","step 0 of training\n","step  2170 | loss: 4.918258 | lr 5.9152e-04 | norm: 0.9197 | dt: 96.32ms | tok/sec: 85053.28\n","start training the model\n","step 0 of training\n","step  2171 | loss: 4.901587 | lr 5.9150e-04 | norm: 0.9536 | dt: 96.30ms | tok/sec: 85066.76\n","start training the model\n","step 0 of training\n","step  2172 | loss: 4.922548 | lr 5.9149e-04 | norm: 0.8990 | dt: 96.87ms | tok/sec: 84562.62\n","start training the model\n","step 0 of training\n","step  2173 | loss: 4.861241 | lr 5.9148e-04 | norm: 0.8165 | dt: 97.15ms | tok/sec: 84326.45\n","start training the model\n","step 0 of training\n","step  2174 | loss: 5.761855 | lr 5.9147e-04 | norm: 1.1106 | dt: 98.66ms | tok/sec: 83034.05\n","start training the model\n","step 0 of training\n","step  2175 | loss: 5.784245 | lr 5.9146e-04 | norm: 1.0093 | dt: 96.24ms | tok/sec: 85122.60\n","start training the model\n","step 0 of training\n","step  2176 | loss: 5.654584 | lr 5.9145e-04 | norm: 0.8399 | dt: 97.83ms | tok/sec: 83736.03\n","start training the model\n","step 0 of training\n","step  2177 | loss: 5.636436 | lr 5.9144e-04 | norm: 0.8598 | dt: 97.74ms | tok/sec: 83811.60\n","start training the model\n","step 0 of training\n","step  2178 | loss: 5.604402 | lr 5.9142e-04 | norm: 1.1600 | dt: 97.16ms | tok/sec: 84313.82\n","start training the model\n","step 0 of training\n","step  2179 | loss: 5.639523 | lr 5.9141e-04 | norm: 0.9023 | dt: 97.56ms | tok/sec: 83966.03\n","start training the model\n","step 0 of training\n","step  2180 | loss: 5.476427 | lr 5.9140e-04 | norm: 0.7561 | dt: 96.94ms | tok/sec: 84509.38\n","start training the model\n","step 0 of training\n","step  2181 | loss: 5.483378 | lr 5.9139e-04 | norm: 0.8113 | dt: 96.88ms | tok/sec: 84560.33\n","start training the model\n","step 0 of training\n","step  2182 | loss: 5.520649 | lr 5.9138e-04 | norm: 0.9147 | dt: 96.39ms | tok/sec: 84986.17\n","start training the model\n","step 0 of training\n","step  2183 | loss: 5.576947 | lr 5.9137e-04 | norm: 1.0221 | dt: 96.66ms | tok/sec: 84750.55\n","start training the model\n","step 0 of training\n","step  2184 | loss: 5.530544 | lr 5.9135e-04 | norm: 0.8964 | dt: 96.72ms | tok/sec: 84699.16\n","start training the model\n","step 0 of training\n","step  2185 | loss: 5.557242 | lr 5.9134e-04 | norm: 0.9481 | dt: 96.24ms | tok/sec: 85122.81\n","start training the model\n","step 0 of training\n","step  2186 | loss: 5.502935 | lr 5.9133e-04 | norm: 0.9669 | dt: 97.26ms | tok/sec: 84230.74\n","start training the model\n","step 0 of training\n","step  2187 | loss: 5.476471 | lr 5.9132e-04 | norm: 1.2174 | dt: 96.15ms | tok/sec: 85196.05\n","start training the model\n","step 0 of training\n","step  2188 | loss: 5.364552 | lr 5.9131e-04 | norm: 1.1874 | dt: 96.21ms | tok/sec: 85144.54\n","start training the model\n","step 0 of training\n","step  2189 | loss: 5.446305 | lr 5.9130e-04 | norm: 1.1153 | dt: 96.82ms | tok/sec: 84609.47\n","start training the model\n","step 0 of training\n","step  2190 | loss: 5.457478 | lr 5.9128e-04 | norm: 1.3925 | dt: 96.56ms | tok/sec: 84834.88\n","start training the model\n","step 0 of training\n","step  2191 | loss: 5.329642 | lr 5.9127e-04 | norm: 0.9985 | dt: 95.88ms | tok/sec: 85441.38\n","start training the model\n","step 0 of training\n","step  2192 | loss: 5.792167 | lr 5.9126e-04 | norm: 2.6215 | dt: 95.89ms | tok/sec: 85433.52\n","start training the model\n","step 0 of training\n","step  2193 | loss: 5.403074 | lr 5.9125e-04 | norm: 1.8448 | dt: 96.64ms | tok/sec: 84767.49\n","start training the model\n","step 0 of training\n","step  2194 | loss: 4.824721 | lr 5.9124e-04 | norm: 2.3463 | dt: 96.27ms | tok/sec: 85097.94\n","start training the model\n","step 0 of training\n","step  2195 | loss: 5.018493 | lr 5.9123e-04 | norm: 1.5351 | dt: 96.20ms | tok/sec: 85153.82\n","start training the model\n","step 0 of training\n","step  2196 | loss: 4.272253 | lr 5.9121e-04 | norm: 1.1362 | dt: 96.14ms | tok/sec: 85208.94\n","start training the model\n","step 0 of training\n","step  2197 | loss: 4.402743 | lr 5.9120e-04 | norm: 1.1607 | dt: 95.92ms | tok/sec: 85402.30\n","start training the model\n","step 0 of training\n","step  2198 | loss: 4.292572 | lr 5.9119e-04 | norm: 1.0853 | dt: 96.48ms | tok/sec: 84909.51\n","start training the model\n","step 0 of training\n","step  2199 | loss: 4.748358 | lr 5.9118e-04 | norm: 1.0178 | dt: 96.45ms | tok/sec: 84937.01\n","start training the model\n","step 0 of training\n","step  2200 | loss: 4.453237 | lr 5.9117e-04 | norm: 1.1515 | dt: 96.88ms | tok/sec: 84558.04\n","start training the model\n","step 0 of training\n","step  2201 | loss: 3.694686 | lr 5.9116e-04 | norm: 1.2292 | dt: 96.39ms | tok/sec: 84985.54\n","start training the model\n","step 0 of training\n","step  2202 | loss: 2.177943 | lr 5.9114e-04 | norm: 1.6799 | dt: 96.29ms | tok/sec: 85076.44\n","start training the model\n","step 0 of training\n","step  2203 | loss: 2.019661 | lr 5.9113e-04 | norm: 0.9385 | dt: 96.74ms | tok/sec: 84680.16\n","start training the model\n","step 0 of training\n","step  2204 | loss: 1.932074 | lr 5.9112e-04 | norm: 1.0919 | dt: 96.74ms | tok/sec: 84684.96\n","start training the model\n","step 0 of training\n","step  2205 | loss: 1.984978 | lr 5.9111e-04 | norm: 1.0777 | dt: 97.14ms | tok/sec: 84335.14\n","start training the model\n","step 0 of training\n","step  2206 | loss: 2.037656 | lr 5.9110e-04 | norm: 0.7170 | dt: 96.13ms | tok/sec: 85216.34\n","start training the model\n","step 0 of training\n","step  2207 | loss: 1.886209 | lr 5.9109e-04 | norm: 0.9133 | dt: 96.20ms | tok/sec: 85153.19\n","start training the model\n","step 0 of training\n","step  2208 | loss: 2.084126 | lr 5.9107e-04 | norm: 0.8990 | dt: 97.13ms | tok/sec: 84343.21\n","start training the model\n","step 0 of training\n","step  2209 | loss: 1.966162 | lr 5.9106e-04 | norm: 0.7545 | dt: 96.88ms | tok/sec: 84555.13\n","start training the model\n","step 0 of training\n","step  2210 | loss: 1.688742 | lr 5.9105e-04 | norm: 0.8473 | dt: 97.16ms | tok/sec: 84317.55\n","start training the model\n","step 0 of training\n","step  2211 | loss: 1.671512 | lr 5.9104e-04 | norm: 0.7488 | dt: 96.51ms | tok/sec: 84879.94\n","start training the model\n","step 0 of training\n","step  2212 | loss: 1.910803 | lr 5.9103e-04 | norm: 0.7235 | dt: 95.73ms | tok/sec: 85572.89\n","start training the model\n","step 0 of training\n","step  2213 | loss: 2.072269 | lr 5.9101e-04 | norm: 1.0405 | dt: 96.71ms | tok/sec: 84705.63\n","start training the model\n","step 0 of training\n","step  2214 | loss: 2.181714 | lr 5.9100e-04 | norm: 1.3878 | dt: 96.76ms | tok/sec: 84664.51\n","start training the model\n","step 0 of training\n","step  2215 | loss: 1.933063 | lr 5.9099e-04 | norm: 0.8137 | dt: 96.25ms | tok/sec: 85107.84\n","start training the model\n","step 0 of training\n","step  2216 | loss: 1.959543 | lr 5.9098e-04 | norm: 0.8481 | dt: 97.01ms | tok/sec: 84440.63\n","start training the model\n","step 0 of training\n","step  2217 | loss: 1.970031 | lr 5.9097e-04 | norm: 0.8730 | dt: 96.85ms | tok/sec: 84585.73\n","start training the model\n","step 0 of training\n","step  2218 | loss: 1.967179 | lr 5.9096e-04 | norm: 0.6058 | dt: 96.23ms | tok/sec: 85127.87\n","start training the model\n","step 0 of training\n","step  2219 | loss: 1.864239 | lr 5.9094e-04 | norm: 1.0722 | dt: 97.62ms | tok/sec: 83915.17\n","start training the model\n","step 0 of training\n","step  2220 | loss: 1.856128 | lr 5.9093e-04 | norm: 0.8757 | dt: 96.65ms | tok/sec: 84758.91\n","start training the model\n","step 0 of training\n","step  2221 | loss: 1.868174 | lr 5.9092e-04 | norm: 0.7272 | dt: 97.46ms | tok/sec: 84054.35\n","start training the model\n","step 0 of training\n","step  2222 | loss: 1.836730 | lr 5.9091e-04 | norm: 0.7755 | dt: 97.06ms | tok/sec: 84397.70\n","start training the model\n","step 0 of training\n","step  2223 | loss: 1.828663 | lr 5.9090e-04 | norm: 1.0007 | dt: 97.30ms | tok/sec: 84196.27\n","start training the model\n","step 0 of training\n","step  2224 | loss: 1.774464 | lr 5.9088e-04 | norm: 1.3324 | dt: 97.80ms | tok/sec: 83759.30\n","start training the model\n","step 0 of training\n","step  2225 | loss: 1.729130 | lr 5.9087e-04 | norm: 0.9012 | dt: 97.25ms | tok/sec: 84237.14\n","start training the model\n","step 0 of training\n","step  2226 | loss: 1.874762 | lr 5.9086e-04 | norm: 1.3107 | dt: 97.70ms | tok/sec: 83846.57\n","start training the model\n","step 0 of training\n","step  2227 | loss: 1.769612 | lr 5.9085e-04 | norm: 1.1066 | dt: 97.75ms | tok/sec: 83808.12\n","start training the model\n","step 0 of training\n","step  2228 | loss: 1.823916 | lr 5.9084e-04 | norm: 0.8843 | dt: 97.96ms | tok/sec: 83626.58\n","start training the model\n","step 0 of training\n","step  2229 | loss: 1.786556 | lr 5.9082e-04 | norm: 1.2936 | dt: 97.07ms | tok/sec: 84396.46\n","start training the model\n","step 0 of training\n","step  2230 | loss: 1.787615 | lr 5.9081e-04 | norm: 0.8076 | dt: 96.93ms | tok/sec: 84518.52\n","start training the model\n","step 0 of training\n","step  2231 | loss: 1.656885 | lr 5.9080e-04 | norm: 1.0665 | dt: 96.70ms | tok/sec: 84717.54\n","start training the model\n","step 0 of training\n","step  2232 | loss: 1.697675 | lr 5.9079e-04 | norm: 1.4959 | dt: 97.10ms | tok/sec: 84364.96\n","start training the model\n","step 0 of training\n","step  2233 | loss: 1.684262 | lr 5.9078e-04 | norm: 1.3669 | dt: 98.07ms | tok/sec: 83528.60\n","start training the model\n","step 0 of training\n","step  2234 | loss: 1.685637 | lr 5.9077e-04 | norm: 1.5533 | dt: 97.45ms | tok/sec: 84063.20\n","start training the model\n","step 0 of training\n","step  2235 | loss: 1.658909 | lr 5.9075e-04 | norm: 1.4561 | dt: 97.49ms | tok/sec: 84030.10\n","start training the model\n","step 0 of training\n","step  2236 | loss: 1.649152 | lr 5.9074e-04 | norm: 1.6337 | dt: 96.90ms | tok/sec: 84539.11\n","start training the model\n","step 0 of training\n","step  2237 | loss: 1.675689 | lr 5.9073e-04 | norm: 1.4445 | dt: 96.60ms | tok/sec: 84800.12\n","start training the model\n","step 0 of training\n","step  2238 | loss: 2.475625 | lr 5.9072e-04 | norm: 1.1795 | dt: 96.73ms | tok/sec: 84685.38\n","start training the model\n","step 0 of training\n","step  2239 | loss: 5.741993 | lr 5.9071e-04 | norm: 2.2568 | dt: 97.03ms | tok/sec: 84423.83\n","start training the model\n","step 0 of training\n","step  2240 | loss: 5.185200 | lr 5.9069e-04 | norm: 1.0598 | dt: 97.57ms | tok/sec: 83958.03\n","start training the model\n","step 0 of training\n","step  2241 | loss: 5.156222 | lr 5.9068e-04 | norm: 0.8550 | dt: 96.66ms | tok/sec: 84754.32\n","start training the model\n","step 0 of training\n","step  2242 | loss: 5.048548 | lr 5.9067e-04 | norm: 0.8329 | dt: 97.00ms | tok/sec: 84457.24\n","start training the model\n","step 0 of training\n","step  2243 | loss: 4.986484 | lr 5.9066e-04 | norm: 0.8162 | dt: 97.26ms | tok/sec: 84231.56\n","start training the model\n","step 0 of training\n","step  2244 | loss: 5.016940 | lr 5.9064e-04 | norm: 0.8178 | dt: 96.51ms | tok/sec: 84882.03\n","start training the model\n","step 0 of training\n","step  2245 | loss: 5.064608 | lr 5.9063e-04 | norm: 0.8698 | dt: 96.53ms | tok/sec: 84868.20\n","start training the model\n","step 0 of training\n","step  2246 | loss: 4.969744 | lr 5.9062e-04 | norm: 0.8618 | dt: 96.70ms | tok/sec: 84717.33\n","start training the model\n","step 0 of training\n","step  2247 | loss: 5.111331 | lr 5.9061e-04 | norm: 0.7105 | dt: 97.10ms | tok/sec: 84366.62\n","start training the model\n","step 0 of training\n","step  2248 | loss: 5.156678 | lr 5.9060e-04 | norm: 0.7978 | dt: 96.53ms | tok/sec: 84862.96\n","start training the model\n","step 0 of training\n","step  2249 | loss: 5.129023 | lr 5.9058e-04 | norm: 0.7126 | dt: 96.79ms | tok/sec: 84634.90\n","validation loss: 5.9731\n","rank 0 sample 0: [CLS] دردم از یار است و درمان نیز هم[SEP] راز[BOB][BOM] این همه بی شمار؟[BOM] آن که باشد[BOB][BOM] گفت به هیچ[BOM] بیا، از سر\n","rank 0 sample 1: [CLS] دردم از یار است و درمان نیز هم[SEP] بیزد[BOB][BOM] بهای من مگر به من هم نه من، لب شیرین به جان از غم بی سر\n","rank 0 sample 2: [CLS] دردم از یار است و درمان نیز هم[SEP]ی[BOB][BOM] اینان هر که ز دست ما روز و جان من[BOM] که تو با ما[BOB][BOM] به\n","rank 0 sample 3: [CLS] دردم از یار است و درمان نیز هم[SEP] تو بود[BOM] ز من[BOB][BOM] با ما تو حافظ[BOB][BOM] طلبد[BOB][BOM] ای تو[BOM] بر من\n","start training the model\n","step 0 of training\n","step  2250 | loss: 5.098777 | lr 5.9057e-04 | norm: 0.7177 | dt: 1301.66ms | tok/sec: 6293.51\n","start training the model\n","step 0 of training\n","step  2251 | loss: 5.144594 | lr 5.9056e-04 | norm: 0.8638 | dt: 95.28ms | tok/sec: 85976.72\n","start training the model\n","step 0 of training\n","step  2252 | loss: 5.238999 | lr 5.9055e-04 | norm: 0.7072 | dt: 95.52ms | tok/sec: 85763.84\n","start training the model\n","step 0 of training\n","step  2253 | loss: 5.153960 | lr 5.9054e-04 | norm: 0.8820 | dt: 95.59ms | tok/sec: 85694.96\n","start training the model\n","step 0 of training\n","step  2254 | loss: 5.132767 | lr 5.9052e-04 | norm: 0.8624 | dt: 96.15ms | tok/sec: 85202.60\n","start training the model\n","step 0 of training\n","step  2255 | loss: 5.135444 | lr 5.9051e-04 | norm: 0.7990 | dt: 96.52ms | tok/sec: 84869.25\n","start training the model\n","step 0 of training\n","step  2256 | loss: 5.010482 | lr 5.9050e-04 | norm: 1.2051 | dt: 95.83ms | tok/sec: 85485.17\n","start training the model\n","step 0 of training\n","step  2257 | loss: 5.282726 | lr 5.9049e-04 | norm: 1.2464 | dt: 96.11ms | tok/sec: 85234.73\n","start training the model\n","step 0 of training\n","step  2258 | loss: 5.043638 | lr 5.9048e-04 | norm: 0.9159 | dt: 96.35ms | tok/sec: 85022.97\n","start training the model\n","step 0 of training\n","step  2259 | loss: 4.974579 | lr 5.9046e-04 | norm: 1.2750 | dt: 96.52ms | tok/sec: 84877.63\n","start training the model\n","step 0 of training\n","step  2260 | loss: 5.108435 | lr 5.9045e-04 | norm: 0.8709 | dt: 96.95ms | tok/sec: 84495.66\n","start training the model\n","step 0 of training\n","step  2261 | loss: 5.149180 | lr 5.9044e-04 | norm: 0.9242 | dt: 96.79ms | tok/sec: 84639.90\n","start training the model\n","step 0 of training\n","step  2262 | loss: 5.033574 | lr 5.9043e-04 | norm: 0.8237 | dt: 97.12ms | tok/sec: 84347.77\n","start training the model\n","step 0 of training\n","step  2263 | loss: 5.000930 | lr 5.9041e-04 | norm: 0.7251 | dt: 96.71ms | tok/sec: 84708.97\n","start training the model\n","step 0 of training\n","step  2264 | loss: 4.980215 | lr 5.9040e-04 | norm: 0.7954 | dt: 96.33ms | tok/sec: 85038.75\n","start training the model\n","step 0 of training\n","step  2265 | loss: 5.084078 | lr 5.9039e-04 | norm: 0.7665 | dt: 94.24ms | tok/sec: 86929.90\n","start training the model\n","step 0 of training\n","step  2266 | loss: 5.183865 | lr 5.9038e-04 | norm: 0.8958 | dt: 96.76ms | tok/sec: 84659.51\n","start training the model\n","step 0 of training\n","step  2267 | loss: 5.267660 | lr 5.9037e-04 | norm: 0.9193 | dt: 100.79ms | tok/sec: 81277.31\n","start training the model\n","step 0 of training\n","step  2268 | loss: 5.306574 | lr 5.9035e-04 | norm: 1.6538 | dt: 96.32ms | tok/sec: 85046.12\n","start training the model\n","step 0 of training\n","step  2269 | loss: 5.333423 | lr 5.9034e-04 | norm: 1.4235 | dt: 96.18ms | tok/sec: 85176.41\n","start training the model\n","step 0 of training\n","step  2270 | loss: 5.330995 | lr 5.9033e-04 | norm: 1.2148 | dt: 96.83ms | tok/sec: 84601.14\n","start training the model\n","step 0 of training\n","step  2271 | loss: 5.078681 | lr 5.9032e-04 | norm: 1.2081 | dt: 96.07ms | tok/sec: 85269.84\n","start training the model\n","step 0 of training\n","step  2272 | loss: 5.069236 | lr 5.9030e-04 | norm: 1.2776 | dt: 96.66ms | tok/sec: 84747.83\n","start training the model\n","step 0 of training\n","step  2273 | loss: 4.929154 | lr 5.9029e-04 | norm: 1.4279 | dt: 96.78ms | tok/sec: 84644.28\n","start training the model\n","step 0 of training\n","step  2274 | loss: 5.085619 | lr 5.9028e-04 | norm: 0.8850 | dt: 97.01ms | tok/sec: 84446.24\n","start training the model\n","step 0 of training\n","step  2275 | loss: 4.925543 | lr 5.9027e-04 | norm: 1.2685 | dt: 96.45ms | tok/sec: 84935.96\n","start training the model\n","step 0 of training\n","step  2276 | loss: 5.036441 | lr 5.9026e-04 | norm: 0.8498 | dt: 96.02ms | tok/sec: 85314.73\n","start training the model\n","step 0 of training\n","step  2277 | loss: 4.995120 | lr 5.9024e-04 | norm: 0.8290 | dt: 96.93ms | tok/sec: 84513.54\n","start training the model\n","step 0 of training\n","step  2278 | loss: 4.856297 | lr 5.9023e-04 | norm: 0.8568 | dt: 96.69ms | tok/sec: 84720.04\n","start training the model\n","step 0 of training\n","step  2279 | loss: 4.886204 | lr 5.9022e-04 | norm: 0.8586 | dt: 96.85ms | tok/sec: 84585.73\n","start training the model\n","step 0 of training\n","step  2280 | loss: 4.918489 | lr 5.9021e-04 | norm: 0.8732 | dt: 96.87ms | tok/sec: 84570.95\n","start training the model\n","step 0 of training\n","step  2281 | loss: 4.960394 | lr 5.9019e-04 | norm: 0.8207 | dt: 96.70ms | tok/sec: 84713.99\n","start training the model\n","step 0 of training\n","step  2282 | loss: 4.965114 | lr 5.9018e-04 | norm: 0.7233 | dt: 97.29ms | tok/sec: 84197.92\n","start training the model\n","step 0 of training\n","step  2283 | loss: 5.142839 | lr 5.9017e-04 | norm: 0.9494 | dt: 95.81ms | tok/sec: 85503.46\n","start training the model\n","step 0 of training\n","step  2284 | loss: 5.085518 | lr 5.9016e-04 | norm: 0.7870 | dt: 97.54ms | tok/sec: 83982.45\n","start training the model\n","step 0 of training\n","step  2285 | loss: 5.191446 | lr 5.9014e-04 | norm: 0.8373 | dt: 97.54ms | tok/sec: 83990.25\n","start training the model\n","step 0 of training\n","step  2286 | loss: 5.090892 | lr 5.9013e-04 | norm: 0.9873 | dt: 97.77ms | tok/sec: 83791.36\n","start training the model\n","step 0 of training\n","step  2287 | loss: 5.144310 | lr 5.9012e-04 | norm: 1.0513 | dt: 97.62ms | tok/sec: 83915.38\n","start training the model\n","step 0 of training\n","step  2288 | loss: 5.240646 | lr 5.9011e-04 | norm: 0.9970 | dt: 97.26ms | tok/sec: 84228.88\n","start training the model\n","step 0 of training\n","step  2289 | loss: 5.072533 | lr 5.9010e-04 | norm: 1.2684 | dt: 96.72ms | tok/sec: 84694.15\n","start training the model\n","step 0 of training\n","step  2290 | loss: 5.017335 | lr 5.9008e-04 | norm: 0.8863 | dt: 97.49ms | tok/sec: 84025.58\n","start training the model\n","step 0 of training\n","step  2291 | loss: 5.010107 | lr 5.9007e-04 | norm: 0.9381 | dt: 96.86ms | tok/sec: 84576.78\n","start training the model\n","step 0 of training\n","step  2292 | loss: 5.110923 | lr 5.9006e-04 | norm: 1.1379 | dt: 97.20ms | tok/sec: 84278.05\n","start training the model\n","step 0 of training\n","step  2293 | loss: 5.055636 | lr 5.9005e-04 | norm: 1.1180 | dt: 96.64ms | tok/sec: 84771.88\n","start training the model\n","step 0 of training\n","step  2294 | loss: 5.032455 | lr 5.9003e-04 | norm: 0.9928 | dt: 96.81ms | tok/sec: 84621.56\n","start training the model\n","step 0 of training\n","step  2295 | loss: 5.027584 | lr 5.9002e-04 | norm: 1.1225 | dt: 96.76ms | tok/sec: 84665.35\n","start training the model\n","step 0 of training\n","step  2296 | loss: 4.855423 | lr 5.9001e-04 | norm: 1.3584 | dt: 96.83ms | tok/sec: 84601.14\n","start training the model\n","step 0 of training\n","step  2297 | loss: 4.832629 | lr 5.9000e-04 | norm: 1.0973 | dt: 97.10ms | tok/sec: 84369.93\n","start training the model\n","step 0 of training\n","step  2298 | loss: 4.874818 | lr 5.8998e-04 | norm: 1.0220 | dt: 96.74ms | tok/sec: 84683.92\n","start training the model\n","step 0 of training\n","step  2299 | loss: 4.822313 | lr 5.8997e-04 | norm: 1.0688 | dt: 96.78ms | tok/sec: 84649.91\n","start training the model\n","step 0 of training\n","step  2300 | loss: 5.750474 | lr 5.8996e-04 | norm: 1.2956 | dt: 97.06ms | tok/sec: 84400.81\n","start training the model\n","step 0 of training\n","step  2301 | loss: 5.798841 | lr 5.8995e-04 | norm: 1.2027 | dt: 96.81ms | tok/sec: 84622.39\n","start training the model\n","step 0 of training\n","step  2302 | loss: 5.641736 | lr 5.8993e-04 | norm: 0.9334 | dt: 95.50ms | tok/sec: 85783.32\n","start training the model\n","step 0 of training\n","step  2303 | loss: 5.575485 | lr 5.8992e-04 | norm: 0.7401 | dt: 97.19ms | tok/sec: 84291.49\n","start training the model\n","step 0 of training\n","step  2304 | loss: 5.527606 | lr 5.8991e-04 | norm: 0.9696 | dt: 97.17ms | tok/sec: 84304.31\n","start training the model\n","step 0 of training\n","step  2305 | loss: 5.579261 | lr 5.8990e-04 | norm: 1.0386 | dt: 96.87ms | tok/sec: 84568.45\n","start training the model\n","step 0 of training\n","step  2306 | loss: 5.403903 | lr 5.8988e-04 | norm: 0.8334 | dt: 96.95ms | tok/sec: 84499.40\n","start training the model\n","step 0 of training\n","step  2307 | loss: 5.419599 | lr 5.8987e-04 | norm: 0.8586 | dt: 97.09ms | tok/sec: 84374.90\n","start training the model\n","step 0 of training\n","step  2308 | loss: 5.452020 | lr 5.8986e-04 | norm: 0.7950 | dt: 97.02ms | tok/sec: 84434.20\n","start training the model\n","step 0 of training\n","step  2309 | loss: 5.532145 | lr 5.8985e-04 | norm: 1.2762 | dt: 97.59ms | tok/sec: 83946.74\n","start training the model\n","step 0 of training\n","step  2310 | loss: 5.486961 | lr 5.8983e-04 | norm: 1.0489 | dt: 97.07ms | tok/sec: 84395.00\n","start training the model\n","step 0 of training\n","step  2311 | loss: 5.499699 | lr 5.8982e-04 | norm: 0.8146 | dt: 97.17ms | tok/sec: 84302.86\n","start training the model\n","step 0 of training\n","step  2312 | loss: 5.455458 | lr 5.8981e-04 | norm: 1.1517 | dt: 96.76ms | tok/sec: 84663.26\n","start training the model\n","step 0 of training\n","step  2313 | loss: 5.458071 | lr 5.8980e-04 | norm: 1.5405 | dt: 96.42ms | tok/sec: 84957.38\n","start training the model\n","step 0 of training\n","step  2314 | loss: 5.306306 | lr 5.8978e-04 | norm: 1.0978 | dt: 96.38ms | tok/sec: 84997.73\n","start training the model\n","step 0 of training\n","step  2315 | loss: 5.393027 | lr 5.8977e-04 | norm: 1.0800 | dt: 96.69ms | tok/sec: 84728.61\n","start training the model\n","step 0 of training\n","step  2316 | loss: 5.403371 | lr 5.8976e-04 | norm: 1.0318 | dt: 96.24ms | tok/sec: 85122.39\n","start training the model\n","step 0 of training\n","step  2317 | loss: 5.284009 | lr 5.8975e-04 | norm: 1.3239 | dt: 96.90ms | tok/sec: 84542.86\n","start training the model\n","step 0 of training\n","step  2318 | loss: 5.753894 | lr 5.8973e-04 | norm: 2.4760 | dt: 96.77ms | tok/sec: 84653.25\n","start training the model\n","step 0 of training\n","step  2319 | loss: 5.609410 | lr 5.8972e-04 | norm: 3.7279 | dt: 96.03ms | tok/sec: 85309.64\n","start training the model\n","step 0 of training\n","step  2320 | loss: 4.999754 | lr 5.8971e-04 | norm: 3.6860 | dt: 95.76ms | tok/sec: 85549.87\n","start training the model\n","step 0 of training\n","step  2321 | loss: 5.080701 | lr 5.8969e-04 | norm: 2.2294 | dt: 97.24ms | tok/sec: 84246.43\n","start training the model\n","step 0 of training\n","step  2322 | loss: 4.303314 | lr 5.8968e-04 | norm: 1.7708 | dt: 96.53ms | tok/sec: 84865.26\n","start training the model\n","step 0 of training\n","step  2323 | loss: 4.425069 | lr 5.8967e-04 | norm: 1.9874 | dt: 96.99ms | tok/sec: 84465.54\n","start training the model\n","step 0 of training\n","step  2324 | loss: 4.275813 | lr 5.8966e-04 | norm: 1.6809 | dt: 96.05ms | tok/sec: 85290.80\n","start training the model\n","step 0 of training\n","step  2325 | loss: 4.761154 | lr 5.8964e-04 | norm: 2.0529 | dt: 96.48ms | tok/sec: 84907.00\n","start training the model\n","step 0 of training\n","step  2326 | loss: 4.464270 | lr 5.8963e-04 | norm: 1.4781 | dt: 96.66ms | tok/sec: 84750.55\n","start training the model\n","step 0 of training\n","step  2327 | loss: 3.690763 | lr 5.8962e-04 | norm: 1.2986 | dt: 96.39ms | tok/sec: 84990.58\n","start training the model\n","step 0 of training\n","step  2328 | loss: 2.195775 | lr 5.8961e-04 | norm: 1.9199 | dt: 96.49ms | tok/sec: 84897.14\n","start training the model\n","step 0 of training\n","step  2329 | loss: 2.031241 | lr 5.8959e-04 | norm: 1.2851 | dt: 96.80ms | tok/sec: 84626.35\n","start training the model\n","step 0 of training\n","step  2330 | loss: 1.952149 | lr 5.8958e-04 | norm: 1.2163 | dt: 97.37ms | tok/sec: 84131.33\n","start training the model\n","step 0 of training\n","step  2331 | loss: 2.020762 | lr 5.8957e-04 | norm: 1.3205 | dt: 96.56ms | tok/sec: 84839.91\n","start training the model\n","step 0 of training\n","step  2332 | loss: 2.052576 | lr 5.8956e-04 | norm: 1.0917 | dt: 96.93ms | tok/sec: 84511.46\n","start training the model\n","step 0 of training\n","step  2333 | loss: 1.878502 | lr 5.8954e-04 | norm: 0.9802 | dt: 97.07ms | tok/sec: 84393.14\n","start training the model\n","step 0 of training\n","step  2334 | loss: 2.076050 | lr 5.8953e-04 | norm: 0.8720 | dt: 96.53ms | tok/sec: 84860.65\n","start training the model\n","step 0 of training\n","step  2335 | loss: 1.966455 | lr 5.8952e-04 | norm: 0.9521 | dt: 97.22ms | tok/sec: 84261.31\n","start training the model\n","step 0 of training\n","step  2336 | loss: 1.693861 | lr 5.8950e-04 | norm: 0.8796 | dt: 96.51ms | tok/sec: 84880.15\n","start training the model\n","step 0 of training\n","step  2337 | loss: 1.666345 | lr 5.8949e-04 | norm: 0.7116 | dt: 97.12ms | tok/sec: 84348.60\n","start training the model\n","step 0 of training\n","step  2338 | loss: 1.924166 | lr 5.8948e-04 | norm: 0.7870 | dt: 96.48ms | tok/sec: 84912.45\n","start training the model\n","step 0 of training\n","step  2339 | loss: 2.062857 | lr 5.8947e-04 | norm: 0.8014 | dt: 96.12ms | tok/sec: 85229.65\n","start training the model\n","step 0 of training\n","step  2340 | loss: 2.185584 | lr 5.8945e-04 | norm: 1.6288 | dt: 97.62ms | tok/sec: 83915.99\n","start training the model\n","step 0 of training\n","step  2341 | loss: 1.898673 | lr 5.8944e-04 | norm: 0.7869 | dt: 99.07ms | tok/sec: 82690.94\n","start training the model\n","step 0 of training\n","step  2342 | loss: 1.938412 | lr 5.8943e-04 | norm: 0.9356 | dt: 97.86ms | tok/sec: 83712.16\n","start training the model\n","step 0 of training\n","step  2343 | loss: 1.944283 | lr 5.8942e-04 | norm: 0.7808 | dt: 97.67ms | tok/sec: 83873.38\n","start training the model\n","step 0 of training\n","step  2344 | loss: 1.939855 | lr 5.8940e-04 | norm: 0.6557 | dt: 97.11ms | tok/sec: 84358.54\n","start training the model\n","step 0 of training\n","step  2345 | loss: 1.847314 | lr 5.8939e-04 | norm: 0.6972 | dt: 97.44ms | tok/sec: 84072.04\n","start training the model\n","step 0 of training\n","step  2346 | loss: 1.831011 | lr 5.8938e-04 | norm: 0.6234 | dt: 96.52ms | tok/sec: 84869.66\n","start training the model\n","step 0 of training\n","step  2347 | loss: 1.850133 | lr 5.8936e-04 | norm: 0.7073 | dt: 97.31ms | tok/sec: 84181.00\n","start training the model\n","step 0 of training\n","step  2348 | loss: 1.807923 | lr 5.8935e-04 | norm: 0.6282 | dt: 96.12ms | tok/sec: 85229.23\n","start training the model\n","step 0 of training\n","step  2349 | loss: 1.793831 | lr 5.8934e-04 | norm: 0.5612 | dt: 96.76ms | tok/sec: 84661.38\n","start training the model\n","step 0 of training\n","step  2350 | loss: 1.756066 | lr 5.8933e-04 | norm: 0.6415 | dt: 97.07ms | tok/sec: 84394.80\n","start training the model\n","step 0 of training\n","step  2351 | loss: 1.697576 | lr 5.8931e-04 | norm: 0.5461 | dt: 97.08ms | tok/sec: 84381.53\n","start training the model\n","step 0 of training\n","step  2352 | loss: 1.846264 | lr 5.8930e-04 | norm: 0.7065 | dt: 97.20ms | tok/sec: 84278.25\n","start training the model\n","step 0 of training\n","step  2353 | loss: 1.737752 | lr 5.8929e-04 | norm: 0.9310 | dt: 96.32ms | tok/sec: 85051.38\n","start training the model\n","step 0 of training\n","step  2354 | loss: 1.787107 | lr 5.8927e-04 | norm: 0.9122 | dt: 96.57ms | tok/sec: 84826.08\n","start training the model\n","step 0 of training\n","step  2355 | loss: 1.776566 | lr 5.8926e-04 | norm: 1.1261 | dt: 96.23ms | tok/sec: 85128.09\n","start training the model\n","step 0 of training\n","step  2356 | loss: 1.759404 | lr 5.8925e-04 | norm: 0.8798 | dt: 96.07ms | tok/sec: 85268.78\n","start training the model\n","step 0 of training\n","step  2357 | loss: 1.635537 | lr 5.8924e-04 | norm: 1.5986 | dt: 97.33ms | tok/sec: 84167.39\n","start training the model\n","step 0 of training\n","step  2358 | loss: 1.680838 | lr 5.8922e-04 | norm: 1.2644 | dt: 96.31ms | tok/sec: 85054.75\n","start training the model\n","step 0 of training\n","step  2359 | loss: 1.660143 | lr 5.8921e-04 | norm: 1.3535 | dt: 96.23ms | tok/sec: 85125.34\n","start training the model\n","step 0 of training\n","step  2360 | loss: 1.668949 | lr 5.8920e-04 | norm: 1.5401 | dt: 95.99ms | tok/sec: 85340.15\n","start training the model\n","step 0 of training\n","step  2361 | loss: 1.628044 | lr 5.8918e-04 | norm: 2.0709 | dt: 97.02ms | tok/sec: 84436.69\n","start training the model\n","step 0 of training\n","step  2362 | loss: 1.637198 | lr 5.8917e-04 | norm: 1.2658 | dt: 96.20ms | tok/sec: 85155.09\n","start training the model\n","step 0 of training\n","step  2363 | loss: 1.650128 | lr 5.8916e-04 | norm: 1.1969 | dt: 96.04ms | tok/sec: 85297.36\n","start training the model\n","step 0 of training\n","step  2364 | loss: 2.440115 | lr 5.8914e-04 | norm: 1.7156 | dt: 95.84ms | tok/sec: 85473.05\n","start training the model\n","step 0 of training\n","step  2365 | loss: 5.678404 | lr 5.8913e-04 | norm: 2.2085 | dt: 97.11ms | tok/sec: 84361.64\n","start training the model\n","step 0 of training\n","step  2366 | loss: 5.108046 | lr 5.8912e-04 | norm: 1.5079 | dt: 96.87ms | tok/sec: 84568.66\n","start training the model\n","step 0 of training\n","step  2367 | loss: 5.075294 | lr 5.8911e-04 | norm: 1.0287 | dt: 96.51ms | tok/sec: 84883.50\n","start training the model\n","step 0 of training\n","step  2368 | loss: 5.003165 | lr 5.8909e-04 | norm: 0.9617 | dt: 96.90ms | tok/sec: 84542.44\n","start training the model\n","step 0 of training\n","step  2369 | loss: 4.944849 | lr 5.8908e-04 | norm: 0.8184 | dt: 97.09ms | tok/sec: 84373.87\n","start training the model\n","step 0 of training\n","step  2370 | loss: 4.971723 | lr 5.8907e-04 | norm: 0.9122 | dt: 96.49ms | tok/sec: 84903.64\n","start training the model\n","step 0 of training\n","step  2371 | loss: 5.008144 | lr 5.8905e-04 | norm: 0.8491 | dt: 97.59ms | tok/sec: 83946.33\n","start training the model\n","step 0 of training\n","step  2372 | loss: 4.907260 | lr 5.8904e-04 | norm: 0.8378 | dt: 96.59ms | tok/sec: 84815.61\n","start training the model\n","step 0 of training\n","step  2373 | loss: 5.049114 | lr 5.8903e-04 | norm: 0.9295 | dt: 96.68ms | tok/sec: 84730.28\n","start training the model\n","step 0 of training\n","step  2374 | loss: 5.086914 | lr 5.8901e-04 | norm: 0.8413 | dt: 96.49ms | tok/sec: 84897.55\n","start training the model\n","step 0 of training\n","step  2375 | loss: 5.071731 | lr 5.8900e-04 | norm: 0.7244 | dt: 96.97ms | tok/sec: 84477.17\n","start training the model\n","step 0 of training\n","step  2376 | loss: 5.041345 | lr 5.8899e-04 | norm: 0.7116 | dt: 95.40ms | tok/sec: 85872.08\n","start training the model\n","step 0 of training\n","step  2377 | loss: 5.107456 | lr 5.8898e-04 | norm: 1.0475 | dt: 96.04ms | tok/sec: 85295.24\n","start training the model\n","step 0 of training\n","step  2378 | loss: 5.195308 | lr 5.8896e-04 | norm: 0.7989 | dt: 95.45ms | tok/sec: 85822.53\n","start training the model\n","step 0 of training\n","step  2379 | loss: 5.106169 | lr 5.8895e-04 | norm: 0.8424 | dt: 95.28ms | tok/sec: 85980.17\n","start training the model\n","step 0 of training\n","step  2380 | loss: 5.077390 | lr 5.8894e-04 | norm: 0.8002 | dt: 95.61ms | tok/sec: 85681.93\n","start training the model\n","step 0 of training\n","step  2381 | loss: 5.113261 | lr 5.8892e-04 | norm: 0.9138 | dt: 96.52ms | tok/sec: 84873.44\n","start training the model\n","step 0 of training\n","step  2382 | loss: 4.997868 | lr 5.8891e-04 | norm: 0.9070 | dt: 95.78ms | tok/sec: 85530.50\n","start training the model\n","step 0 of training\n","step  2383 | loss: 5.279908 | lr 5.8890e-04 | norm: 1.2909 | dt: 95.87ms | tok/sec: 85446.69\n","start training the model\n","step 0 of training\n","step  2384 | loss: 5.029872 | lr 5.8888e-04 | norm: 1.5039 | dt: 95.99ms | tok/sec: 85342.70\n","start training the model\n","step 0 of training\n","step  2385 | loss: 4.986111 | lr 5.8887e-04 | norm: 1.3087 | dt: 96.32ms | tok/sec: 85050.54\n","start training the model\n","step 0 of training\n","step  2386 | loss: 5.077859 | lr 5.8886e-04 | norm: 0.8987 | dt: 96.69ms | tok/sec: 84725.68\n","start training the model\n","step 0 of training\n","step  2387 | loss: 5.136776 | lr 5.8884e-04 | norm: 1.1714 | dt: 96.60ms | tok/sec: 84801.38\n","start training the model\n","step 0 of training\n","step  2388 | loss: 5.034363 | lr 5.8883e-04 | norm: 1.2043 | dt: 96.70ms | tok/sec: 84713.57\n","start training the model\n","step 0 of training\n","step  2389 | loss: 4.987501 | lr 5.8882e-04 | norm: 1.0698 | dt: 96.76ms | tok/sec: 84661.38\n","start training the model\n","step 0 of training\n","step  2390 | loss: 4.952024 | lr 5.8881e-04 | norm: 0.9096 | dt: 96.66ms | tok/sec: 84754.32\n","start training the model\n","step 0 of training\n","step  2391 | loss: 5.053647 | lr 5.8879e-04 | norm: 0.9279 | dt: 97.04ms | tok/sec: 84416.15\n","start training the model\n","step 0 of training\n","step  2392 | loss: 5.145579 | lr 5.8878e-04 | norm: 0.9574 | dt: 96.70ms | tok/sec: 84716.70\n","start training the model\n","step 0 of training\n","step  2393 | loss: 5.216477 | lr 5.8877e-04 | norm: 1.1633 | dt: 101.63ms | tok/sec: 80605.76\n","start training the model\n","step 0 of training\n","step  2394 | loss: 5.326626 | lr 5.8875e-04 | norm: 1.5510 | dt: 95.46ms | tok/sec: 85816.75\n","start training the model\n","step 0 of training\n","step  2395 | loss: 5.340707 | lr 5.8874e-04 | norm: 1.3159 | dt: 95.83ms | tok/sec: 85480.49\n","start training the model\n","step 0 of training\n","step  2396 | loss: 5.340862 | lr 5.8873e-04 | norm: 1.2629 | dt: 96.03ms | tok/sec: 85305.62\n","start training the model\n","step 0 of training\n","step  2397 | loss: 5.065132 | lr 5.8871e-04 | norm: 1.2842 | dt: 96.43ms | tok/sec: 84948.98\n","start training the model\n","step 0 of training\n","step  2398 | loss: 5.057060 | lr 5.8870e-04 | norm: 1.0756 | dt: 95.98ms | tok/sec: 85349.27\n","start training the model\n","step 0 of training\n","step  2399 | loss: 4.905235 | lr 5.8869e-04 | norm: 1.3836 | dt: 96.19ms | tok/sec: 85166.70\n","start training the model\n","step 0 of training\n","step  2400 | loss: 5.055569 | lr 5.8867e-04 | norm: 0.8681 | dt: 96.58ms | tok/sec: 84822.73\n","start training the model\n","step 0 of training\n","step  2401 | loss: 4.913238 | lr 5.8866e-04 | norm: 0.9942 | dt: 96.45ms | tok/sec: 84934.91\n","start training the model\n","step 0 of training\n","step  2402 | loss: 4.994482 | lr 5.8865e-04 | norm: 0.9215 | dt: 97.11ms | tok/sec: 84357.92\n","start training the model\n","step 0 of training\n","step  2403 | loss: 4.944493 | lr 5.8863e-04 | norm: 0.7712 | dt: 96.60ms | tok/sec: 84804.31\n","start training the model\n","step 0 of training\n","step  2404 | loss: 4.814818 | lr 5.8862e-04 | norm: 0.9184 | dt: 96.47ms | tok/sec: 84918.12\n","start training the model\n","step 0 of training\n","step  2405 | loss: 4.833707 | lr 5.8861e-04 | norm: 0.9009 | dt: 96.63ms | tok/sec: 84773.76\n","start training the model\n","step 0 of training\n","step  2406 | loss: 4.852933 | lr 5.8859e-04 | norm: 0.8982 | dt: 96.80ms | tok/sec: 84626.15\n","start training the model\n","step 0 of training\n","step  2407 | loss: 4.933271 | lr 5.8858e-04 | norm: 1.2142 | dt: 97.26ms | tok/sec: 84227.02\n","start training the model\n","step 0 of training\n","step  2408 | loss: 4.939096 | lr 5.8857e-04 | norm: 1.0712 | dt: 96.33ms | tok/sec: 85044.02\n","start training the model\n","step 0 of training\n","step  2409 | loss: 5.109244 | lr 5.8855e-04 | norm: 1.1723 | dt: 96.80ms | tok/sec: 84629.69\n","start training the model\n","step 0 of training\n","step  2410 | loss: 5.096466 | lr 5.8854e-04 | norm: 1.4851 | dt: 96.87ms | tok/sec: 84564.70\n","start training the model\n","step 0 of training\n","step  2411 | loss: 5.185014 | lr 5.8853e-04 | norm: 1.2573 | dt: 96.65ms | tok/sec: 84763.51\n","start training the model\n","step 0 of training\n","step  2412 | loss: 5.062134 | lr 5.8851e-04 | norm: 0.9944 | dt: 96.71ms | tok/sec: 84703.54\n","start training the model\n","step 0 of training\n","step  2413 | loss: 5.113325 | lr 5.8850e-04 | norm: 0.9866 | dt: 95.61ms | tok/sec: 85683.64\n","start training the model\n","step 0 of training\n","step  2414 | loss: 5.201592 | lr 5.8849e-04 | norm: 1.0012 | dt: 96.72ms | tok/sec: 84700.41\n","start training the model\n","step 0 of training\n","step  2415 | loss: 5.027006 | lr 5.8847e-04 | norm: 1.1491 | dt: 96.99ms | tok/sec: 84466.17\n","start training the model\n","step 0 of training\n","step  2416 | loss: 4.963429 | lr 5.8846e-04 | norm: 1.0002 | dt: 97.31ms | tok/sec: 84185.13\n","start training the model\n","step 0 of training\n","step  2417 | loss: 4.940918 | lr 5.8845e-04 | norm: 0.7554 | dt: 97.42ms | tok/sec: 84091.59\n","start training the model\n","step 0 of training\n","step  2418 | loss: 5.072113 | lr 5.8843e-04 | norm: 0.9604 | dt: 97.73ms | tok/sec: 83825.71\n","start training the model\n","step 0 of training\n","step  2419 | loss: 5.009293 | lr 5.8842e-04 | norm: 0.9441 | dt: 98.29ms | tok/sec: 83347.05\n","start training the model\n","step 0 of training\n","step  2420 | loss: 4.971454 | lr 5.8841e-04 | norm: 0.8269 | dt: 97.32ms | tok/sec: 84172.13\n","start training the model\n","step 0 of training\n","step  2421 | loss: 4.966418 | lr 5.8839e-04 | norm: 0.9546 | dt: 98.01ms | tok/sec: 83580.20\n","start training the model\n","step 0 of training\n","step  2422 | loss: 4.842365 | lr 5.8838e-04 | norm: 1.1632 | dt: 97.45ms | tok/sec: 84064.43\n","start training the model\n","step 0 of training\n","step  2423 | loss: 4.760102 | lr 5.8837e-04 | norm: 1.0989 | dt: 97.07ms | tok/sec: 84392.10\n","start training the model\n","step 0 of training\n","step  2424 | loss: 4.824630 | lr 5.8835e-04 | norm: 1.1538 | dt: 97.21ms | tok/sec: 84273.09\n","start training the model\n","step 0 of training\n","step  2425 | loss: 4.757613 | lr 5.8834e-04 | norm: 1.0931 | dt: 97.59ms | tok/sec: 83944.69\n","start training the model\n","step 0 of training\n","step  2426 | loss: 5.688219 | lr 5.8833e-04 | norm: 1.1784 | dt: 97.07ms | tok/sec: 84390.24\n","start training the model\n","step 0 of training\n","step  2427 | loss: 5.753816 | lr 5.8831e-04 | norm: 1.3138 | dt: 97.01ms | tok/sec: 84446.44\n","start training the model\n","step 0 of training\n","step  2428 | loss: 5.597063 | lr 5.8830e-04 | norm: 1.0742 | dt: 96.49ms | tok/sec: 84904.06\n","start training the model\n","step 0 of training\n","step  2429 | loss: 5.535313 | lr 5.8829e-04 | norm: 0.9417 | dt: 97.04ms | tok/sec: 84420.92\n","start training the model\n","step 0 of training\n","step  2430 | loss: 5.453976 | lr 5.8827e-04 | norm: 0.8813 | dt: 96.88ms | tok/sec: 84554.30\n","start training the model\n","step 0 of training\n","step  2431 | loss: 5.525280 | lr 5.8826e-04 | norm: 1.1625 | dt: 97.03ms | tok/sec: 84430.47\n","start training the model\n","step 0 of training\n","step  2432 | loss: 5.349043 | lr 5.8825e-04 | norm: 0.9416 | dt: 97.75ms | tok/sec: 83801.79\n","start training the model\n","step 0 of training\n","step  2433 | loss: 5.360499 | lr 5.8823e-04 | norm: 0.8846 | dt: 95.99ms | tok/sec: 85338.67\n","start training the model\n","step 0 of training\n","step  2434 | loss: 5.410392 | lr 5.8822e-04 | norm: 1.0012 | dt: 97.07ms | tok/sec: 84388.79\n","start training the model\n","step 0 of training\n","step  2435 | loss: 5.475592 | lr 5.8821e-04 | norm: 1.0026 | dt: 96.52ms | tok/sec: 84869.66\n","start training the model\n","step 0 of training\n","step  2436 | loss: 5.415464 | lr 5.8819e-04 | norm: 0.8380 | dt: 96.03ms | tok/sec: 85305.83\n","start training the model\n","step 0 of training\n","step  2437 | loss: 5.415344 | lr 5.8818e-04 | norm: 0.7121 | dt: 96.83ms | tok/sec: 84602.39\n","start training the model\n","step 0 of training\n","step  2438 | loss: 5.410992 | lr 5.8816e-04 | norm: 0.8458 | dt: 96.51ms | tok/sec: 84885.18\n","start training the model\n","step 0 of training\n","step  2439 | loss: 5.430503 | lr 5.8815e-04 | norm: 1.0048 | dt: 97.13ms | tok/sec: 84340.32\n","start training the model\n","step 0 of training\n","step  2440 | loss: 5.251321 | lr 5.8814e-04 | norm: 0.9992 | dt: 96.54ms | tok/sec: 84857.30\n","start training the model\n","step 0 of training\n","step  2441 | loss: 5.342117 | lr 5.8812e-04 | norm: 0.9012 | dt: 96.95ms | tok/sec: 84494.42\n","start training the model\n","step 0 of training\n","step  2442 | loss: 5.322053 | lr 5.8811e-04 | norm: 0.9749 | dt: 96.97ms | tok/sec: 84483.40\n","start training the model\n","step 0 of training\n","step  2443 | loss: 5.246401 | lr 5.8810e-04 | norm: 0.9590 | dt: 96.38ms | tok/sec: 84995.21\n","start training the model\n","step 0 of training\n","step  2444 | loss: 5.672809 | lr 5.8808e-04 | norm: 1.9669 | dt: 97.11ms | tok/sec: 84354.40\n","start training the model\n","step 0 of training\n","step  2445 | loss: 5.405380 | lr 5.8807e-04 | norm: 2.3774 | dt: 96.82ms | tok/sec: 84614.06\n","start training the model\n","step 0 of training\n","step  2446 | loss: 4.824830 | lr 5.8806e-04 | norm: 2.6680 | dt: 96.73ms | tok/sec: 84685.80\n","start training the model\n","step 0 of training\n","step  2447 | loss: 5.037223 | lr 5.8804e-04 | norm: 2.4198 | dt: 96.72ms | tok/sec: 84701.25\n","start training the model\n","step 0 of training\n","step  2448 | loss: 4.237939 | lr 5.8803e-04 | norm: 1.7215 | dt: 96.66ms | tok/sec: 84747.21\n","start training the model\n","step 0 of training\n","step  2449 | loss: 4.326126 | lr 5.8802e-04 | norm: 1.5198 | dt: 97.57ms | tok/sec: 83960.90\n","start training the model\n","step 0 of training\n","step  2450 | loss: 4.188868 | lr 5.8800e-04 | norm: 1.3387 | dt: 96.66ms | tok/sec: 84753.69\n","start training the model\n","step 0 of training\n","step  2451 | loss: 4.673139 | lr 5.8799e-04 | norm: 1.2555 | dt: 95.93ms | tok/sec: 85395.30\n","start training the model\n","step 0 of training\n","step  2452 | loss: 4.368255 | lr 5.8797e-04 | norm: 1.6946 | dt: 95.92ms | tok/sec: 85408.46\n","start training the model\n","step 0 of training\n","step  2453 | loss: 3.598022 | lr 5.8796e-04 | norm: 1.2521 | dt: 95.94ms | tok/sec: 85386.39\n","start training the model\n","step 0 of training\n","step  2454 | loss: 2.114333 | lr 5.8795e-04 | norm: 1.5517 | dt: 96.76ms | tok/sec: 84662.85\n","start training the model\n","step 0 of training\n","step  2455 | loss: 1.969671 | lr 5.8793e-04 | norm: 0.9225 | dt: 96.51ms | tok/sec: 84884.97\n","start training the model\n","step 0 of training\n","step  2456 | loss: 1.892319 | lr 5.8792e-04 | norm: 1.0089 | dt: 97.02ms | tok/sec: 84434.20\n","start training the model\n","step 0 of training\n","step  2457 | loss: 1.951383 | lr 5.8791e-04 | norm: 1.0119 | dt: 96.06ms | tok/sec: 85282.12\n","start training the model\n","step 0 of training\n","step  2458 | loss: 1.988591 | lr 5.8789e-04 | norm: 0.7081 | dt: 96.03ms | tok/sec: 85305.83\n","start training the model\n","step 0 of training\n","step  2459 | loss: 1.838156 | lr 5.8788e-04 | norm: 0.9918 | dt: 96.32ms | tok/sec: 85050.96\n","start training the model\n","step 0 of training\n","step  2460 | loss: 2.031026 | lr 5.8787e-04 | norm: 0.8689 | dt: 96.33ms | tok/sec: 85041.91\n","start training the model\n","step 0 of training\n","step  2461 | loss: 1.900517 | lr 5.8785e-04 | norm: 0.5974 | dt: 96.91ms | tok/sec: 84529.75\n","start training the model\n","step 0 of training\n","step  2462 | loss: 1.644389 | lr 5.8784e-04 | norm: 0.8069 | dt: 95.83ms | tok/sec: 85481.77\n","start training the model\n","step 0 of training\n","step  2463 | loss: 1.623334 | lr 5.8782e-04 | norm: 0.7005 | dt: 95.69ms | tok/sec: 85605.93\n","start training the model\n","step 0 of training\n","step  2464 | loss: 1.866448 | lr 5.8781e-04 | norm: 0.6593 | dt: 96.21ms | tok/sec: 85149.81\n","start training the model\n","step 0 of training\n","step  2465 | loss: 2.004440 | lr 5.8780e-04 | norm: 0.8651 | dt: 96.29ms | tok/sec: 85077.29\n","start training the model\n","step 0 of training\n","step  2466 | loss: 2.125683 | lr 5.8778e-04 | norm: 1.5029 | dt: 97.67ms | tok/sec: 83872.16\n","start training the model\n","step 0 of training\n","step  2467 | loss: 1.864677 | lr 5.8777e-04 | norm: 1.2397 | dt: 95.68ms | tok/sec: 85621.72\n","start training the model\n","step 0 of training\n","step  2468 | loss: 1.880320 | lr 5.8776e-04 | norm: 0.8314 | dt: 95.79ms | tok/sec: 85520.06\n","start training the model\n","step 0 of training\n","step  2469 | loss: 1.895074 | lr 5.8774e-04 | norm: 1.2847 | dt: 95.94ms | tok/sec: 85391.06\n","start training the model\n","step 0 of training\n","step  2470 | loss: 1.905364 | lr 5.8773e-04 | norm: 1.0171 | dt: 95.62ms | tok/sec: 85670.18\n","start training the model\n","step 0 of training\n","step  2471 | loss: 1.828738 | lr 5.8771e-04 | norm: 1.8389 | dt: 97.35ms | tok/sec: 84146.98\n","start training the model\n","step 0 of training\n","step  2472 | loss: 1.816943 | lr 5.8770e-04 | norm: 1.7134 | dt: 97.16ms | tok/sec: 84312.17\n","start training the model\n","step 0 of training\n","step  2473 | loss: 1.804192 | lr 5.8769e-04 | norm: 0.6476 | dt: 97.09ms | tok/sec: 84376.77\n","start training the model\n","step 0 of training\n","step  2474 | loss: 1.791657 | lr 5.8767e-04 | norm: 1.1668 | dt: 97.55ms | tok/sec: 83976.49\n","start training the model\n","step 0 of training\n","step  2475 | loss: 1.785446 | lr 5.8766e-04 | norm: 1.2515 | dt: 98.74ms | tok/sec: 82965.08\n","start training the model\n","step 0 of training\n","step  2476 | loss: 1.724182 | lr 5.8765e-04 | norm: 0.7427 | dt: 97.01ms | tok/sec: 84447.90\n","start training the model\n","step 0 of training\n","step  2477 | loss: 1.671988 | lr 5.8763e-04 | norm: 0.9301 | dt: 97.91ms | tok/sec: 83665.48\n","start training the model\n","step 0 of training\n","step  2478 | loss: 1.817863 | lr 5.8762e-04 | norm: 1.1374 | dt: 97.00ms | tok/sec: 84451.22\n","start training the model\n","step 0 of training\n","step  2479 | loss: 1.700965 | lr 5.8760e-04 | norm: 0.9284 | dt: 97.74ms | tok/sec: 83818.35\n","start training the model\n","step 0 of training\n","step  2480 | loss: 1.748252 | lr 5.8759e-04 | norm: 1.0284 | dt: 96.83ms | tok/sec: 84597.60\n","start training the model\n","step 0 of training\n","step  2481 | loss: 1.737720 | lr 5.8758e-04 | norm: 0.9024 | dt: 96.41ms | tok/sec: 84968.52\n","start training the model\n","step 0 of training\n","step  2482 | loss: 1.711493 | lr 5.8756e-04 | norm: 0.9797 | dt: 96.75ms | tok/sec: 84669.31\n","start training the model\n","step 0 of training\n","step  2483 | loss: 1.604194 | lr 5.8755e-04 | norm: 1.1792 | dt: 96.54ms | tok/sec: 84852.48\n","start training the model\n","step 0 of training\n","step  2484 | loss: 1.651057 | lr 5.8753e-04 | norm: 1.1302 | dt: 96.72ms | tok/sec: 84697.91\n","start training the model\n","step 0 of training\n","step  2485 | loss: 1.638288 | lr 5.8752e-04 | norm: 1.1696 | dt: 96.62ms | tok/sec: 84786.10\n","start training the model\n","step 0 of training\n","step  2486 | loss: 1.642737 | lr 5.8751e-04 | norm: 1.1607 | dt: 96.57ms | tok/sec: 84832.16\n","start training the model\n","step 0 of training\n","step  2487 | loss: 1.606861 | lr 5.8749e-04 | norm: 0.9849 | dt: 96.99ms | tok/sec: 84461.39\n","start training the model\n","step 0 of training\n","step  2488 | loss: 1.598965 | lr 5.8748e-04 | norm: 1.5069 | dt: 96.33ms | tok/sec: 85037.28\n","start training the model\n","step 0 of training\n","step  2489 | loss: 1.611455 | lr 5.8747e-04 | norm: 1.4528 | dt: 95.73ms | tok/sec: 85569.90\n","start training the model\n","step 0 of training\n","step  2490 | loss: 2.402466 | lr 5.8745e-04 | norm: 1.1220 | dt: 97.03ms | tok/sec: 84431.71\n","start training the model\n","step 0 of training\n","step  2491 | loss: 5.526061 | lr 5.8744e-04 | norm: 1.1175 | dt: 97.26ms | tok/sec: 84228.26\n","start training the model\n","step 0 of training\n","step  2492 | loss: 5.013715 | lr 5.8742e-04 | norm: 1.1652 | dt: 97.10ms | tok/sec: 84364.13\n","start training the model\n","step 0 of training\n","step  2493 | loss: 4.972377 | lr 5.8741e-04 | norm: 1.0032 | dt: 97.45ms | tok/sec: 84067.31\n","start training the model\n","step 0 of training\n","step  2494 | loss: 4.902404 | lr 5.8740e-04 | norm: 0.8992 | dt: 97.28ms | tok/sec: 84207.20\n","start training the model\n","step 0 of training\n","step  2495 | loss: 4.835483 | lr 5.8738e-04 | norm: 0.7813 | dt: 96.99ms | tok/sec: 84458.28\n","start training the model\n","step 0 of training\n","step  2496 | loss: 4.892825 | lr 5.8737e-04 | norm: 0.9847 | dt: 98.78ms | tok/sec: 82929.24\n","start training the model\n","step 0 of training\n","step  2497 | loss: 4.932870 | lr 5.8735e-04 | norm: 0.8978 | dt: 97.49ms | tok/sec: 84029.48\n","start training the model\n","step 0 of training\n","step  2498 | loss: 4.854262 | lr 5.8734e-04 | norm: 0.9273 | dt: 98.39ms | tok/sec: 83263.84\n","start training the model\n","step 0 of training\n","step  2499 | loss: 4.979915 | lr 5.8733e-04 | norm: 0.8434 | dt: 97.19ms | tok/sec: 84291.90\n","validation loss: 6.0078\n","rank 0 sample 0: [CLS] دردم از یار است و درمان نیز هم[SEP] ما را و من و خرابات نیست ز مذهب دیگر نیست[BOM] که تو است، از ما نیز نیست[BOB][BOM]\n","rank 0 sample 1: [CLS] دردم از یار است و درمان نیز هم[SEP] کرم باد[BOM] ما و ز]>[BOB][BOM] چشم بر باد[BOM] هیچ که از دست دراز و گر در\n","rank 0 sample 2: [CLS] دردم از یار است و درمان نیز هم[SEP] و هر لحظه[BOB][BOM] ای آسمان به[BOM] می که در ره[BOM] حافظ[BOB][BOM] ز من و کار[BOB]\n","rank 0 sample 3: [CLS] دردم از یار است و درمان نیز هم[SEP] امری نیست[BOM] هر آن[BOB][BOM] که چو تو که یک سر بر این لطف نیست[BOM] روی نیست[BOM]\n","start training the model\n","step 0 of training\n","step  2500 | loss: 5.016039 | lr 5.8731e-04 | norm: 0.9502 | dt: 1314.43ms | tok/sec: 6232.36\n","start training the model\n","step 0 of training\n","step  2501 | loss: 5.012397 | lr 5.8730e-04 | norm: 0.8264 | dt: 97.01ms | tok/sec: 84440.63\n","start training the model\n","step 0 of training\n","step  2502 | loss: 4.986600 | lr 5.8728e-04 | norm: 0.7534 | dt: 97.68ms | tok/sec: 83869.29\n","start training the model\n","step 0 of training\n","step  2503 | loss: 5.029913 | lr 5.8727e-04 | norm: 0.8008 | dt: 97.02ms | tok/sec: 84434.82\n","start training the model\n","step 0 of training\n","step  2504 | loss: 5.133152 | lr 5.8726e-04 | norm: 0.8515 | dt: 94.89ms | tok/sec: 86328.83\n","start training the model\n","step 0 of training\n","step  2505 | loss: 5.026342 | lr 5.8724e-04 | norm: 0.9317 | dt: 95.75ms | tok/sec: 85557.76\n","start training the model\n","step 0 of training\n","step  2506 | loss: 5.024942 | lr 5.8723e-04 | norm: 0.7987 | dt: 96.02ms | tok/sec: 85314.09\n","start training the model\n","step 0 of training\n","step  2507 | loss: 5.056549 | lr 5.8721e-04 | norm: 0.8395 | dt: 96.20ms | tok/sec: 85158.68\n","start training the model\n","step 0 of training\n","step  2508 | loss: 4.932073 | lr 5.8720e-04 | norm: 0.9916 | dt: 95.87ms | tok/sec: 85445.42\n","start training the model\n","step 0 of training\n","step  2509 | loss: 5.215372 | lr 5.8719e-04 | norm: 2.0218 | dt: 96.10ms | tok/sec: 85248.90\n","start training the model\n","step 0 of training\n","step  2510 | loss: 4.971153 | lr 5.8717e-04 | norm: 1.1247 | dt: 95.64ms | tok/sec: 85654.37\n","start training the model\n","step 0 of training\n","step  2511 | loss: 4.924612 | lr 5.8716e-04 | norm: 0.9663 | dt: 95.93ms | tok/sec: 85395.51\n","start training the model\n","step 0 of training\n","step  2512 | loss: 5.020396 | lr 5.8714e-04 | norm: 0.9813 | dt: 95.68ms | tok/sec: 85621.72\n","start training the model\n","step 0 of training\n","step  2513 | loss: 5.079868 | lr 5.8713e-04 | norm: 1.1759 | dt: 96.29ms | tok/sec: 85077.08\n","start training the model\n","step 0 of training\n","step  2514 | loss: 4.963502 | lr 5.8712e-04 | norm: 0.9572 | dt: 96.45ms | tok/sec: 84937.85\n","start training the model\n","step 0 of training\n","step  2515 | loss: 4.912812 | lr 5.8710e-04 | norm: 0.8130 | dt: 96.18ms | tok/sec: 85176.83\n","start training the model\n","step 0 of training\n","step  2516 | loss: 4.873265 | lr 5.8709e-04 | norm: 0.8579 | dt: 96.99ms | tok/sec: 84459.52\n","start training the model\n","step 0 of training\n","step  2517 | loss: 4.963661 | lr 5.8707e-04 | norm: 0.8327 | dt: 96.69ms | tok/sec: 84722.34\n","start training the model\n","step 0 of training\n","step  2518 | loss: 5.078823 | lr 5.8706e-04 | norm: 0.9456 | dt: 96.30ms | tok/sec: 85066.33\n","start training the model\n","step 0 of training\n","step  2519 | loss: 5.175753 | lr 5.8705e-04 | norm: 0.8120 | dt: 101.33ms | tok/sec: 80843.21\n","start training the model\n","step 0 of training\n","step  2520 | loss: 5.215905 | lr 5.8703e-04 | norm: 1.4114 | dt: 95.58ms | tok/sec: 85704.15\n","start training the model\n","step 0 of training\n","step  2521 | loss: 5.269218 | lr 5.8702e-04 | norm: 1.2735 | dt: 96.18ms | tok/sec: 85174.51\n","start training the model\n","step 0 of training\n","step  2522 | loss: 5.221628 | lr 5.8700e-04 | norm: 1.2794 | dt: 95.27ms | tok/sec: 85983.39\n","start training the model\n","step 0 of training\n","step  2523 | loss: 5.005148 | lr 5.8699e-04 | norm: 1.2953 | dt: 95.82ms | tok/sec: 85490.27\n","start training the model\n","step 0 of training\n","step  2524 | loss: 4.943221 | lr 5.8697e-04 | norm: 1.2325 | dt: 95.72ms | tok/sec: 85579.28\n","start training the model\n","step 0 of training\n","step  2525 | loss: 4.862251 | lr 5.8696e-04 | norm: 1.3929 | dt: 96.04ms | tok/sec: 85298.63\n","start training the model\n","step 0 of training\n","step  2526 | loss: 4.992063 | lr 5.8695e-04 | norm: 0.8579 | dt: 96.20ms | tok/sec: 85151.71\n","start training the model\n","step 0 of training\n","step  2527 | loss: 4.838534 | lr 5.8693e-04 | norm: 1.0692 | dt: 96.38ms | tok/sec: 85000.68\n","start training the model\n","step 0 of training\n","step  2528 | loss: 4.919080 | lr 5.8692e-04 | norm: 0.8635 | dt: 97.19ms | tok/sec: 84292.52\n","start training the model\n","step 0 of training\n","step  2529 | loss: 4.892101 | lr 5.8690e-04 | norm: 1.0131 | dt: 96.31ms | tok/sec: 85058.54\n","start training the model\n","step 0 of training\n","step  2530 | loss: 4.741475 | lr 5.8689e-04 | norm: 0.9299 | dt: 96.33ms | tok/sec: 85044.44\n","start training the model\n","step 0 of training\n","step  2531 | loss: 4.768159 | lr 5.8687e-04 | norm: 0.9033 | dt: 96.31ms | tok/sec: 85055.17\n","start training the model\n","step 0 of training\n","step  2532 | loss: 4.790362 | lr 5.8686e-04 | norm: 0.9461 | dt: 95.88ms | tok/sec: 85436.92\n","start training the model\n","step 0 of training\n","step  2533 | loss: 4.889614 | lr 5.8685e-04 | norm: 1.0104 | dt: 97.47ms | tok/sec: 84043.05\n","start training the model\n","step 0 of training\n","step  2534 | loss: 4.870049 | lr 5.8683e-04 | norm: 0.9533 | dt: 96.22ms | tok/sec: 85135.68\n","start training the model\n","step 0 of training\n","step  2535 | loss: 5.049520 | lr 5.8682e-04 | norm: 1.3813 | dt: 96.43ms | tok/sec: 84948.56\n","start training the model\n","step 0 of training\n","step  2536 | loss: 5.027632 | lr 5.8680e-04 | norm: 1.0346 | dt: 96.14ms | tok/sec: 85207.88\n","start training the model\n","step 0 of training\n","step  2537 | loss: 5.100909 | lr 5.8679e-04 | norm: 0.8936 | dt: 96.49ms | tok/sec: 84903.22\n","start training the model\n","step 0 of training\n","step  2538 | loss: 4.937447 | lr 5.8678e-04 | norm: 0.8636 | dt: 96.98ms | tok/sec: 84467.20\n","start training the model\n","step 0 of training\n","step  2539 | loss: 5.004728 | lr 5.8676e-04 | norm: 1.0032 | dt: 96.83ms | tok/sec: 84598.02\n","start training the model\n","step 0 of training\n","step  2540 | loss: 5.113453 | lr 5.8675e-04 | norm: 0.9427 | dt: 96.43ms | tok/sec: 84955.28\n","start training the model\n","step 0 of training\n","step  2541 | loss: 4.953929 | lr 5.8673e-04 | norm: 1.0427 | dt: 96.61ms | tok/sec: 84795.10\n","start training the model\n","step 0 of training\n","step  2542 | loss: 4.903890 | lr 5.8672e-04 | norm: 0.8402 | dt: 97.05ms | tok/sec: 84410.97\n","start training the model\n","step 0 of training\n","step  2543 | loss: 4.858999 | lr 5.8670e-04 | norm: 0.8128 | dt: 97.65ms | tok/sec: 83889.97\n","start training the model\n","step 0 of training\n","step  2544 | loss: 5.010132 | lr 5.8669e-04 | norm: 1.0535 | dt: 97.27ms | tok/sec: 84220.62\n","start training the model\n","step 0 of training\n","step  2545 | loss: 4.952975 | lr 5.8668e-04 | norm: 0.9627 | dt: 97.19ms | tok/sec: 84284.46\n","start training the model\n","step 0 of training\n","step  2546 | loss: 4.902992 | lr 5.8666e-04 | norm: 0.7935 | dt: 97.42ms | tok/sec: 84085.41\n","start training the model\n","step 0 of training\n","step  2547 | loss: 4.898173 | lr 5.8665e-04 | norm: 1.0520 | dt: 97.33ms | tok/sec: 84170.07\n","start training the model\n","step 0 of training\n","step  2548 | loss: 4.791075 | lr 5.8663e-04 | norm: 1.2129 | dt: 98.51ms | tok/sec: 83162.87\n","start training the model\n","step 0 of training\n","step  2549 | loss: 4.694520 | lr 5.8662e-04 | norm: 1.0131 | dt: 96.78ms | tok/sec: 84643.03\n","start training the model\n","step 0 of training\n","step  2550 | loss: 4.756370 | lr 5.8660e-04 | norm: 0.9358 | dt: 97.44ms | tok/sec: 84071.01\n","start training the model\n","step 0 of training\n","step  2551 | loss: 4.679603 | lr 5.8659e-04 | norm: 0.9846 | dt: 96.48ms | tok/sec: 84908.46\n","start training the model\n","step 0 of training\n","step  2552 | loss: 5.634788 | lr 5.8657e-04 | norm: 1.6907 | dt: 97.20ms | tok/sec: 84277.43\n","start training the model\n","step 0 of training\n","step  2553 | loss: 5.716388 | lr 5.8656e-04 | norm: 1.3211 | dt: 95.81ms | tok/sec: 85504.95\n","start training the model\n","step 0 of training\n","step  2554 | loss: 5.506237 | lr 5.8655e-04 | norm: 0.9511 | dt: 96.50ms | tok/sec: 84890.63\n","start training the model\n","step 0 of training\n","step  2555 | loss: 5.444347 | lr 5.8653e-04 | norm: 0.9775 | dt: 96.18ms | tok/sec: 85172.19\n","start training the model\n","step 0 of training\n","step  2556 | loss: 5.395616 | lr 5.8652e-04 | norm: 1.2172 | dt: 95.87ms | tok/sec: 85447.33\n","start training the model\n","step 0 of training\n","step  2557 | loss: 5.496742 | lr 5.8650e-04 | norm: 1.1743 | dt: 96.02ms | tok/sec: 85315.57\n","start training the model\n","step 0 of training\n","step  2558 | loss: 5.289797 | lr 5.8649e-04 | norm: 1.1734 | dt: 95.82ms | tok/sec: 85494.10\n","start training the model\n","step 0 of training\n","step  2559 | loss: 5.296345 | lr 5.8647e-04 | norm: 0.9506 | dt: 96.42ms | tok/sec: 84962.63\n","start training the model\n","step 0 of training\n","step  2560 | loss: 5.354982 | lr 5.8646e-04 | norm: 0.9299 | dt: 95.59ms | tok/sec: 85695.82\n","start training the model\n","step 0 of training\n","step  2561 | loss: 5.375497 | lr 5.8645e-04 | norm: 1.1751 | dt: 97.04ms | tok/sec: 84420.92\n","start training the model\n","step 0 of training\n","step  2562 | loss: 5.346570 | lr 5.8643e-04 | norm: 1.0519 | dt: 97.41ms | tok/sec: 84096.11\n","start training the model\n","step 0 of training\n","step  2563 | loss: 5.352545 | lr 5.8642e-04 | norm: 0.8894 | dt: 96.56ms | tok/sec: 84837.18\n","start training the model\n","step 0 of training\n","step  2564 | loss: 5.306127 | lr 5.8640e-04 | norm: 0.8253 | dt: 96.61ms | tok/sec: 84797.19\n","start training the model\n","step 0 of training\n","step  2565 | loss: 5.330615 | lr 5.8639e-04 | norm: 1.2211 | dt: 96.76ms | tok/sec: 84661.59\n","start training the model\n","step 0 of training\n","step  2566 | loss: 5.164467 | lr 5.8637e-04 | norm: 1.1783 | dt: 96.30ms | tok/sec: 85066.54\n","start training the model\n","step 0 of training\n","step  2567 | loss: 5.262147 | lr 5.8636e-04 | norm: 1.0845 | dt: 96.75ms | tok/sec: 84670.77\n","start training the model\n","step 0 of training\n","step  2568 | loss: 5.240726 | lr 5.8634e-04 | norm: 1.1374 | dt: 97.33ms | tok/sec: 84167.39\n","start training the model\n","step 0 of training\n","step  2569 | loss: 5.167957 | lr 5.8633e-04 | norm: 1.0095 | dt: 97.05ms | tok/sec: 84409.93\n","start training the model\n","step 0 of training\n","step  2570 | loss: 5.568286 | lr 5.8631e-04 | norm: 2.1983 | dt: 97.26ms | tok/sec: 84224.54\n","start training the model\n","step 0 of training\n","step  2571 | loss: 5.295072 | lr 5.8630e-04 | norm: 1.8919 | dt: 96.20ms | tok/sec: 85157.62\n","start training the model\n","step 0 of training\n","step  2572 | loss: 4.683158 | lr 5.8629e-04 | norm: 1.9386 | dt: 96.22ms | tok/sec: 85136.73\n","start training the model\n","step 0 of training\n","step  2573 | loss: 4.940408 | lr 5.8627e-04 | norm: 2.0027 | dt: 96.35ms | tok/sec: 85021.71\n","start training the model\n","step 0 of training\n","step  2574 | loss: 4.132678 | lr 5.8626e-04 | norm: 1.0888 | dt: 96.53ms | tok/sec: 84868.62\n","start training the model\n","step 0 of training\n","step  2575 | loss: 4.241037 | lr 5.8624e-04 | norm: 1.3268 | dt: 96.63ms | tok/sec: 84778.15\n","start training the model\n","step 0 of training\n","step  2576 | loss: 4.120527 | lr 5.8623e-04 | norm: 1.6269 | dt: 96.44ms | tok/sec: 84947.30\n","start training the model\n","step 0 of training\n","step  2577 | loss: 4.602502 | lr 5.8621e-04 | norm: 1.6007 | dt: 97.26ms | tok/sec: 84229.50\n","start training the model\n","step 0 of training\n","step  2578 | loss: 4.322146 | lr 5.8620e-04 | norm: 1.0486 | dt: 96.38ms | tok/sec: 84999.84\n","start training the model\n","step 0 of training\n","step  2579 | loss: 3.507178 | lr 5.8618e-04 | norm: 1.1254 | dt: 96.40ms | tok/sec: 84977.55\n","start training the model\n","step 0 of training\n","step  2580 | loss: 2.046924 | lr 5.8617e-04 | norm: 1.1116 | dt: 96.51ms | tok/sec: 84880.78\n","start training the model\n","step 0 of training\n","step  2581 | loss: 1.926926 | lr 5.8615e-04 | norm: 1.1251 | dt: 95.94ms | tok/sec: 85383.42\n","start training the model\n","step 0 of training\n","step  2582 | loss: 1.843760 | lr 5.8614e-04 | norm: 0.7742 | dt: 96.90ms | tok/sec: 84542.65\n","start training the model\n","step 0 of training\n","step  2583 | loss: 1.891858 | lr 5.8613e-04 | norm: 0.9275 | dt: 96.31ms | tok/sec: 85057.70\n","start training the model\n","step 0 of training\n","step  2584 | loss: 1.960538 | lr 5.8611e-04 | norm: 0.8000 | dt: 96.48ms | tok/sec: 84908.46\n","start training the model\n","step 0 of training\n","step  2585 | loss: 1.797338 | lr 5.8610e-04 | norm: 0.7497 | dt: 96.81ms | tok/sec: 84617.39\n","start training the model\n","step 0 of training\n","step  2586 | loss: 2.003640 | lr 5.8608e-04 | norm: 1.0094 | dt: 96.84ms | tok/sec: 84594.48\n","start training the model\n","step 0 of training\n","step  2587 | loss: 1.879194 | lr 5.8607e-04 | norm: 0.7969 | dt: 97.44ms | tok/sec: 84068.95\n","start training the model\n","step 0 of training\n","step  2588 | loss: 1.621153 | lr 5.8605e-04 | norm: 0.8900 | dt: 96.22ms | tok/sec: 85142.22\n","start training the model\n","step 0 of training\n","step  2589 | loss: 1.593599 | lr 5.8604e-04 | norm: 0.7730 | dt: 96.36ms | tok/sec: 85016.66\n","start training the model\n","step 0 of training\n","step  2590 | loss: 1.840697 | lr 5.8602e-04 | norm: 0.8685 | dt: 96.15ms | tok/sec: 85201.76\n","start training the model\n","step 0 of training\n","step  2591 | loss: 1.972689 | lr 5.8601e-04 | norm: 1.2050 | dt: 95.66ms | tok/sec: 85640.71\n","start training the model\n","step 0 of training\n","step  2592 | loss: 2.064515 | lr 5.8599e-04 | norm: 2.7519 | dt: 96.12ms | tok/sec: 85222.47\n","start training the model\n","step 0 of training\n","step  2593 | loss: 1.835138 | lr 5.8598e-04 | norm: 0.9033 | dt: 96.48ms | tok/sec: 84912.87\n","start training the model\n","step 0 of training\n","step  2594 | loss: 1.844379 | lr 5.8596e-04 | norm: 0.9322 | dt: 96.94ms | tok/sec: 84505.22\n","start training the model\n","step 0 of training\n","step  2595 | loss: 1.856504 | lr 5.8595e-04 | norm: 0.9087 | dt: 96.08ms | tok/sec: 85263.49\n","start training the model\n","step 0 of training\n","step  2596 | loss: 1.861367 | lr 5.8593e-04 | norm: 0.8919 | dt: 96.42ms | tok/sec: 84964.52\n","start training the model\n","step 0 of training\n","step  2597 | loss: 1.797727 | lr 5.8592e-04 | norm: 0.9566 | dt: 97.17ms | tok/sec: 84309.07\n","start training the model\n","step 0 of training\n","step  2598 | loss: 1.778767 | lr 5.8591e-04 | norm: 0.8805 | dt: 95.91ms | tok/sec: 85413.98\n","start training the model\n","step 0 of training\n","step  2599 | loss: 1.775015 | lr 5.8589e-04 | norm: 1.4585 | dt: 96.66ms | tok/sec: 84749.51\n","start training the model\n","step 0 of training\n","step  2600 | loss: 1.758492 | lr 5.8588e-04 | norm: 0.9029 | dt: 96.43ms | tok/sec: 84949.40\n","start training the model\n","step 0 of training\n","step  2601 | loss: 1.751270 | lr 5.8586e-04 | norm: 1.3551 | dt: 96.77ms | tok/sec: 84658.46\n","start training the model\n","step 0 of training\n","step  2602 | loss: 1.672438 | lr 5.8585e-04 | norm: 0.8259 | dt: 97.04ms | tok/sec: 84417.19\n","start training the model\n","step 0 of training\n","step  2603 | loss: 1.642425 | lr 5.8583e-04 | norm: 1.5897 | dt: 96.35ms | tok/sec: 85020.24\n","start training the model\n","step 0 of training\n","step  2604 | loss: 1.781067 | lr 5.8582e-04 | norm: 1.2614 | dt: 97.07ms | tok/sec: 84396.25\n","start training the model\n","step 0 of training\n","step  2605 | loss: 1.656666 | lr 5.8580e-04 | norm: 0.8070 | dt: 96.74ms | tok/sec: 84677.66\n","start training the model\n","step 0 of training\n","step  2606 | loss: 1.709735 | lr 5.8579e-04 | norm: 1.0202 | dt: 96.17ms | tok/sec: 85184.01\n","start training the model\n","step 0 of training\n","step  2607 | loss: 1.699670 | lr 5.8577e-04 | norm: 1.0349 | dt: 96.45ms | tok/sec: 84931.97\n","start training the model\n","step 0 of training\n","step  2608 | loss: 1.676996 | lr 5.8576e-04 | norm: 0.8947 | dt: 95.96ms | tok/sec: 85367.93\n","start training the model\n","step 0 of training\n","step  2609 | loss: 1.569001 | lr 5.8574e-04 | norm: 1.2822 | dt: 96.17ms | tok/sec: 85181.90\n","start training the model\n","step 0 of training\n","step  2610 | loss: 1.618180 | lr 5.8573e-04 | norm: 1.6052 | dt: 96.31ms | tok/sec: 85056.23\n","start training the model\n","step 0 of training\n","step  2611 | loss: 1.607687 | lr 5.8571e-04 | norm: 1.2574 | dt: 96.47ms | tok/sec: 84916.86\n","start training the model\n","step 0 of training\n","step  2612 | loss: 1.612168 | lr 5.8570e-04 | norm: 1.9420 | dt: 96.50ms | tok/sec: 84891.68\n","start training the model\n","step 0 of training\n","step  2613 | loss: 1.559479 | lr 5.8568e-04 | norm: 1.5303 | dt: 96.65ms | tok/sec: 84762.68\n","start training the model\n","step 0 of training\n","step  2614 | loss: 1.583950 | lr 5.8567e-04 | norm: 2.1070 | dt: 97.19ms | tok/sec: 84288.80\n","start training the model\n","step 0 of training\n","step  2615 | loss: 1.596669 | lr 5.8565e-04 | norm: 1.4771 | dt: 96.76ms | tok/sec: 84666.81\n","start training the model\n","step 0 of training\n","step  2616 | loss: 2.361717 | lr 5.8564e-04 | norm: 1.5125 | dt: 95.64ms | tok/sec: 85653.95\n","start training the model\n","step 0 of training\n","step  2617 | loss: 5.458965 | lr 5.8562e-04 | norm: 1.6853 | dt: 97.10ms | tok/sec: 84370.34\n","start training the model\n","step 0 of training\n","step  2618 | loss: 4.961432 | lr 5.8561e-04 | norm: 1.1401 | dt: 97.26ms | tok/sec: 84228.46\n","start training the model\n","step 0 of training\n","step  2619 | loss: 4.911837 | lr 5.8559e-04 | norm: 1.0198 | dt: 97.87ms | tok/sec: 83706.04\n","start training the model\n","step 0 of training\n","step  2620 | loss: 4.853858 | lr 5.8558e-04 | norm: 1.0353 | dt: 98.17ms | tok/sec: 83444.61\n","start training the model\n","step 0 of training\n","step  2621 | loss: 4.794312 | lr 5.8556e-04 | norm: 1.1098 | dt: 97.37ms | tok/sec: 84130.50\n","start training the model\n","step 0 of training\n","step  2622 | loss: 4.816535 | lr 5.8555e-04 | norm: 0.9844 | dt: 96.68ms | tok/sec: 84733.62\n","start training the model\n","step 0 of training\n","step  2623 | loss: 4.863151 | lr 5.8554e-04 | norm: 0.8783 | dt: 97.08ms | tok/sec: 84384.64\n","start training the model\n","step 0 of training\n","step  2624 | loss: 4.773277 | lr 5.8552e-04 | norm: 0.8565 | dt: 97.33ms | tok/sec: 84169.87\n","start training the model\n","step 0 of training\n","step  2625 | loss: 4.893869 | lr 5.8551e-04 | norm: 0.7502 | dt: 96.37ms | tok/sec: 85002.57\n","start training the model\n","step 0 of training\n","step  2626 | loss: 4.923890 | lr 5.8549e-04 | norm: 0.8293 | dt: 96.09ms | tok/sec: 85249.74\n","start training the model\n","step 0 of training\n","step  2627 | loss: 4.919521 | lr 5.8548e-04 | norm: 0.7558 | dt: 97.00ms | tok/sec: 84453.50\n","start training the model\n","step 0 of training\n","step  2628 | loss: 4.924340 | lr 5.8546e-04 | norm: 0.7909 | dt: 97.00ms | tok/sec: 84451.01\n","start training the model\n","step 0 of training\n","step  2629 | loss: 4.941142 | lr 5.8545e-04 | norm: 0.9013 | dt: 97.48ms | tok/sec: 84039.76\n","start training the model\n","step 0 of training\n","step  2630 | loss: 5.054572 | lr 5.8543e-04 | norm: 0.7190 | dt: 96.87ms | tok/sec: 84563.87\n","start training the model\n","step 0 of training\n","step  2631 | loss: 4.933388 | lr 5.8542e-04 | norm: 0.8000 | dt: 97.05ms | tok/sec: 84406.20\n","start training the model\n","step 0 of training\n","step  2632 | loss: 4.936385 | lr 5.8540e-04 | norm: 0.7230 | dt: 96.73ms | tok/sec: 84686.84\n","start training the model\n","step 0 of training\n","step  2633 | loss: 4.963785 | lr 5.8539e-04 | norm: 0.8680 | dt: 96.47ms | tok/sec: 84913.50\n","start training the model\n","step 0 of training\n","step  2634 | loss: 4.834657 | lr 5.8537e-04 | norm: 0.9551 | dt: 96.95ms | tok/sec: 84499.61\n","start training the model\n","step 0 of training\n","step  2635 | loss: 5.172026 | lr 5.8536e-04 | norm: 1.5178 | dt: 96.34ms | tok/sec: 85032.44\n","start training the model\n","step 0 of training\n","step  2636 | loss: 4.839815 | lr 5.8534e-04 | norm: 0.9817 | dt: 96.87ms | tok/sec: 84570.74\n","start training the model\n","step 0 of training\n","step  2637 | loss: 4.825267 | lr 5.8533e-04 | norm: 0.8058 | dt: 95.68ms | tok/sec: 85615.96\n","start training the model\n","step 0 of training\n","step  2638 | loss: 4.963130 | lr 5.8531e-04 | norm: 1.2687 | dt: 96.52ms | tok/sec: 84876.16\n","start training the model\n","step 0 of training\n","step  2639 | loss: 5.032985 | lr 5.8530e-04 | norm: 1.2306 | dt: 96.67ms | tok/sec: 84742.82\n","start training the model\n","step 0 of training\n","step  2640 | loss: 4.874631 | lr 5.8528e-04 | norm: 0.8996 | dt: 96.55ms | tok/sec: 84843.68\n","start training the model\n","step 0 of training\n","step  2641 | loss: 4.843955 | lr 5.8527e-04 | norm: 0.8453 | dt: 97.13ms | tok/sec: 84339.49\n","start training the model\n","step 0 of training\n","step  2642 | loss: 4.794298 | lr 5.8525e-04 | norm: 0.9350 | dt: 96.43ms | tok/sec: 84954.44\n","start training the model\n","step 0 of training\n","step  2643 | loss: 4.885106 | lr 5.8524e-04 | norm: 0.8464 | dt: 96.68ms | tok/sec: 84732.58\n","start training the model\n","step 0 of training\n","step  2644 | loss: 5.007060 | lr 5.8522e-04 | norm: 1.0814 | dt: 96.90ms | tok/sec: 84544.10\n","start training the model\n","step 0 of training\n","step  2645 | loss: 5.076087 | lr 5.8521e-04 | norm: 0.8922 | dt: 100.62ms | tok/sec: 81411.92\n","start training the model\n","step 0 of training\n","step  2646 | loss: 5.195929 | lr 5.8519e-04 | norm: 1.5064 | dt: 95.99ms | tok/sec: 85339.09\n","start training the model\n","step 0 of training\n","step  2647 | loss: 5.219429 | lr 5.8518e-04 | norm: 1.1868 | dt: 95.67ms | tok/sec: 85627.27\n","start training the model\n","step 0 of training\n","step  2648 | loss: 5.214433 | lr 5.8516e-04 | norm: 1.2992 | dt: 96.74ms | tok/sec: 84684.34\n","start training the model\n","step 0 of training\n","step  2649 | loss: 4.992198 | lr 5.8514e-04 | norm: 1.0798 | dt: 96.43ms | tok/sec: 84950.03\n","start training the model\n","step 0 of training\n","step  2650 | loss: 4.938463 | lr 5.8513e-04 | norm: 1.0211 | dt: 96.43ms | tok/sec: 84950.03\n","start training the model\n","step 0 of training\n","step  2651 | loss: 4.827362 | lr 5.8511e-04 | norm: 1.0639 | dt: 96.25ms | tok/sec: 85113.11\n","start training the model\n","step 0 of training\n","step  2652 | loss: 4.971607 | lr 5.8510e-04 | norm: 1.5638 | dt: 96.60ms | tok/sec: 84803.26\n","start training the model\n","step 0 of training\n","step  2653 | loss: 4.776759 | lr 5.8508e-04 | norm: 1.3483 | dt: 95.84ms | tok/sec: 85473.26\n","start training the model\n","step 0 of training\n","step  2654 | loss: 4.871030 | lr 5.8507e-04 | norm: 1.2380 | dt: 96.99ms | tok/sec: 84466.58\n","start training the model\n","step 0 of training\n","step  2655 | loss: 4.827824 | lr 5.8505e-04 | norm: 0.9328 | dt: 99.58ms | tok/sec: 82268.23\n","start training the model\n","step 0 of training\n","step  2656 | loss: 4.709908 | lr 5.8504e-04 | norm: 1.1499 | dt: 97.53ms | tok/sec: 83992.92\n","start training the model\n","step 0 of training\n","step  2657 | loss: 4.710074 | lr 5.8502e-04 | norm: 1.0710 | dt: 98.19ms | tok/sec: 83427.80\n","start training the model\n","step 0 of training\n","step  2658 | loss: 4.722928 | lr 5.8501e-04 | norm: 0.9394 | dt: 97.08ms | tok/sec: 84387.96\n","start training the model\n","step 0 of training\n","step  2659 | loss: 4.827268 | lr 5.8499e-04 | norm: 1.1735 | dt: 97.68ms | tok/sec: 83866.01\n","start training the model\n","step 0 of training\n","step  2660 | loss: 4.829246 | lr 5.8498e-04 | norm: 1.1320 | dt: 96.64ms | tok/sec: 84766.02\n","start training the model\n","step 0 of training\n","step  2661 | loss: 5.010404 | lr 5.8496e-04 | norm: 1.2564 | dt: 96.73ms | tok/sec: 84691.23\n","start training the model\n","step 0 of training\n","step  2662 | loss: 4.930459 | lr 5.8495e-04 | norm: 1.0941 | dt: 96.98ms | tok/sec: 84472.19\n","start training the model\n","step 0 of training\n","step  2663 | loss: 5.038107 | lr 5.8493e-04 | norm: 1.3421 | dt: 96.59ms | tok/sec: 84808.08\n","start training the model\n","step 0 of training\n","step  2664 | loss: 4.866473 | lr 5.8492e-04 | norm: 0.9964 | dt: 97.67ms | tok/sec: 83871.54\n","start training the model\n","step 0 of training\n","step  2665 | loss: 4.951449 | lr 5.8490e-04 | norm: 1.4254 | dt: 96.66ms | tok/sec: 84746.58\n","start training the model\n","step 0 of training\n","step  2666 | loss: 5.034450 | lr 5.8489e-04 | norm: 1.0634 | dt: 97.39ms | tok/sec: 84118.56\n","start training the model\n","step 0 of training\n","step  2667 | loss: 4.889468 | lr 5.8487e-04 | norm: 1.0947 | dt: 96.35ms | tok/sec: 85020.66\n","start training the model\n","step 0 of training\n","step  2668 | loss: 4.843088 | lr 5.8486e-04 | norm: 0.9991 | dt: 96.85ms | tok/sec: 84584.48\n","start training the model\n","step 0 of training\n","step  2669 | loss: 4.790071 | lr 5.8484e-04 | norm: 0.7779 | dt: 97.03ms | tok/sec: 84423.21\n","start training the model\n","step 0 of training\n","step  2670 | loss: 4.948721 | lr 5.8483e-04 | norm: 1.1785 | dt: 96.43ms | tok/sec: 84955.49\n","start training the model\n","step 0 of training\n","step  2671 | loss: 4.873060 | lr 5.8481e-04 | norm: 1.2193 | dt: 96.98ms | tok/sec: 84474.68\n","start training the model\n","step 0 of training\n","step  2672 | loss: 4.849126 | lr 5.8480e-04 | norm: 1.0922 | dt: 96.05ms | tok/sec: 85292.07\n","start training the model\n","step 0 of training\n","step  2673 | loss: 4.836915 | lr 5.8478e-04 | norm: 0.9272 | dt: 97.17ms | tok/sec: 84309.48\n","start training the model\n","step 0 of training\n","step  2674 | loss: 4.720476 | lr 5.8477e-04 | norm: 1.1412 | dt: 95.44ms | tok/sec: 85831.97\n","start training the model\n","step 0 of training\n","step  2675 | loss: 4.631113 | lr 5.8475e-04 | norm: 1.2211 | dt: 96.12ms | tok/sec: 85225.85\n","start training the model\n","step 0 of training\n","step  2676 | loss: 4.707330 | lr 5.8473e-04 | norm: 1.0643 | dt: 96.72ms | tok/sec: 84700.83\n","start training the model\n","step 0 of training\n","step  2677 | loss: 4.611562 | lr 5.8472e-04 | norm: 0.9718 | dt: 96.40ms | tok/sec: 84979.44\n","start training the model\n","step 0 of training\n","step  2678 | loss: 5.605920 | lr 5.8470e-04 | norm: 1.7226 | dt: 96.97ms | tok/sec: 84476.76\n","start training the model\n","step 0 of training\n","step  2679 | loss: 5.617861 | lr 5.8469e-04 | norm: 1.1412 | dt: 96.70ms | tok/sec: 84713.78\n","start training the model\n","step 0 of training\n","step  2680 | loss: 5.396436 | lr 5.8467e-04 | norm: 0.9735 | dt: 97.28ms | tok/sec: 84209.89\n","start training the model\n","step 0 of training\n","step  2681 | loss: 5.360985 | lr 5.8466e-04 | norm: 1.0968 | dt: 96.45ms | tok/sec: 84935.33\n","start training the model\n","step 0 of training\n","step  2682 | loss: 5.330601 | lr 5.8464e-04 | norm: 0.9958 | dt: 96.09ms | tok/sec: 85251.22\n","start training the model\n","step 0 of training\n","step  2683 | loss: 5.423162 | lr 5.8463e-04 | norm: 1.3184 | dt: 96.34ms | tok/sec: 85033.91\n","start training the model\n","step 0 of training\n","step  2684 | loss: 5.214878 | lr 5.8461e-04 | norm: 1.0150 | dt: 96.74ms | tok/sec: 84681.62\n","start training the model\n","step 0 of training\n","step  2685 | loss: 5.240183 | lr 5.8460e-04 | norm: 1.1746 | dt: 96.67ms | tok/sec: 84737.80\n","start training the model\n","step 0 of training\n","step  2686 | loss: 5.279429 | lr 5.8458e-04 | norm: 1.0490 | dt: 95.92ms | tok/sec: 85402.09\n","start training the model\n","step 0 of training\n","step  2687 | loss: 5.311550 | lr 5.8457e-04 | norm: 1.1022 | dt: 96.58ms | tok/sec: 84820.43\n","start training the model\n","step 0 of training\n","step  2688 | loss: 5.263752 | lr 5.8455e-04 | norm: 0.9422 | dt: 96.73ms | tok/sec: 84688.93\n","start training the model\n","step 0 of training\n","step  2689 | loss: 5.252035 | lr 5.8453e-04 | norm: 0.8506 | dt: 96.65ms | tok/sec: 84755.15\n","start training the model\n","step 0 of training\n","step  2690 | loss: 5.223405 | lr 5.8452e-04 | norm: 0.9630 | dt: 96.85ms | tok/sec: 84587.19\n","start training the model\n","step 0 of training\n","step  2691 | loss: 5.276845 | lr 5.8450e-04 | norm: 1.1710 | dt: 96.96ms | tok/sec: 84484.86\n","start training the model\n","step 0 of training\n","step  2692 | loss: 5.105240 | lr 5.8449e-04 | norm: 1.0418 | dt: 96.44ms | tok/sec: 84947.09\n","start training the model\n","step 0 of training\n","step  2693 | loss: 5.204396 | lr 5.8447e-04 | norm: 1.2194 | dt: 96.28ms | tok/sec: 85086.35\n","start training the model\n","step 0 of training\n","step  2694 | loss: 5.189912 | lr 5.8446e-04 | norm: 1.2602 | dt: 96.52ms | tok/sec: 84873.65\n","start training the model\n","step 0 of training\n","step  2695 | loss: 5.090493 | lr 5.8444e-04 | norm: 1.0440 | dt: 97.22ms | tok/sec: 84266.68\n","start training the model\n","step 0 of training\n","step  2696 | loss: 5.570356 | lr 5.8443e-04 | norm: 1.7964 | dt: 95.83ms | tok/sec: 85481.98\n","start training the model\n","step 0 of training\n","step  2697 | loss: 5.239281 | lr 5.8441e-04 | norm: 1.6786 | dt: 96.52ms | tok/sec: 84870.08\n","start training the model\n","step 0 of training\n","step  2698 | loss: 4.626448 | lr 5.8440e-04 | norm: 1.8764 | dt: 96.45ms | tok/sec: 84933.23\n","start training the model\n","step 0 of training\n","step  2699 | loss: 4.904626 | lr 5.8438e-04 | norm: 1.8979 | dt: 96.36ms | tok/sec: 85014.77\n","start training the model\n","step 0 of training\n","step  2700 | loss: 4.117579 | lr 5.8437e-04 | norm: 1.7533 | dt: 96.55ms | tok/sec: 84844.52\n","start training the model\n","step 0 of training\n","step  2701 | loss: 4.192091 | lr 5.8435e-04 | norm: 1.3706 | dt: 96.19ms | tok/sec: 85165.01\n","start training the model\n","step 0 of training\n","step  2702 | loss: 4.079020 | lr 5.8433e-04 | norm: 1.1787 | dt: 97.47ms | tok/sec: 84049.62\n","start training the model\n","step 0 of training\n","step  2703 | loss: 4.552170 | lr 5.8432e-04 | norm: 1.5896 | dt: 96.55ms | tok/sec: 84848.71\n","start training the model\n","step 0 of training\n","step  2704 | loss: 4.223471 | lr 5.8430e-04 | norm: 1.2342 | dt: 96.85ms | tok/sec: 84580.94\n","start training the model\n","step 0 of training\n","step  2705 | loss: 3.447264 | lr 5.8429e-04 | norm: 1.1482 | dt: 96.07ms | tok/sec: 85267.73\n","start training the model\n","step 0 of training\n","step  2706 | loss: 2.051289 | lr 5.8427e-04 | norm: 1.8428 | dt: 95.97ms | tok/sec: 85362.42\n","start training the model\n","step 0 of training\n","step  2707 | loss: 1.925083 | lr 5.8426e-04 | norm: 1.5604 | dt: 96.17ms | tok/sec: 85180.63\n","start training the model\n","step 0 of training\n","step  2708 | loss: 1.825909 | lr 5.8424e-04 | norm: 1.2991 | dt: 96.11ms | tok/sec: 85237.27\n","start training the model\n","step 0 of training\n","step  2709 | loss: 1.867469 | lr 5.8423e-04 | norm: 1.7736 | dt: 95.95ms | tok/sec: 85376.42\n","start training the model\n","step 0 of training\n","step  2710 | loss: 1.935375 | lr 5.8421e-04 | norm: 1.6611 | dt: 97.54ms | tok/sec: 83981.83\n","start training the model\n","step 0 of training\n","step  2711 | loss: 1.772803 | lr 5.8419e-04 | norm: 1.1067 | dt: 98.43ms | tok/sec: 83230.56\n","start training the model\n","step 0 of training\n","step  2712 | loss: 1.975185 | lr 5.8418e-04 | norm: 1.1570 | dt: 96.79ms | tok/sec: 84635.53\n","start training the model\n","step 0 of training\n","step  2713 | loss: 1.845188 | lr 5.8416e-04 | norm: 1.0056 | dt: 97.15ms | tok/sec: 84324.38\n","start training the model\n","step 0 of training\n","step  2714 | loss: 1.610718 | lr 5.8415e-04 | norm: 1.3395 | dt: 96.68ms | tok/sec: 84735.92\n","start training the model\n","step 0 of training\n","step  2715 | loss: 1.590566 | lr 5.8413e-04 | norm: 0.9382 | dt: 96.96ms | tok/sec: 84486.94\n","start training the model\n","step 0 of training\n","step  2716 | loss: 1.836628 | lr 5.8412e-04 | norm: 1.1258 | dt: 97.27ms | tok/sec: 84216.08\n","start training the model\n","step 0 of training\n","step  2717 | loss: 1.961606 | lr 5.8410e-04 | norm: 0.9510 | dt: 97.04ms | tok/sec: 84417.81\n","start training the model\n","step 0 of training\n","step  2718 | loss: 2.081494 | lr 5.8409e-04 | norm: 1.6311 | dt: 97.84ms | tok/sec: 83730.92\n","start training the model\n","step 0 of training\n","step  2719 | loss: 1.795536 | lr 5.8407e-04 | norm: 0.9107 | dt: 96.04ms | tok/sec: 85295.03\n","start training the model\n","step 0 of training\n","step  2720 | loss: 1.810620 | lr 5.8405e-04 | norm: 0.9364 | dt: 96.82ms | tok/sec: 84609.47\n","start training the model\n","step 0 of training\n","step  2721 | loss: 1.814997 | lr 5.8404e-04 | norm: 1.1623 | dt: 96.65ms | tok/sec: 84758.71\n","start training the model\n","step 0 of training\n","step  2722 | loss: 1.806425 | lr 5.8402e-04 | norm: 0.8051 | dt: 96.38ms | tok/sec: 84996.68\n","start training the model\n","step 0 of training\n","step  2723 | loss: 1.737843 | lr 5.8401e-04 | norm: 0.8610 | dt: 96.71ms | tok/sec: 84702.92\n","start training the model\n","step 0 of training\n","step  2724 | loss: 1.721906 | lr 5.8399e-04 | norm: 0.8312 | dt: 96.76ms | tok/sec: 84659.92\n","start training the model\n","step 0 of training\n","step  2725 | loss: 1.752839 | lr 5.8398e-04 | norm: 1.1528 | dt: 96.96ms | tok/sec: 84492.75\n","start training the model\n","step 0 of training\n","step  2726 | loss: 1.711706 | lr 5.8396e-04 | norm: 1.2500 | dt: 96.52ms | tok/sec: 84874.28\n","start training the model\n","step 0 of training\n","step  2727 | loss: 1.715263 | lr 5.8394e-04 | norm: 0.9025 | dt: 97.23ms | tok/sec: 84249.94\n","start training the model\n","step 0 of training\n","step  2728 | loss: 1.633853 | lr 5.8393e-04 | norm: 0.8391 | dt: 96.45ms | tok/sec: 84933.86\n","start training the model\n","step 0 of training\n","step  2729 | loss: 1.621046 | lr 5.8391e-04 | norm: 0.8204 | dt: 95.66ms | tok/sec: 85639.64\n","start training the model\n","step 0 of training\n","step  2730 | loss: 1.737981 | lr 5.8390e-04 | norm: 0.8656 | dt: 96.50ms | tok/sec: 84887.49\n","start training the model\n","step 0 of training\n","step  2731 | loss: 1.612669 | lr 5.8388e-04 | norm: 0.8774 | dt: 96.25ms | tok/sec: 85110.79\n","start training the model\n","step 0 of training\n","step  2732 | loss: 1.664942 | lr 5.8387e-04 | norm: 1.0674 | dt: 96.08ms | tok/sec: 85265.82\n","start training the model\n","step 0 of training\n","step  2733 | loss: 1.645638 | lr 5.8385e-04 | norm: 1.1299 | dt: 96.48ms | tok/sec: 84910.56\n","start training the model\n","step 0 of training\n","step  2734 | loss: 1.628603 | lr 5.8383e-04 | norm: 1.2425 | dt: 96.39ms | tok/sec: 84991.21\n","start training the model\n","step 0 of training\n","step  2735 | loss: 1.550402 | lr 5.8382e-04 | norm: 2.3705 | dt: 97.18ms | tok/sec: 84294.59\n","start training the model\n","step 0 of training\n","step  2736 | loss: 1.607273 | lr 5.8380e-04 | norm: 1.5500 | dt: 96.15ms | tok/sec: 85203.87\n","start training the model\n","step 0 of training\n","step  2737 | loss: 1.592566 | lr 5.8379e-04 | norm: 1.9838 | dt: 96.42ms | tok/sec: 84960.95\n","start training the model\n","step 0 of training\n","step  2738 | loss: 1.601584 | lr 5.8377e-04 | norm: 1.3809 | dt: 97.30ms | tok/sec: 84193.17\n","start training the model\n","step 0 of training\n","step  2739 | loss: 1.542126 | lr 5.8376e-04 | norm: 1.8701 | dt: 96.68ms | tok/sec: 84733.83\n","start training the model\n","step 0 of training\n","step  2740 | loss: 1.572556 | lr 5.8374e-04 | norm: 1.6150 | dt: 96.85ms | tok/sec: 84583.02\n","start training the model\n","step 0 of training\n","step  2741 | loss: 1.558360 | lr 5.8372e-04 | norm: 1.3606 | dt: 96.62ms | tok/sec: 84787.57\n","start training the model\n","step 0 of training\n","step  2742 | loss: 2.317561 | lr 5.8371e-04 | norm: 1.2748 | dt: 96.76ms | tok/sec: 84663.05\n","start training the model\n","step 0 of training\n","step  2743 | loss: 5.384902 | lr 5.8369e-04 | norm: 1.3873 | dt: 96.65ms | tok/sec: 84756.82\n","start training the model\n","step 0 of training\n","step  2744 | loss: 4.862496 | lr 5.8368e-04 | norm: 1.3034 | dt: 96.17ms | tok/sec: 85181.06\n","start training the model\n","step 0 of training\n","step  2745 | loss: 4.789852 | lr 5.8366e-04 | norm: 0.8924 | dt: 97.02ms | tok/sec: 84433.79\n","start training the model\n","step 0 of training\n","step  2746 | loss: 4.738770 | lr 5.8364e-04 | norm: 0.8905 | dt: 95.35ms | tok/sec: 85912.66\n","start training the model\n","step 0 of training\n","step  2747 | loss: 4.687498 | lr 5.8363e-04 | norm: 0.9536 | dt: 96.44ms | tok/sec: 84940.79\n","start training the model\n","step 0 of training\n","step  2748 | loss: 4.742416 | lr 5.8361e-04 | norm: 0.9420 | dt: 96.38ms | tok/sec: 84993.32\n","start training the model\n","step 0 of training\n","step  2749 | loss: 4.775371 | lr 5.8360e-04 | norm: 0.8305 | dt: 95.95ms | tok/sec: 85377.48\n","validation loss: 6.0375\n","rank 0 sample 0: [CLS] دردم از یار است و درمان نیز هم[SEP] است[BOB][BOM] که از خود بر آستان این مجلس[BOM] دیده از تو است از این دل با دوست به دل\n","rank 0 sample 1: [CLS] دردم از یار است و درمان نیز هم[SEP] و که دست و این که این راز[BOB][BOM] مرا گفتم و به کار، به امیدت که می لعل\n","rank 0 sample 2: [CLS] دردم از یار است و درمان نیز هم[SEP][BOB][BOM] وز پی سخن گوی کن از این معما هستی و این است؟[BOM] هر کس را که در این\n","rank 0 sample 3: [CLS] دردم از یار است و درمان نیز هم[SEP] و هم بخت و در عالم[BOB][BOM] چو آمد[BOM] که نه[BOM] دل[BOB][BOM] گر صد نورش از\n","start training the model\n","step 0 of training\n","step  2750 | loss: 4.699391 | lr 5.8358e-04 | norm: 0.9976 | dt: 1374.62ms | tok/sec: 5959.48\n","start training the model\n","step 0 of training\n","step  2751 | loss: 4.819266 | lr 5.8357e-04 | norm: 0.8935 | dt: 96.20ms | tok/sec: 85156.99\n","start training the model\n","step 0 of training\n","step  2752 | loss: 4.837433 | lr 5.8355e-04 | norm: 0.8001 | dt: 96.54ms | tok/sec: 84859.39\n","start training the model\n","step 0 of training\n","step  2753 | loss: 4.844329 | lr 5.8353e-04 | norm: 0.7169 | dt: 95.62ms | tok/sec: 85675.09\n","start training the model\n","step 0 of training\n","step  2754 | loss: 4.859057 | lr 5.8352e-04 | norm: 0.8669 | dt: 95.98ms | tok/sec: 85352.24\n","start training the model\n","step 0 of training\n","step  2755 | loss: 4.847038 | lr 5.8350e-04 | norm: 0.9250 | dt: 96.38ms | tok/sec: 84994.37\n","start training the model\n","step 0 of training\n","step  2756 | loss: 4.976554 | lr 5.8349e-04 | norm: 0.8469 | dt: 96.45ms | tok/sec: 84936.38\n","start training the model\n","step 0 of training\n","step  2757 | loss: 4.851537 | lr 5.8347e-04 | norm: 0.9156 | dt: 97.34ms | tok/sec: 84155.23\n","start training the model\n","step 0 of training\n","step  2758 | loss: 4.851565 | lr 5.8345e-04 | norm: 0.8518 | dt: 95.92ms | tok/sec: 85404.21\n","start training the model\n","step 0 of training\n","step  2759 | loss: 4.876004 | lr 5.8344e-04 | norm: 0.8939 | dt: 96.03ms | tok/sec: 85306.47\n","start training the model\n","step 0 of training\n","step  2760 | loss: 4.741439 | lr 5.8342e-04 | norm: 0.9187 | dt: 96.84ms | tok/sec: 84589.27\n","start training the model\n","step 0 of training\n","step  2761 | loss: 5.071239 | lr 5.8341e-04 | norm: 1.6647 | dt: 95.39ms | tok/sec: 85878.52\n","start training the model\n","step 0 of training\n","step  2762 | loss: 4.766802 | lr 5.8339e-04 | norm: 1.5421 | dt: 96.85ms | tok/sec: 84584.27\n","start training the model\n","step 0 of training\n","step  2763 | loss: 4.754564 | lr 5.8337e-04 | norm: 0.9914 | dt: 98.74ms | tok/sec: 82964.88\n","start training the model\n","step 0 of training\n","step  2764 | loss: 4.911143 | lr 5.8336e-04 | norm: 0.8267 | dt: 97.77ms | tok/sec: 83792.39\n","start training the model\n","step 0 of training\n","step  2765 | loss: 4.978323 | lr 5.8334e-04 | norm: 1.2049 | dt: 98.17ms | tok/sec: 83444.41\n","start training the model\n","step 0 of training\n","step  2766 | loss: 4.796244 | lr 5.8333e-04 | norm: 1.1693 | dt: 98.30ms | tok/sec: 83339.57\n","start training the model\n","step 0 of training\n","step  2767 | loss: 4.779244 | lr 5.8331e-04 | norm: 0.9342 | dt: 96.91ms | tok/sec: 84532.46\n","start training the model\n","step 0 of training\n","step  2768 | loss: 4.718387 | lr 5.8330e-04 | norm: 0.7975 | dt: 97.02ms | tok/sec: 84437.11\n","start training the model\n","step 0 of training\n","step  2769 | loss: 4.801285 | lr 5.8328e-04 | norm: 1.0333 | dt: 97.43ms | tok/sec: 84079.86\n","start training the model\n","step 0 of training\n","step  2770 | loss: 4.947777 | lr 5.8326e-04 | norm: 1.1195 | dt: 97.19ms | tok/sec: 84288.38\n","start training the model\n","step 0 of training\n","step  2771 | loss: 4.976511 | lr 5.8325e-04 | norm: 1.0211 | dt: 99.99ms | tok/sec: 81931.60\n","start training the model\n","step 0 of training\n","step  2772 | loss: 5.125631 | lr 5.8323e-04 | norm: 1.2230 | dt: 96.01ms | tok/sec: 85327.23\n","start training the model\n","step 0 of training\n","step  2773 | loss: 5.148651 | lr 5.8322e-04 | norm: 1.2804 | dt: 96.41ms | tok/sec: 84966.83\n","start training the model\n","step 0 of training\n","step  2774 | loss: 5.131688 | lr 5.8320e-04 | norm: 1.2531 | dt: 96.57ms | tok/sec: 84826.92\n","start training the model\n","step 0 of training\n","step  2775 | loss: 4.933145 | lr 5.8318e-04 | norm: 1.4125 | dt: 95.92ms | tok/sec: 85402.09\n","start training the model\n","step 0 of training\n","step  2776 | loss: 4.876858 | lr 5.8317e-04 | norm: 1.3132 | dt: 96.80ms | tok/sec: 84627.19\n","start training the model\n","step 0 of training\n","step  2777 | loss: 4.755676 | lr 5.8315e-04 | norm: 1.1940 | dt: 96.54ms | tok/sec: 84851.85\n","start training the model\n","step 0 of training\n","step  2778 | loss: 4.954679 | lr 5.8313e-04 | norm: 1.1336 | dt: 96.47ms | tok/sec: 84917.91\n","start training the model\n","step 0 of training\n","step  2779 | loss: 4.755150 | lr 5.8312e-04 | norm: 1.4892 | dt: 96.29ms | tok/sec: 85078.97\n","start training the model\n","step 0 of training\n","step  2780 | loss: 4.860016 | lr 5.8310e-04 | norm: 1.2198 | dt: 97.14ms | tok/sec: 84333.69\n","start training the model\n","step 0 of training\n","step  2781 | loss: 4.800954 | lr 5.8309e-04 | norm: 1.0691 | dt: 98.08ms | tok/sec: 83527.38\n","start training the model\n","step 0 of training\n","step  2782 | loss: 4.667890 | lr 5.8307e-04 | norm: 1.3105 | dt: 97.04ms | tok/sec: 84416.98\n","start training the model\n","step 0 of training\n","step  2783 | loss: 4.654478 | lr 5.8305e-04 | norm: 0.9906 | dt: 97.16ms | tok/sec: 84317.96\n","start training the model\n","step 0 of training\n","step  2784 | loss: 4.694423 | lr 5.8304e-04 | norm: 1.0617 | dt: 98.07ms | tok/sec: 83535.91\n","start training the model\n","step 0 of training\n","step  2785 | loss: 4.788380 | lr 5.8302e-04 | norm: 1.0271 | dt: 98.05ms | tok/sec: 83549.11\n","start training the model\n","step 0 of training\n","step  2786 | loss: 4.760905 | lr 5.8301e-04 | norm: 0.8404 | dt: 97.53ms | tok/sec: 83990.86\n","start training the model\n","step 0 of training\n","step  2787 | loss: 4.950359 | lr 5.8299e-04 | norm: 1.1929 | dt: 96.93ms | tok/sec: 84510.83\n","start training the model\n","step 0 of training\n","step  2788 | loss: 4.859554 | lr 5.8297e-04 | norm: 1.0525 | dt: 98.18ms | tok/sec: 83438.74\n","start training the model\n","step 0 of training\n","step  2789 | loss: 5.000448 | lr 5.8296e-04 | norm: 1.0235 | dt: 96.53ms | tok/sec: 84860.65\n","start training the model\n","step 0 of training\n","step  2790 | loss: 4.792076 | lr 5.8294e-04 | norm: 0.9928 | dt: 97.24ms | tok/sec: 84242.71\n","start training the model\n","step 0 of training\n","step  2791 | loss: 4.933615 | lr 5.8293e-04 | norm: 1.3280 | dt: 96.48ms | tok/sec: 84905.74\n","start training the model\n","step 0 of training\n","step  2792 | loss: 4.986201 | lr 5.8291e-04 | norm: 1.2263 | dt: 96.74ms | tok/sec: 84676.82\n","start training the model\n","step 0 of training\n","step  2793 | loss: 4.866747 | lr 5.8289e-04 | norm: 1.3894 | dt: 97.82ms | tok/sec: 83748.07\n","start training the model\n","step 0 of training\n","step  2794 | loss: 4.796605 | lr 5.8288e-04 | norm: 1.0726 | dt: 96.65ms | tok/sec: 84755.57\n","start training the model\n","step 0 of training\n","step  2795 | loss: 4.740665 | lr 5.8286e-04 | norm: 0.9021 | dt: 97.12ms | tok/sec: 84349.01\n","start training the model\n","step 0 of training\n","step  2796 | loss: 4.894733 | lr 5.8284e-04 | norm: 0.9524 | dt: 96.25ms | tok/sec: 85109.74\n","start training the model\n","step 0 of training\n","step  2797 | loss: 4.838229 | lr 5.8283e-04 | norm: 1.1271 | dt: 96.92ms | tok/sec: 84526.22\n","start training the model\n","step 0 of training\n","step  2798 | loss: 4.787215 | lr 5.8281e-04 | norm: 1.1136 | dt: 95.72ms | tok/sec: 85579.92\n","start training the model\n","step 0 of training\n","step  2799 | loss: 4.787202 | lr 5.8280e-04 | norm: 1.3829 | dt: 97.45ms | tok/sec: 84067.72\n","start training the model\n","step 0 of training\n","step  2800 | loss: 4.660398 | lr 5.8278e-04 | norm: 1.3567 | dt: 97.95ms | tok/sec: 83635.95\n","start training the model\n","step 0 of training\n","step  2801 | loss: 4.569209 | lr 5.8276e-04 | norm: 1.1367 | dt: 97.33ms | tok/sec: 84170.28\n","start training the model\n","step 0 of training\n","step  2802 | loss: 4.619761 | lr 5.8275e-04 | norm: 0.8702 | dt: 97.20ms | tok/sec: 84279.08\n","start training the model\n","step 0 of training\n","step  2803 | loss: 4.523553 | lr 5.8273e-04 | norm: 1.2220 | dt: 96.79ms | tok/sec: 84635.94\n","start training the model\n","step 0 of training\n","step  2804 | loss: 5.561851 | lr 5.8271e-04 | norm: 1.3000 | dt: 96.65ms | tok/sec: 84756.20\n","start training the model\n","step 0 of training\n","step  2805 | loss: 5.564981 | lr 5.8270e-04 | norm: 1.3519 | dt: 96.71ms | tok/sec: 84708.76\n","start training the model\n","step 0 of training\n","step  2806 | loss: 5.337485 | lr 5.8268e-04 | norm: 0.9879 | dt: 96.94ms | tok/sec: 84503.35\n","start training the model\n","step 0 of training\n","step  2807 | loss: 5.312778 | lr 5.8267e-04 | norm: 0.9218 | dt: 96.55ms | tok/sec: 84843.68\n","start training the model\n","step 0 of training\n","step  2808 | loss: 5.257889 | lr 5.8265e-04 | norm: 1.1647 | dt: 97.94ms | tok/sec: 83644.70\n","start training the model\n","step 0 of training\n","step  2809 | loss: 5.355957 | lr 5.8263e-04 | norm: 0.9971 | dt: 96.69ms | tok/sec: 84726.10\n","start training the model\n","step 0 of training\n","step  2810 | loss: 5.117533 | lr 5.8262e-04 | norm: 1.2079 | dt: 97.42ms | tok/sec: 84091.79\n","start training the model\n","step 0 of training\n","step  2811 | loss: 5.170020 | lr 5.8260e-04 | norm: 1.0544 | dt: 96.36ms | tok/sec: 85016.87\n","start training the model\n","step 0 of training\n","step  2812 | loss: 5.211372 | lr 5.8258e-04 | norm: 1.1373 | dt: 96.19ms | tok/sec: 85167.54\n","start training the model\n","step 0 of training\n","step  2813 | loss: 5.223469 | lr 5.8257e-04 | norm: 1.3244 | dt: 98.14ms | tok/sec: 83475.83\n","start training the model\n","step 0 of training\n","step  2814 | loss: 5.187453 | lr 5.8255e-04 | norm: 1.1769 | dt: 96.57ms | tok/sec: 84828.60\n","start training the model\n","step 0 of training\n","step  2815 | loss: 5.195859 | lr 5.8254e-04 | norm: 1.2815 | dt: 96.63ms | tok/sec: 84773.34\n","start training the model\n","step 0 of training\n","step  2816 | loss: 5.154464 | lr 5.8252e-04 | norm: 1.2798 | dt: 96.10ms | tok/sec: 85241.71\n","start training the model\n","step 0 of training\n","step  2817 | loss: 5.173237 | lr 5.8250e-04 | norm: 1.0063 | dt: 97.08ms | tok/sec: 84384.85\n","start training the model\n","step 0 of training\n","step  2818 | loss: 5.003593 | lr 5.8249e-04 | norm: 0.9750 | dt: 96.24ms | tok/sec: 85123.66\n","start training the model\n","step 0 of training\n","step  2819 | loss: 5.133345 | lr 5.8247e-04 | norm: 1.0970 | dt: 97.52ms | tok/sec: 84001.75\n","start training the model\n","step 0 of training\n","step  2820 | loss: 5.106495 | lr 5.8245e-04 | norm: 1.2659 | dt: 97.39ms | tok/sec: 84115.88\n","start training the model\n","step 0 of training\n","step  2821 | loss: 5.018958 | lr 5.8244e-04 | norm: 1.2644 | dt: 98.43ms | tok/sec: 83226.32\n","start training the model\n","step 0 of training\n","step  2822 | loss: 5.338283 | lr 5.8242e-04 | norm: 1.7578 | dt: 97.48ms | tok/sec: 84035.65\n","start training the model\n","step 0 of training\n","step  2823 | loss: 5.097270 | lr 5.8240e-04 | norm: 1.8543 | dt: 98.11ms | tok/sec: 83493.89\n","start training the model\n","step 0 of training\n","step  2824 | loss: 4.465300 | lr 5.8239e-04 | norm: 1.8747 | dt: 97.28ms | tok/sec: 84211.95\n","start training the model\n","step 0 of training\n","step  2825 | loss: 4.872813 | lr 5.8237e-04 | norm: 2.3984 | dt: 98.27ms | tok/sec: 83359.79\n","start training the model\n","step 0 of training\n","step  2826 | loss: 4.049445 | lr 5.8236e-04 | norm: 1.7843 | dt: 97.60ms | tok/sec: 83930.75\n","start training the model\n","step 0 of training\n","step  2827 | loss: 4.067020 | lr 5.8234e-04 | norm: 1.2810 | dt: 98.18ms | tok/sec: 83434.48\n","start training the model\n","step 0 of training\n","step  2828 | loss: 3.944497 | lr 5.8232e-04 | norm: 1.2567 | dt: 96.93ms | tok/sec: 84514.16\n","start training the model\n","step 0 of training\n","step  2829 | loss: 4.438575 | lr 5.8231e-04 | norm: 1.1395 | dt: 97.33ms | tok/sec: 84169.66\n","start training the model\n","step 0 of training\n","step  2830 | loss: 4.096138 | lr 5.8229e-04 | norm: 0.9288 | dt: 97.11ms | tok/sec: 84358.74\n","start training the model\n","step 0 of training\n","step  2831 | loss: 3.347649 | lr 5.8227e-04 | norm: 1.0103 | dt: 96.64ms | tok/sec: 84768.12\n","start training the model\n","step 0 of training\n","step  2832 | loss: 1.959871 | lr 5.8226e-04 | norm: 1.0110 | dt: 96.75ms | tok/sec: 84668.48\n","start training the model\n","step 0 of training\n","step  2833 | loss: 1.851339 | lr 5.8224e-04 | norm: 0.8021 | dt: 96.72ms | tok/sec: 84698.53\n","start training the model\n","step 0 of training\n","step  2834 | loss: 1.767894 | lr 5.8222e-04 | norm: 0.9334 | dt: 97.54ms | tok/sec: 83984.91\n","start training the model\n","step 0 of training\n","step  2835 | loss: 1.857285 | lr 5.8221e-04 | norm: 2.5878 | dt: 96.16ms | tok/sec: 85192.67\n","start training the model\n","step 0 of training\n","step  2836 | loss: 1.895811 | lr 5.8219e-04 | norm: 0.9968 | dt: 97.68ms | tok/sec: 83863.56\n","start training the model\n","step 0 of training\n","step  2837 | loss: 1.709911 | lr 5.8217e-04 | norm: 1.5495 | dt: 98.18ms | tok/sec: 83434.89\n","start training the model\n","step 0 of training\n","step  2838 | loss: 1.934339 | lr 5.8216e-04 | norm: 1.2103 | dt: 97.66ms | tok/sec: 83886.69\n","start training the model\n","step 0 of training\n","step  2839 | loss: 1.797439 | lr 5.8214e-04 | norm: 0.9082 | dt: 97.78ms | tok/sec: 83779.51\n","start training the model\n","step 0 of training\n","step  2840 | loss: 1.571798 | lr 5.8212e-04 | norm: 1.2095 | dt: 96.99ms | tok/sec: 84464.30\n","start training the model\n","step 0 of training\n","step  2841 | loss: 1.533950 | lr 5.8211e-04 | norm: 0.9117 | dt: 97.09ms | tok/sec: 84371.79\n","start training the model\n","step 0 of training\n","step  2842 | loss: 1.785025 | lr 5.8209e-04 | norm: 1.4265 | dt: 96.89ms | tok/sec: 84552.63\n","start training the model\n","step 0 of training\n","step  2843 | loss: 1.894850 | lr 5.8208e-04 | norm: 1.1046 | dt: 97.37ms | tok/sec: 84133.18\n","start training the model\n","step 0 of training\n","step  2844 | loss: 2.004705 | lr 5.8206e-04 | norm: 1.4408 | dt: 96.91ms | tok/sec: 84528.92\n","start training the model\n","step 0 of training\n","step  2845 | loss: 1.793132 | lr 5.8204e-04 | norm: 2.0244 | dt: 97.41ms | tok/sec: 84101.26\n","start training the model\n","step 0 of training\n","step  2846 | loss: 1.802775 | lr 5.8203e-04 | norm: 1.2678 | dt: 96.58ms | tok/sec: 84824.41\n","start training the model\n","step 0 of training\n","step  2847 | loss: 1.813941 | lr 5.8201e-04 | norm: 1.1130 | dt: 97.28ms | tok/sec: 84207.61\n","start training the model\n","step 0 of training\n","step  2848 | loss: 1.778274 | lr 5.8199e-04 | norm: 1.1641 | dt: 96.14ms | tok/sec: 85213.17\n","start training the model\n","step 0 of training\n","step  2849 | loss: 1.700784 | lr 5.8198e-04 | norm: 0.8562 | dt: 96.68ms | tok/sec: 84730.28\n","start training the model\n","step 0 of training\n","step  2850 | loss: 1.700403 | lr 5.8196e-04 | norm: 1.3084 | dt: 96.99ms | tok/sec: 84465.96\n","start training the model\n","step 0 of training\n","step  2851 | loss: 1.705769 | lr 5.8194e-04 | norm: 1.1182 | dt: 95.98ms | tok/sec: 85347.15\n","start training the model\n","step 0 of training\n","step  2852 | loss: 1.676415 | lr 5.8193e-04 | norm: 1.0272 | dt: 96.89ms | tok/sec: 84550.97\n","start training the model\n","step 0 of training\n","step  2853 | loss: 1.666118 | lr 5.8191e-04 | norm: 1.2490 | dt: 96.48ms | tok/sec: 84912.45\n","start training the model\n","step 0 of training\n","step  2854 | loss: 1.598427 | lr 5.8189e-04 | norm: 1.2485 | dt: 95.58ms | tok/sec: 85705.86\n","start training the model\n","step 0 of training\n","step  2855 | loss: 1.575459 | lr 5.8188e-04 | norm: 1.0901 | dt: 96.89ms | tok/sec: 84553.67\n","start training the model\n","step 0 of training\n","step  2856 | loss: 1.692782 | lr 5.8186e-04 | norm: 1.2885 | dt: 97.07ms | tok/sec: 84392.52\n","start training the model\n","step 0 of training\n","step  2857 | loss: 1.589883 | lr 5.8184e-04 | norm: 1.3816 | dt: 97.89ms | tok/sec: 83682.80\n","start training the model\n","step 0 of training\n","step  2858 | loss: 1.646603 | lr 5.8183e-04 | norm: 1.6204 | dt: 97.80ms | tok/sec: 83760.32\n","start training the model\n","step 0 of training\n","step  2859 | loss: 1.630577 | lr 5.8181e-04 | norm: 1.0390 | dt: 97.75ms | tok/sec: 83809.76\n","start training the model\n","step 0 of training\n","step  2860 | loss: 1.617666 | lr 5.8179e-04 | norm: 1.7287 | dt: 97.13ms | tok/sec: 84336.59\n","start training the model\n","step 0 of training\n","step  2861 | loss: 1.548909 | lr 5.8178e-04 | norm: 1.8891 | dt: 97.67ms | tok/sec: 83872.97\n","start training the model\n","step 0 of training\n","step  2862 | loss: 1.583956 | lr 5.8176e-04 | norm: 1.3453 | dt: 96.76ms | tok/sec: 84660.13\n","start training the model\n","step 0 of training\n","step  2863 | loss: 1.583785 | lr 5.8174e-04 | norm: 1.8903 | dt: 97.52ms | tok/sec: 84004.01\n","start training the model\n","step 0 of training\n","step  2864 | loss: 1.570190 | lr 5.8173e-04 | norm: 1.3138 | dt: 97.03ms | tok/sec: 84426.32\n","start training the model\n","step 0 of training\n","step  2865 | loss: 1.529001 | lr 5.8171e-04 | norm: 1.6268 | dt: 96.73ms | tok/sec: 84685.17\n","start training the model\n","step 0 of training\n","step  2866 | loss: 1.534804 | lr 5.8169e-04 | norm: 1.1860 | dt: 97.07ms | tok/sec: 84396.46\n","start training the model\n","step 0 of training\n","step  2867 | loss: 1.538722 | lr 5.8168e-04 | norm: 1.3121 | dt: 97.01ms | tok/sec: 84441.05\n","start training the model\n","step 0 of training\n","step  2868 | loss: 2.287121 | lr 5.8166e-04 | norm: 1.1306 | dt: 98.39ms | tok/sec: 83256.98\n","start training the model\n","step 0 of training\n","step  2869 | loss: 5.309432 | lr 5.8164e-04 | norm: 1.7612 | dt: 96.28ms | tok/sec: 85081.29\n","start training the model\n","step 0 of training\n","step  2870 | loss: 4.796199 | lr 5.8163e-04 | norm: 1.4225 | dt: 96.39ms | tok/sec: 84984.70\n","start training the model\n","step 0 of training\n","step  2871 | loss: 4.716096 | lr 5.8161e-04 | norm: 1.2973 | dt: 96.91ms | tok/sec: 84535.16\n","start training the model\n","step 0 of training\n","step  2872 | loss: 4.656665 | lr 5.8159e-04 | norm: 1.2059 | dt: 96.18ms | tok/sec: 85171.55\n","start training the model\n","step 0 of training\n","step  2873 | loss: 4.614522 | lr 5.8158e-04 | norm: 1.1461 | dt: 97.15ms | tok/sec: 84324.58\n","start training the model\n","step 0 of training\n","step  2874 | loss: 4.677706 | lr 5.8156e-04 | norm: 1.1125 | dt: 96.41ms | tok/sec: 84968.31\n","start training the model\n","step 0 of training\n","step  2875 | loss: 4.699629 | lr 5.8154e-04 | norm: 0.9603 | dt: 96.80ms | tok/sec: 84625.73\n","start training the model\n","step 0 of training\n","step  2876 | loss: 4.601758 | lr 5.8153e-04 | norm: 0.8378 | dt: 96.25ms | tok/sec: 85109.11\n","start training the model\n","step 0 of training\n","step  2877 | loss: 4.733037 | lr 5.8151e-04 | norm: 0.8772 | dt: 97.26ms | tok/sec: 84231.77\n","start training the model\n","step 0 of training\n","step  2878 | loss: 4.750918 | lr 5.8149e-04 | norm: 0.9531 | dt: 96.56ms | tok/sec: 84836.77\n","start training the model\n","step 0 of training\n","step  2879 | loss: 4.767571 | lr 5.8148e-04 | norm: 0.9668 | dt: 97.14ms | tok/sec: 84334.73\n","start training the model\n","step 0 of training\n","step  2880 | loss: 4.776996 | lr 5.8146e-04 | norm: 0.9355 | dt: 95.80ms | tok/sec: 85511.98\n","start training the model\n","step 0 of training\n","step  2881 | loss: 4.752881 | lr 5.8144e-04 | norm: 0.8517 | dt: 96.28ms | tok/sec: 85084.87\n","start training the model\n","step 0 of training\n","step  2882 | loss: 4.875167 | lr 5.8142e-04 | norm: 0.8500 | dt: 96.70ms | tok/sec: 84718.37\n","start training the model\n","step 0 of training\n","step  2883 | loss: 4.738162 | lr 5.8141e-04 | norm: 1.0471 | dt: 96.75ms | tok/sec: 84670.98\n","start training the model\n","step 0 of training\n","step  2884 | loss: 4.769143 | lr 5.8139e-04 | norm: 0.9159 | dt: 96.18ms | tok/sec: 85177.04\n","start training the model\n","step 0 of training\n","step  2885 | loss: 4.792293 | lr 5.8137e-04 | norm: 0.9027 | dt: 96.05ms | tok/sec: 85287.83\n","start training the model\n","step 0 of training\n","step  2886 | loss: 4.648819 | lr 5.8136e-04 | norm: 0.8240 | dt: 96.66ms | tok/sec: 84754.73\n","start training the model\n","step 0 of training\n","step  2887 | loss: 4.958858 | lr 5.8134e-04 | norm: 1.2012 | dt: 95.99ms | tok/sec: 85341.00\n","start training the model\n","step 0 of training\n","step  2888 | loss: 4.691513 | lr 5.8132e-04 | norm: 1.0765 | dt: 96.44ms | tok/sec: 84944.57\n","start training the model\n","step 0 of training\n","step  2889 | loss: 4.661088 | lr 5.8131e-04 | norm: 0.9222 | dt: 95.95ms | tok/sec: 85381.29\n","start training the model\n","step 0 of training\n","step  2890 | loss: 4.817427 | lr 5.8129e-04 | norm: 1.0513 | dt: 96.06ms | tok/sec: 85283.39\n","start training the model\n","step 0 of training\n","step  2891 | loss: 4.882793 | lr 5.8127e-04 | norm: 0.9621 | dt: 95.68ms | tok/sec: 85619.16\n","start training the model\n","step 0 of training\n","step  2892 | loss: 4.731452 | lr 5.8126e-04 | norm: 1.0072 | dt: 96.91ms | tok/sec: 84535.58\n","start training the model\n","step 0 of training\n","step  2893 | loss: 4.674108 | lr 5.8124e-04 | norm: 0.7653 | dt: 97.88ms | tok/sec: 83693.19\n","start training the model\n","step 0 of training\n","step  2894 | loss: 4.635417 | lr 5.8122e-04 | norm: 0.8524 | dt: 97.63ms | tok/sec: 83910.05\n","start training the model\n","step 0 of training\n","step  2895 | loss: 4.707177 | lr 5.8121e-04 | norm: 0.8570 | dt: 97.41ms | tok/sec: 84100.85\n","start training the model\n","step 0 of training\n","step  2896 | loss: 4.855966 | lr 5.8119e-04 | norm: 1.0033 | dt: 96.65ms | tok/sec: 84760.38\n","start training the model\n","step 0 of training\n","step  2897 | loss: 4.863405 | lr 5.8117e-04 | norm: 0.9718 | dt: 101.08ms | tok/sec: 81043.81\n","start training the model\n","step 0 of training\n","step  2898 | loss: 5.008883 | lr 5.8115e-04 | norm: 1.4148 | dt: 96.35ms | tok/sec: 85020.24\n","start training the model\n","step 0 of training\n","step  2899 | loss: 5.070229 | lr 5.8114e-04 | norm: 1.3377 | dt: 96.43ms | tok/sec: 84948.77\n","start training the model\n","step 0 of training\n","step  2900 | loss: 5.044090 | lr 5.8112e-04 | norm: 1.1395 | dt: 96.24ms | tok/sec: 85120.92\n","start training the model\n","step 0 of training\n","step  2901 | loss: 4.862908 | lr 5.8110e-04 | norm: 1.1647 | dt: 96.25ms | tok/sec: 85114.17\n","start training the model\n","step 0 of training\n","step  2902 | loss: 4.802747 | lr 5.8109e-04 | norm: 1.0562 | dt: 96.98ms | tok/sec: 84471.77\n","start training the model\n","step 0 of training\n","step  2903 | loss: 4.693651 | lr 5.8107e-04 | norm: 1.2239 | dt: 95.98ms | tok/sec: 85349.48\n","start training the model\n","step 0 of training\n","step  2904 | loss: 4.874744 | lr 5.8105e-04 | norm: 0.9657 | dt: 96.58ms | tok/sec: 84818.75\n","start training the model\n","step 0 of training\n","step  2905 | loss: 4.704679 | lr 5.8104e-04 | norm: 1.0138 | dt: 95.79ms | tok/sec: 85517.94\n","start training the model\n","step 0 of training\n","step  2906 | loss: 4.798242 | lr 5.8102e-04 | norm: 1.1916 | dt: 96.03ms | tok/sec: 85304.56\n","start training the model\n","step 0 of training\n","step  2907 | loss: 4.708277 | lr 5.8100e-04 | norm: 0.9182 | dt: 96.41ms | tok/sec: 84968.52\n","start training the model\n","step 0 of training\n","step  2908 | loss: 4.599034 | lr 5.8098e-04 | norm: 0.9852 | dt: 96.19ms | tok/sec: 85161.84\n","start training the model\n","step 0 of training\n","step  2909 | loss: 4.582637 | lr 5.8097e-04 | norm: 1.0780 | dt: 97.79ms | tok/sec: 83770.73\n","start training the model\n","step 0 of training\n","step  2910 | loss: 4.619426 | lr 5.8095e-04 | norm: 1.1379 | dt: 96.23ms | tok/sec: 85133.57\n","start training the model\n","step 0 of training\n","step  2911 | loss: 4.687654 | lr 5.8093e-04 | norm: 0.9945 | dt: 96.04ms | tok/sec: 85299.48\n","start training the model\n","step 0 of training\n","step  2912 | loss: 4.671349 | lr 5.8092e-04 | norm: 1.0833 | dt: 96.73ms | tok/sec: 84691.23\n","start training the model\n","step 0 of training\n","step  2913 | loss: 4.871322 | lr 5.8090e-04 | norm: 1.2839 | dt: 96.26ms | tok/sec: 85106.16\n","start training the model\n","step 0 of training\n","step  2914 | loss: 4.775033 | lr 5.8088e-04 | norm: 1.0712 | dt: 96.99ms | tok/sec: 84462.43\n","start training the model\n","step 0 of training\n","step  2915 | loss: 4.881179 | lr 5.8087e-04 | norm: 1.1577 | dt: 96.00ms | tok/sec: 85337.19\n","start training the model\n","step 0 of training\n","step  2916 | loss: 4.687378 | lr 5.8085e-04 | norm: 1.0038 | dt: 95.83ms | tok/sec: 85485.38\n","start training the model\n","step 0 of training\n","step  2917 | loss: 4.835226 | lr 5.8083e-04 | norm: 1.1037 | dt: 96.73ms | tok/sec: 84687.26\n","start training the model\n","step 0 of training\n","step  2918 | loss: 4.919298 | lr 5.8081e-04 | norm: 1.1634 | dt: 95.81ms | tok/sec: 85503.89\n","start training the model\n","step 0 of training\n","step  2919 | loss: 4.857834 | lr 5.8080e-04 | norm: 1.5743 | dt: 96.43ms | tok/sec: 84953.81\n","start training the model\n","step 0 of training\n","step  2920 | loss: 4.729168 | lr 5.8078e-04 | norm: 1.2165 | dt: 95.87ms | tok/sec: 85445.84\n","start training the model\n","step 0 of training\n","step  2921 | loss: 4.654259 | lr 5.8076e-04 | norm: 0.9663 | dt: 96.24ms | tok/sec: 85124.71\n","start training the model\n","step 0 of training\n","step  2922 | loss: 4.787153 | lr 5.8075e-04 | norm: 1.0533 | dt: 96.18ms | tok/sec: 85173.46\n","start training the model\n","step 0 of training\n","step  2923 | loss: 4.737831 | lr 5.8073e-04 | norm: 1.1715 | dt: 96.19ms | tok/sec: 85163.74\n","start training the model\n","step 0 of training\n","step  2924 | loss: 4.705913 | lr 5.8071e-04 | norm: 1.0536 | dt: 96.15ms | tok/sec: 85200.07\n","start training the model\n","step 0 of training\n","step  2925 | loss: 4.710269 | lr 5.8069e-04 | norm: 1.1702 | dt: 96.46ms | tok/sec: 84926.72\n","start training the model\n","step 0 of training\n","step  2926 | loss: 4.590005 | lr 5.8068e-04 | norm: 1.4111 | dt: 96.02ms | tok/sec: 85319.39\n","start training the model\n","step 0 of training\n","step  2927 | loss: 4.488178 | lr 5.8066e-04 | norm: 1.2804 | dt: 96.74ms | tok/sec: 84681.00\n","start training the model\n","step 0 of training\n","step  2928 | loss: 4.532963 | lr 5.8064e-04 | norm: 0.9727 | dt: 95.55ms | tok/sec: 85731.31\n","start training the model\n","step 0 of training\n","step  2929 | loss: 4.472898 | lr 5.8063e-04 | norm: 1.1848 | dt: 95.96ms | tok/sec: 85366.23\n","start training the model\n","step 0 of training\n","step  2930 | loss: 5.513643 | lr 5.8061e-04 | norm: 1.5959 | dt: 96.91ms | tok/sec: 84532.25\n","start training the model\n","step 0 of training\n","step  2931 | loss: 5.521729 | lr 5.8059e-04 | norm: 1.6157 | dt: 97.59ms | tok/sec: 83945.31\n","start training the model\n","step 0 of training\n","step  2932 | loss: 5.263350 | lr 5.8057e-04 | norm: 1.1911 | dt: 96.76ms | tok/sec: 84665.35\n","start training the model\n","step 0 of training\n","step  2933 | loss: 5.205852 | lr 5.8056e-04 | norm: 1.1266 | dt: 96.87ms | tok/sec: 84565.54\n","start training the model\n","step 0 of training\n","step  2934 | loss: 5.157949 | lr 5.8054e-04 | norm: 1.1680 | dt: 97.18ms | tok/sec: 84300.79\n","start training the model\n","step 0 of training\n","step  2935 | loss: 5.231155 | lr 5.8052e-04 | norm: 1.3520 | dt: 96.65ms | tok/sec: 84757.24\n","start training the model\n","step 0 of training\n","step  2936 | loss: 5.055105 | lr 5.8051e-04 | norm: 1.2138 | dt: 96.62ms | tok/sec: 84786.73\n","start training the model\n","step 0 of training\n","step  2937 | loss: 5.051801 | lr 5.8049e-04 | norm: 1.1654 | dt: 96.67ms | tok/sec: 84744.49\n","start training the model\n","step 0 of training\n","step  2938 | loss: 5.104329 | lr 5.8047e-04 | norm: 0.9539 | dt: 96.80ms | tok/sec: 84625.94\n","start training the model\n","step 0 of training\n","step  2939 | loss: 5.169529 | lr 5.8045e-04 | norm: 1.3077 | dt: 97.17ms | tok/sec: 84302.45\n","start training the model\n","step 0 of training\n","step  2940 | loss: 5.112507 | lr 5.8044e-04 | norm: 1.2195 | dt: 96.46ms | tok/sec: 84926.09\n","start training the model\n","step 0 of training\n","step  2941 | loss: 5.143734 | lr 5.8042e-04 | norm: 1.1765 | dt: 96.46ms | tok/sec: 84924.62\n","start training the model\n","step 0 of training\n","step  2942 | loss: 5.069313 | lr 5.8040e-04 | norm: 0.9610 | dt: 96.24ms | tok/sec: 85116.49\n","start training the model\n","step 0 of training\n","step  2943 | loss: 5.071180 | lr 5.8038e-04 | norm: 1.2869 | dt: 96.39ms | tok/sec: 84991.43\n","start training the model\n","step 0 of training\n","step  2944 | loss: 4.923491 | lr 5.8037e-04 | norm: 1.1746 | dt: 95.59ms | tok/sec: 85700.52\n","start training the model\n","step 0 of training\n","step  2945 | loss: 5.030349 | lr 5.8035e-04 | norm: 1.0727 | dt: 96.58ms | tok/sec: 84825.04\n","start training the model\n","step 0 of training\n","step  2946 | loss: 5.016413 | lr 5.8033e-04 | norm: 1.1096 | dt: 97.21ms | tok/sec: 84271.23\n","start training the model\n","step 0 of training\n","step  2947 | loss: 4.959589 | lr 5.8032e-04 | norm: 1.1070 | dt: 95.86ms | tok/sec: 85453.92\n","start training the model\n","step 0 of training\n","step  2948 | loss: 5.227125 | lr 5.8030e-04 | norm: 1.7264 | dt: 96.67ms | tok/sec: 84738.01\n","start training the model\n","step 0 of training\n","step  2949 | loss: 5.111971 | lr 5.8028e-04 | norm: 2.2886 | dt: 96.91ms | tok/sec: 84534.95\n","start training the model\n","step 0 of training\n","step  2950 | loss: 4.577056 | lr 5.8026e-04 | norm: 2.9388 | dt: 97.79ms | tok/sec: 83773.39\n","start training the model\n","step 0 of training\n","step  2951 | loss: 4.857577 | lr 5.8025e-04 | norm: 1.6177 | dt: 97.30ms | tok/sec: 84195.44\n","start training the model\n","step 0 of training\n","step  2952 | loss: 4.048477 | lr 5.8023e-04 | norm: 1.1998 | dt: 98.09ms | tok/sec: 83515.60\n","start training the model\n","step 0 of training\n","step  2953 | loss: 4.100901 | lr 5.8021e-04 | norm: 2.4735 | dt: 98.12ms | tok/sec: 83493.68\n","start training the model\n","step 0 of training\n","step  2954 | loss: 3.998401 | lr 5.8019e-04 | norm: 2.3279 | dt: 97.55ms | tok/sec: 83973.21\n","start training the model\n","step 0 of training\n","step  2955 | loss: 4.428561 | lr 5.8018e-04 | norm: 2.5706 | dt: 97.00ms | tok/sec: 84449.97\n","start training the model\n","step 0 of training\n","step  2956 | loss: 4.036077 | lr 5.8016e-04 | norm: 1.5447 | dt: 98.12ms | tok/sec: 83487.60\n","start training the model\n","step 0 of training\n","step  2957 | loss: 3.308196 | lr 5.8014e-04 | norm: 1.1966 | dt: 96.92ms | tok/sec: 84520.19\n","start training the model\n","step 0 of training\n","step  2958 | loss: 1.943884 | lr 5.8012e-04 | norm: 1.1754 | dt: 97.58ms | tok/sec: 83948.18\n","start training the model\n","step 0 of training\n","step  2959 | loss: 1.863235 | lr 5.8011e-04 | norm: 1.4132 | dt: 97.11ms | tok/sec: 84359.99\n","start training the model\n","step 0 of training\n","step  2960 | loss: 1.760882 | lr 5.8009e-04 | norm: 1.3572 | dt: 97.13ms | tok/sec: 84336.59\n","start training the model\n","step 0 of training\n","step  2961 | loss: 1.859743 | lr 5.8007e-04 | norm: 1.0790 | dt: 96.14ms | tok/sec: 85212.32\n","start training the model\n","step 0 of training\n","step  2962 | loss: 1.850350 | lr 5.8005e-04 | norm: 0.9453 | dt: 96.98ms | tok/sec: 84472.81\n","start training the model\n","step 0 of training\n","step  2963 | loss: 1.720298 | lr 5.8004e-04 | norm: 1.2328 | dt: 96.84ms | tok/sec: 84590.10\n","start training the model\n","step 0 of training\n","step  2964 | loss: 1.908036 | lr 5.8002e-04 | norm: 1.0436 | dt: 96.25ms | tok/sec: 85113.96\n","start training the model\n","step 0 of training\n","step  2965 | loss: 1.747548 | lr 5.8000e-04 | norm: 0.7870 | dt: 96.99ms | tok/sec: 84460.98\n","start training the model\n","step 0 of training\n","step  2966 | loss: 1.530576 | lr 5.7999e-04 | norm: 1.1898 | dt: 96.00ms | tok/sec: 85335.70\n","start training the model\n","step 0 of training\n","step  2967 | loss: 1.504627 | lr 5.7997e-04 | norm: 0.9336 | dt: 97.69ms | tok/sec: 83853.73\n","start training the model\n","step 0 of training\n","step  2968 | loss: 1.775750 | lr 5.7995e-04 | norm: 1.2031 | dt: 97.93ms | tok/sec: 83654.68\n","start training the model\n","step 0 of training\n","step  2969 | loss: 1.850896 | lr 5.7993e-04 | norm: 1.0412 | dt: 98.10ms | tok/sec: 83507.69\n","start training the model\n","step 0 of training\n","step  2970 | loss: 1.956460 | lr 5.7992e-04 | norm: 1.2215 | dt: 97.49ms | tok/sec: 84030.92\n","start training the model\n","step 0 of training\n","step  2971 | loss: 1.758911 | lr 5.7990e-04 | norm: 1.0952 | dt: 97.63ms | tok/sec: 83908.61\n","start training the model\n","step 0 of training\n","step  2972 | loss: 1.766578 | lr 5.7988e-04 | norm: 1.1609 | dt: 97.51ms | tok/sec: 84012.02\n","start training the model\n","step 0 of training\n","step  2973 | loss: 1.763687 | lr 5.7986e-04 | norm: 1.2348 | dt: 96.03ms | tok/sec: 85306.89\n","start training the model\n","step 0 of training\n","step  2974 | loss: 1.739875 | lr 5.7985e-04 | norm: 1.0040 | dt: 96.94ms | tok/sec: 84505.84\n","start training the model\n","step 0 of training\n","step  2975 | loss: 1.683101 | lr 5.7983e-04 | norm: 1.6947 | dt: 97.16ms | tok/sec: 84315.07\n","start training the model\n","step 0 of training\n","step  2976 | loss: 1.664342 | lr 5.7981e-04 | norm: 1.2582 | dt: 97.19ms | tok/sec: 84291.49\n","start training the model\n","step 0 of training\n","step  2977 | loss: 1.666929 | lr 5.7979e-04 | norm: 1.5381 | dt: 96.42ms | tok/sec: 84963.89\n","start training the model\n","step 0 of training\n","step  2978 | loss: 1.635399 | lr 5.7978e-04 | norm: 1.5073 | dt: 96.48ms | tok/sec: 84911.61\n","start training the model\n","step 0 of training\n","step  2979 | loss: 1.631353 | lr 5.7976e-04 | norm: 1.0009 | dt: 96.73ms | tok/sec: 84686.01\n","start training the model\n","step 0 of training\n","step  2980 | loss: 1.571770 | lr 5.7974e-04 | norm: 1.4045 | dt: 96.65ms | tok/sec: 84759.75\n","start training the model\n","step 0 of training\n","step  2981 | loss: 1.545531 | lr 5.7972e-04 | norm: 1.2058 | dt: 96.56ms | tok/sec: 84837.18\n","start training the model\n","step 0 of training\n","step  2982 | loss: 1.667468 | lr 5.7971e-04 | norm: 1.1170 | dt: 97.05ms | tok/sec: 84407.03\n","start training the model\n","step 0 of training\n","step  2983 | loss: 1.572863 | lr 5.7969e-04 | norm: 1.4554 | dt: 96.53ms | tok/sec: 84863.38\n","start training the model\n","step 0 of training\n","step  2984 | loss: 1.625513 | lr 5.7967e-04 | norm: 1.1127 | dt: 97.91ms | tok/sec: 83668.53\n","start training the model\n","step 0 of training\n","step  2985 | loss: 1.572861 | lr 5.7965e-04 | norm: 0.9170 | dt: 96.56ms | tok/sec: 84835.09\n","start training the model\n","step 0 of training\n","step  2986 | loss: 1.607267 | lr 5.7963e-04 | norm: 1.2447 | dt: 97.45ms | tok/sec: 84065.05\n","start training the model\n","step 0 of training\n","step  2987 | loss: 1.518335 | lr 5.7962e-04 | norm: 1.2468 | dt: 95.92ms | tok/sec: 85406.76\n","start training the model\n","step 0 of training\n","step  2988 | loss: 1.539752 | lr 5.7960e-04 | norm: 1.3589 | dt: 96.14ms | tok/sec: 85205.77\n","start training the model\n","step 0 of training\n","step  2989 | loss: 1.548922 | lr 5.7958e-04 | norm: 1.2991 | dt: 95.91ms | tok/sec: 85415.25\n","start training the model\n","step 0 of training\n","step  2990 | loss: 1.524872 | lr 5.7956e-04 | norm: 1.5491 | dt: 96.27ms | tok/sec: 85090.56\n","start training the model\n","step 0 of training\n","step  2991 | loss: 1.494660 | lr 5.7955e-04 | norm: 1.2676 | dt: 96.76ms | tok/sec: 84660.13\n","start training the model\n","step 0 of training\n","step  2992 | loss: 1.488079 | lr 5.7953e-04 | norm: 1.8352 | dt: 97.18ms | tok/sec: 84293.55\n","start training the model\n","step 0 of training\n","step  2993 | loss: 1.486498 | lr 5.7951e-04 | norm: 1.2123 | dt: 97.46ms | tok/sec: 84053.32\n","start training the model\n","step 0 of training\n","step  2994 | loss: 2.200987 | lr 5.7949e-04 | norm: 0.8947 | dt: 96.50ms | tok/sec: 84888.12\n","start training the model\n","step 0 of training\n","step  2995 | loss: 5.237350 | lr 5.7948e-04 | norm: 1.5665 | dt: 96.66ms | tok/sec: 84752.43\n","start training the model\n","step 0 of training\n","step  2996 | loss: 4.751284 | lr 5.7946e-04 | norm: 1.1727 | dt: 96.54ms | tok/sec: 84853.53\n","start training the model\n","step 0 of training\n","step  2997 | loss: 4.656448 | lr 5.7944e-04 | norm: 0.9435 | dt: 96.93ms | tok/sec: 84517.28\n","start training the model\n","step 0 of training\n","step  2998 | loss: 4.575578 | lr 5.7942e-04 | norm: 0.9704 | dt: 96.68ms | tok/sec: 84731.74\n","start training the model\n","step 0 of training\n","step  2999 | loss: 4.514325 | lr 5.7941e-04 | norm: 1.1728 | dt: 96.47ms | tok/sec: 84919.59\n","validation loss: 6.0563\n","rank 0 sample 0: [CLS] دردم از یار است و درمان نیز هم[SEP] و <[SOT]>[BOB][BOM] صد و به حکم کن و هم مایه که به دست\n","rank 0 sample 1: [CLS] دردم از یار است و درمان نیز هم[SEP] در مذهب گو مباش ز کس از کس را به که هست در عالم بند بند و ما را به سوی ما\n","rank 0 sample 2: [CLS] دردم از یار است و درمان نیز هم[SEP] است و ترک سوگند[BOB][BOM] یاد باد در آنجا خوب است است که در این چه غم و در میکده در\n","rank 0 sample 3: [CLS] دردم از یار است و درمان نیز هم[SEP] هم هیچ]>[BOB][BOM] در ره آن دو عالم در سفر شد که از دیده گریان روان ما ز چین\n","start training the model\n","step 0 of training\n","step  3000 | loss: 4.580903 | lr 5.7939e-04 | norm: 1.2308 | dt: 1612.49ms | tok/sec: 5080.35\n","start training the model\n","step 0 of training\n","step  3001 | loss: 4.593338 | lr 5.7937e-04 | norm: 1.0267 | dt: 96.70ms | tok/sec: 84712.94\n","start training the model\n","step 0 of training\n","step  3002 | loss: 4.500191 | lr 5.7935e-04 | norm: 1.0050 | dt: 97.30ms | tok/sec: 84192.35\n","start training the model\n","step 0 of training\n","step  3003 | loss: 4.647904 | lr 5.7933e-04 | norm: 1.0854 | dt: 97.24ms | tok/sec: 84246.43\n","start training the model\n","step 0 of training\n","step  3004 | loss: 4.653903 | lr 5.7932e-04 | norm: 1.0389 | dt: 97.25ms | tok/sec: 84235.07\n","start training the model\n","step 0 of training\n","step  3005 | loss: 4.654634 | lr 5.7930e-04 | norm: 0.7934 | dt: 97.30ms | tok/sec: 84193.58\n","start training the model\n","step 0 of training\n","step  3006 | loss: 4.698632 | lr 5.7928e-04 | norm: 1.0999 | dt: 96.68ms | tok/sec: 84733.00\n","start training the model\n","step 0 of training\n","step  3007 | loss: 4.671917 | lr 5.7926e-04 | norm: 1.2284 | dt: 97.35ms | tok/sec: 84149.46\n","start training the model\n","step 0 of training\n","step  3008 | loss: 4.803433 | lr 5.7925e-04 | norm: 1.0103 | dt: 97.96ms | tok/sec: 83623.33\n","start training the model\n","step 0 of training\n","step  3009 | loss: 4.690977 | lr 5.7923e-04 | norm: 1.2884 | dt: 97.70ms | tok/sec: 83846.16\n","start training the model\n","step 0 of training\n","step  3010 | loss: 4.696035 | lr 5.7921e-04 | norm: 0.9773 | dt: 96.10ms | tok/sec: 85242.76\n","start training the model\n","step 0 of training\n","step  3011 | loss: 4.736448 | lr 5.7919e-04 | norm: 1.2842 | dt: 96.78ms | tok/sec: 84648.66\n","start training the model\n","step 0 of training\n","step  3012 | loss: 4.579463 | lr 5.7917e-04 | norm: 1.2542 | dt: 96.65ms | tok/sec: 84763.51\n","start training the model\n","step 0 of training\n","step  3013 | loss: 4.917100 | lr 5.7916e-04 | norm: 1.4192 | dt: 96.69ms | tok/sec: 84726.73\n","start training the model\n","step 0 of training\n","step  3014 | loss: 4.602584 | lr 5.7914e-04 | norm: 1.0230 | dt: 96.94ms | tok/sec: 84505.64\n","start training the model\n","step 0 of training\n","step  3015 | loss: 4.586960 | lr 5.7912e-04 | norm: 1.0362 | dt: 97.03ms | tok/sec: 84429.22\n","start training the model\n","step 0 of training\n","step  3016 | loss: 4.741879 | lr 5.7910e-04 | norm: 1.1188 | dt: 97.65ms | tok/sec: 83892.22\n","start training the model\n","step 0 of training\n","step  3017 | loss: 4.799675 | lr 5.7909e-04 | norm: 1.0935 | dt: 96.25ms | tok/sec: 85111.64\n","start training the model\n","step 0 of training\n","step  3018 | loss: 4.657504 | lr 5.7907e-04 | norm: 1.0293 | dt: 95.56ms | tok/sec: 85729.17\n","start training the model\n","step 0 of training\n","step  3019 | loss: 4.623738 | lr 5.7905e-04 | norm: 1.2208 | dt: 96.74ms | tok/sec: 84679.54\n","start training the model\n","step 0 of training\n","step  3020 | loss: 4.581184 | lr 5.7903e-04 | norm: 1.1689 | dt: 96.82ms | tok/sec: 84613.43\n","start training the model\n","step 0 of training\n","step  3021 | loss: 4.635874 | lr 5.7901e-04 | norm: 0.8914 | dt: 96.32ms | tok/sec: 85049.91\n","start training the model\n","step 0 of training\n","step  3022 | loss: 4.779207 | lr 5.7900e-04 | norm: 1.3020 | dt: 96.78ms | tok/sec: 84642.61\n","start training the model\n","step 0 of training\n","step  3023 | loss: 4.788110 | lr 5.7898e-04 | norm: 1.2082 | dt: 102.45ms | tok/sec: 79961.04\n","start training the model\n","step 0 of training\n","step  3024 | loss: 5.007268 | lr 5.7896e-04 | norm: 1.9060 | dt: 97.19ms | tok/sec: 84285.90\n","start training the model\n","step 0 of training\n","step  3025 | loss: 5.045271 | lr 5.7894e-04 | norm: 1.7261 | dt: 97.33ms | tok/sec: 84164.51\n","start training the model\n","step 0 of training\n","step  3026 | loss: 4.956506 | lr 5.7893e-04 | norm: 1.3442 | dt: 97.84ms | tok/sec: 83731.74\n","start training the model\n","step 0 of training\n","step  3027 | loss: 4.840182 | lr 5.7891e-04 | norm: 2.1899 | dt: 96.98ms | tok/sec: 84474.27\n","start training the model\n","step 0 of training\n","step  3028 | loss: 4.734455 | lr 5.7889e-04 | norm: 1.2578 | dt: 97.62ms | tok/sec: 83916.61\n","start training the model\n","step 0 of training\n","step  3029 | loss: 4.624984 | lr 5.7887e-04 | norm: 1.1383 | dt: 96.77ms | tok/sec: 84654.71\n","start training the model\n","step 0 of training\n","step  3030 | loss: 4.775817 | lr 5.7885e-04 | norm: 1.0764 | dt: 96.37ms | tok/sec: 85002.36\n","start training the model\n","step 0 of training\n","step  3031 | loss: 4.616459 | lr 5.7884e-04 | norm: 1.2320 | dt: 96.37ms | tok/sec: 85006.14\n","start training the model\n","step 0 of training\n","step  3032 | loss: 4.750688 | lr 5.7882e-04 | norm: 1.1508 | dt: 96.67ms | tok/sec: 84744.28\n","start training the model\n","step 0 of training\n","step  3033 | loss: 4.643407 | lr 5.7880e-04 | norm: 0.9890 | dt: 97.39ms | tok/sec: 84116.50\n","start training the model\n","step 0 of training\n","step  3034 | loss: 4.543620 | lr 5.7878e-04 | norm: 1.0835 | dt: 96.02ms | tok/sec: 85312.18\n","start training the model\n","step 0 of training\n","step  3035 | loss: 4.519837 | lr 5.7876e-04 | norm: 0.9304 | dt: 95.82ms | tok/sec: 85495.59\n","start training the model\n","step 0 of training\n","step  3036 | loss: 4.550126 | lr 5.7875e-04 | norm: 1.1657 | dt: 96.83ms | tok/sec: 84601.56\n","start training the model\n","step 0 of training\n","step  3037 | loss: 4.620085 | lr 5.7873e-04 | norm: 1.0965 | dt: 95.99ms | tok/sec: 85346.09\n","start training the model\n","step 0 of training\n","step  3038 | loss: 4.603445 | lr 5.7871e-04 | norm: 1.1259 | dt: 96.75ms | tok/sec: 84668.06\n","start training the model\n","step 0 of training\n","step  3039 | loss: 4.836926 | lr 5.7869e-04 | norm: 1.6457 | dt: 95.96ms | tok/sec: 85365.81\n","start training the model\n","step 0 of training\n","step  3040 | loss: 4.715600 | lr 5.7867e-04 | norm: 1.1999 | dt: 96.82ms | tok/sec: 84614.06\n","start training the model\n","step 0 of training\n","step  3041 | loss: 4.808869 | lr 5.7866e-04 | norm: 1.3191 | dt: 95.95ms | tok/sec: 85374.51\n","start training the model\n","step 0 of training\n","step  3042 | loss: 4.635503 | lr 5.7864e-04 | norm: 1.3897 | dt: 96.03ms | tok/sec: 85309.22\n","start training the model\n","step 0 of training\n","step  3043 | loss: 4.730568 | lr 5.7862e-04 | norm: 1.1611 | dt: 96.89ms | tok/sec: 84548.68\n","start training the model\n","step 0 of training\n","step  3044 | loss: 4.824607 | lr 5.7860e-04 | norm: 1.0752 | dt: 96.28ms | tok/sec: 85081.08\n","start training the model\n","step 0 of training\n","step  3045 | loss: 4.838566 | lr 5.7858e-04 | norm: 1.7651 | dt: 97.00ms | tok/sec: 84452.67\n","start training the model\n","step 0 of training\n","step  3046 | loss: 4.704996 | lr 5.7857e-04 | norm: 1.3894 | dt: 96.12ms | tok/sec: 85227.96\n","start training the model\n","step 0 of training\n","step  3047 | loss: 4.576231 | lr 5.7855e-04 | norm: 1.0804 | dt: 96.32ms | tok/sec: 85053.49\n","start training the model\n","step 0 of training\n","step  3048 | loss: 4.739149 | lr 5.7853e-04 | norm: 1.4359 | dt: 96.33ms | tok/sec: 85037.70\n","start training the model\n","step 0 of training\n","step  3049 | loss: 4.656558 | lr 5.7851e-04 | norm: 1.4400 | dt: 95.82ms | tok/sec: 85491.34\n","start training the model\n","step 0 of training\n","step  3050 | loss: 4.642905 | lr 5.7849e-04 | norm: 1.5413 | dt: 96.56ms | tok/sec: 84841.58\n","start training the model\n","step 0 of training\n","step  3051 | loss: 4.645031 | lr 5.7848e-04 | norm: 1.4412 | dt: 95.75ms | tok/sec: 85560.10\n","start training the model\n","step 0 of training\n","step  3052 | loss: 4.576945 | lr 5.7846e-04 | norm: 1.4554 | dt: 95.66ms | tok/sec: 85636.23\n","start training the model\n","step 0 of training\n","step  3053 | loss: 4.475690 | lr 5.7844e-04 | norm: 1.7633 | dt: 96.42ms | tok/sec: 84958.01\n","start training the model\n","step 0 of training\n","step  3054 | loss: 4.466419 | lr 5.7842e-04 | norm: 1.5403 | dt: 96.28ms | tok/sec: 85082.13\n","start training the model\n","step 0 of training\n","step  3055 | loss: 4.430830 | lr 5.7840e-04 | norm: 1.3830 | dt: 95.50ms | tok/sec: 85780.54\n","start training the model\n","step 0 of training\n","step  3056 | loss: 5.422036 | lr 5.7839e-04 | norm: 1.4237 | dt: 97.05ms | tok/sec: 84409.31\n","start training the model\n","step 0 of training\n","step  3057 | loss: 5.443289 | lr 5.7837e-04 | norm: 1.3169 | dt: 97.76ms | tok/sec: 83794.23\n","start training the model\n","step 0 of training\n","step  3058 | loss: 5.149828 | lr 5.7835e-04 | norm: 1.2707 | dt: 97.84ms | tok/sec: 83726.44\n","start training the model\n","step 0 of training\n","step  3059 | loss: 5.120987 | lr 5.7833e-04 | norm: 1.6076 | dt: 98.49ms | tok/sec: 83176.56\n","start training the model\n","step 0 of training\n","step  3060 | loss: 5.080956 | lr 5.7831e-04 | norm: 1.2298 | dt: 98.00ms | tok/sec: 83594.23\n","start training the model\n","step 0 of training\n","step  3061 | loss: 5.203172 | lr 5.7830e-04 | norm: 1.5327 | dt: 97.55ms | tok/sec: 83978.55\n","start training the model\n","step 0 of training\n","step  3062 | loss: 4.972194 | lr 5.7828e-04 | norm: 1.1925 | dt: 97.01ms | tok/sec: 84446.65\n","start training the model\n","step 0 of training\n","step  3063 | loss: 5.017905 | lr 5.7826e-04 | norm: 1.2498 | dt: 97.72ms | tok/sec: 83827.14\n","start training the model\n","step 0 of training\n","step  3064 | loss: 5.011362 | lr 5.7824e-04 | norm: 1.3348 | dt: 96.56ms | tok/sec: 84834.67\n","start training the model\n","step 0 of training\n","step  3065 | loss: 5.107696 | lr 5.7822e-04 | norm: 1.2934 | dt: 96.75ms | tok/sec: 84675.78\n","start training the model\n","step 0 of training\n","step  3066 | loss: 5.061455 | lr 5.7820e-04 | norm: 1.4331 | dt: 96.84ms | tok/sec: 84596.35\n","start training the model\n","step 0 of training\n","step  3067 | loss: 5.091070 | lr 5.7819e-04 | norm: 1.4889 | dt: 96.14ms | tok/sec: 85208.31\n","start training the model\n","step 0 of training\n","step  3068 | loss: 4.981478 | lr 5.7817e-04 | norm: 1.5119 | dt: 97.20ms | tok/sec: 84278.05\n","start training the model\n","step 0 of training\n","step  3069 | loss: 5.057941 | lr 5.7815e-04 | norm: 1.5878 | dt: 96.96ms | tok/sec: 84491.30\n","start training the model\n","step 0 of training\n","step  3070 | loss: 4.840155 | lr 5.7813e-04 | norm: 1.1581 | dt: 96.57ms | tok/sec: 84827.76\n","start training the model\n","step 0 of training\n","step  3071 | loss: 4.925814 | lr 5.7811e-04 | norm: 1.0541 | dt: 96.37ms | tok/sec: 85008.04\n","start training the model\n","step 0 of training\n","step  3072 | loss: 4.935601 | lr 5.7810e-04 | norm: 1.5174 | dt: 96.67ms | tok/sec: 84739.89\n","start training the model\n","step 0 of training\n","step  3073 | loss: 4.838327 | lr 5.7808e-04 | norm: 1.2958 | dt: 96.95ms | tok/sec: 84497.74\n","start training the model\n","step 0 of training\n","step  3074 | loss: 5.093101 | lr 5.7806e-04 | norm: 1.4737 | dt: 95.70ms | tok/sec: 85604.23\n","start training the model\n","step 0 of training\n","step  3075 | loss: 5.057846 | lr 5.7804e-04 | norm: 2.1954 | dt: 97.05ms | tok/sec: 84410.97\n","start training the model\n","step 0 of training\n","step  3076 | loss: 4.479213 | lr 5.7802e-04 | norm: 2.3090 | dt: 97.42ms | tok/sec: 84087.47\n","start training the model\n","step 0 of training\n","step  3077 | loss: 4.691608 | lr 5.7800e-04 | norm: 1.4793 | dt: 96.89ms | tok/sec: 84546.39\n","start training the model\n","step 0 of training\n","step  3078 | loss: 3.932550 | lr 5.7799e-04 | norm: 1.5417 | dt: 97.83ms | tok/sec: 83738.68\n","start training the model\n","step 0 of training\n","step  3079 | loss: 4.043650 | lr 5.7797e-04 | norm: 1.6608 | dt: 97.36ms | tok/sec: 84137.09\n","start training the model\n","step 0 of training\n","step  3080 | loss: 3.883169 | lr 5.7795e-04 | norm: 1.3301 | dt: 97.99ms | tok/sec: 83598.91\n","start training the model\n","step 0 of training\n","step  3081 | loss: 4.342308 | lr 5.7793e-04 | norm: 1.4643 | dt: 97.47ms | tok/sec: 84050.65\n","start training the model\n","step 0 of training\n","step  3082 | loss: 3.961569 | lr 5.7791e-04 | norm: 1.1367 | dt: 98.72ms | tok/sec: 82982.11\n","start training the model\n","step 0 of training\n","step  3083 | loss: 3.205896 | lr 5.7789e-04 | norm: 1.1022 | dt: 97.81ms | tok/sec: 83757.46\n","start training the model\n","step 0 of training\n","step  3084 | loss: 1.898007 | lr 5.7788e-04 | norm: 1.4329 | dt: 97.33ms | tok/sec: 84167.39\n","start training the model\n","step 0 of training\n","step  3085 | loss: 1.815319 | lr 5.7786e-04 | norm: 1.2020 | dt: 97.04ms | tok/sec: 84418.02\n","start training the model\n","step 0 of training\n","step  3086 | loss: 1.725356 | lr 5.7784e-04 | norm: 1.1080 | dt: 98.09ms | tok/sec: 83517.02\n","start training the model\n","step 0 of training\n","step  3087 | loss: 1.801268 | lr 5.7782e-04 | norm: 1.2123 | dt: 96.75ms | tok/sec: 84669.10\n","start training the model\n","step 0 of training\n","step  3088 | loss: 1.796277 | lr 5.7780e-04 | norm: 1.2644 | dt: 96.47ms | tok/sec: 84918.96\n","start training the model\n","step 0 of training\n","step  3089 | loss: 1.654750 | lr 5.7778e-04 | norm: 0.8580 | dt: 96.37ms | tok/sec: 85006.56\n","start training the model\n","step 0 of training\n","step  3090 | loss: 1.854254 | lr 5.7777e-04 | norm: 1.1226 | dt: 97.06ms | tok/sec: 84404.54\n","start training the model\n","step 0 of training\n","step  3091 | loss: 1.714479 | lr 5.7775e-04 | norm: 0.8529 | dt: 96.87ms | tok/sec: 84569.70\n","start training the model\n","step 0 of training\n","step  3092 | loss: 1.496123 | lr 5.7773e-04 | norm: 1.2330 | dt: 96.56ms | tok/sec: 84836.77\n","start training the model\n","step 0 of training\n","step  3093 | loss: 1.456539 | lr 5.7771e-04 | norm: 1.0528 | dt: 95.57ms | tok/sec: 85719.12\n","start training the model\n","step 0 of training\n","step  3094 | loss: 1.721157 | lr 5.7769e-04 | norm: 1.9163 | dt: 97.33ms | tok/sec: 84171.31\n","start training the model\n","step 0 of training\n","step  3095 | loss: 1.812750 | lr 5.7767e-04 | norm: 1.0550 | dt: 98.19ms | tok/sec: 83428.20\n","start training the model\n","step 0 of training\n","step  3096 | loss: 1.920788 | lr 5.7766e-04 | norm: 1.7097 | dt: 97.48ms | tok/sec: 84040.58\n","start training the model\n","step 0 of training\n","step  3097 | loss: 1.734092 | lr 5.7764e-04 | norm: 1.3897 | dt: 97.45ms | tok/sec: 84063.40\n","start training the model\n","step 0 of training\n","step  3098 | loss: 1.749942 | lr 5.7762e-04 | norm: 1.7630 | dt: 97.52ms | tok/sec: 84004.01\n","start training the model\n","step 0 of training\n","step  3099 | loss: 1.747412 | lr 5.7760e-04 | norm: 1.6881 | dt: 97.85ms | tok/sec: 83718.07\n","start training the model\n","step 0 of training\n","step  3100 | loss: 1.724695 | lr 5.7758e-04 | norm: 1.2767 | dt: 97.57ms | tok/sec: 83962.95\n","start training the model\n","step 0 of training\n","step  3101 | loss: 1.702124 | lr 5.7756e-04 | norm: 1.2908 | dt: 97.13ms | tok/sec: 84337.62\n","start training the model\n","step 0 of training\n","step  3102 | loss: 1.661911 | lr 5.7755e-04 | norm: 1.3038 | dt: 97.22ms | tok/sec: 84263.17\n","start training the model\n","step 0 of training\n","step  3103 | loss: 1.654686 | lr 5.7753e-04 | norm: 1.0386 | dt: 97.82ms | tok/sec: 83745.21\n","start training the model\n","step 0 of training\n","step  3104 | loss: 1.621206 | lr 5.7751e-04 | norm: 1.1792 | dt: 96.37ms | tok/sec: 85007.41\n","start training the model\n","step 0 of training\n","step  3105 | loss: 1.590939 | lr 5.7749e-04 | norm: 1.0694 | dt: 96.06ms | tok/sec: 85280.63\n","start training the model\n","step 0 of training\n","step  3106 | loss: 1.558523 | lr 5.7747e-04 | norm: 1.2772 | dt: 96.52ms | tok/sec: 84874.07\n","start training the model\n","step 0 of training\n","step  3107 | loss: 1.507100 | lr 5.7745e-04 | norm: 1.0982 | dt: 96.18ms | tok/sec: 85172.40\n","start training the model\n","step 0 of training\n","step  3108 | loss: 1.624377 | lr 5.7743e-04 | norm: 1.2821 | dt: 97.38ms | tok/sec: 84123.50\n","start training the model\n","step 0 of training\n","step  3109 | loss: 1.531837 | lr 5.7742e-04 | norm: 1.3100 | dt: 95.90ms | tok/sec: 85418.44\n","start training the model\n","step 0 of training\n","step  3110 | loss: 1.566161 | lr 5.7740e-04 | norm: 1.0292 | dt: 96.81ms | tok/sec: 84621.56\n","start training the model\n","step 0 of training\n","step  3111 | loss: 1.527075 | lr 5.7738e-04 | norm: 1.4855 | dt: 95.87ms | tok/sec: 85450.09\n","start training the model\n","step 0 of training\n","step  3112 | loss: 1.560682 | lr 5.7736e-04 | norm: 1.3426 | dt: 95.60ms | tok/sec: 85687.06\n","start training the model\n","step 0 of training\n","step  3113 | loss: 1.474239 | lr 5.7734e-04 | norm: 1.0754 | dt: 97.24ms | tok/sec: 84249.32\n","start training the model\n","step 0 of training\n","step  3114 | loss: 1.533281 | lr 5.7732e-04 | norm: 1.6732 | dt: 96.70ms | tok/sec: 84719.63\n","start training the model\n","step 0 of training\n","step  3115 | loss: 1.539850 | lr 5.7731e-04 | norm: 1.5907 | dt: 96.60ms | tok/sec: 84807.03\n","start training the model\n","step 0 of training\n","step  3116 | loss: 1.520383 | lr 5.7729e-04 | norm: 1.7867 | dt: 96.82ms | tok/sec: 84611.14\n","start training the model\n","step 0 of training\n","step  3117 | loss: 1.463702 | lr 5.7727e-04 | norm: 1.6120 | dt: 96.62ms | tok/sec: 84786.31\n","start training the model\n","step 0 of training\n","step  3118 | loss: 1.494250 | lr 5.7725e-04 | norm: 1.7858 | dt: 97.50ms | tok/sec: 84018.38\n","start training the model\n","step 0 of training\n","step  3119 | loss: 1.453724 | lr 5.7723e-04 | norm: 1.1245 | dt: 96.52ms | tok/sec: 84873.44\n","start training the model\n","step 0 of training\n","step  3120 | loss: 2.171306 | lr 5.7721e-04 | norm: 1.7986 | dt: 97.48ms | tok/sec: 84038.93\n","start training the model\n","step 0 of training\n","step  3121 | loss: 5.222758 | lr 5.7719e-04 | norm: 1.9769 | dt: 96.51ms | tok/sec: 84884.55\n","start training the model\n","step 0 of training\n","step  3122 | loss: 4.676759 | lr 5.7718e-04 | norm: 1.5817 | dt: 96.69ms | tok/sec: 84727.77\n","start training the model\n","step 0 of training\n","step  3123 | loss: 4.576314 | lr 5.7716e-04 | norm: 1.2563 | dt: 96.47ms | tok/sec: 84916.65\n","start training the model\n","step 0 of training\n","step  3124 | loss: 4.492746 | lr 5.7714e-04 | norm: 1.1740 | dt: 96.49ms | tok/sec: 84901.12\n","start training the model\n","step 0 of training\n","step  3125 | loss: 4.449378 | lr 5.7712e-04 | norm: 0.9688 | dt: 96.39ms | tok/sec: 84988.69\n","start training the model\n","step 0 of training\n","step  3126 | loss: 4.502320 | lr 5.7710e-04 | norm: 1.1208 | dt: 96.37ms | tok/sec: 85008.25\n","start training the model\n","step 0 of training\n","step  3127 | loss: 4.505621 | lr 5.7708e-04 | norm: 1.1494 | dt: 96.80ms | tok/sec: 84625.10\n","start training the model\n","step 0 of training\n","step  3128 | loss: 4.408842 | lr 5.7706e-04 | norm: 1.2058 | dt: 96.78ms | tok/sec: 84645.33\n","start training the model\n","step 0 of training\n","step  3129 | loss: 4.551287 | lr 5.7705e-04 | norm: 1.1494 | dt: 96.34ms | tok/sec: 85036.02\n","start training the model\n","step 0 of training\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-91f64adaeb53>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# wait for the GPU to finish work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;31m# time difference in seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_synchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","torch.set_float32_matmul_precision('high') # this  is for gpu tunning\n","\n","# create model\n","model = GPT(GPTConfig(vocab_size=50304))\n","# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n","model.to(device)\n","use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n","if use_compile:\n","    model = torch.compile(model)\n","if ddp:\n","    model = DDP(model, device_ids=[ddp_local_rank])\n","raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n","\n","max_lr = 6e-4\n","min_lr = max_lr * 0.1\n","warmup_steps = 700\n","max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n","def get_lr(it):\n","    # 1) linear warmup for warmup_iters steps\n","    if it < warmup_steps:\n","        return max_lr * (it+1) / warmup_steps\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > max_steps:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n","    return min_lr + coeff * (max_lr - min_lr)\n","\n","# optimize!\n","optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n","\n","# create the log directory we will write checkpoints to and log to\n","log_dir = data_root + \"/log\"\n","os.makedirs(log_dir, exist_ok=True)\n","log_file = os.path.join(log_dir, f\"log.txt\")\n","with open(log_file, \"w\") as f: # open for writing to clear the file\n","    pass\n","output_file = data_root + \"/output.txt\"\n","for step in range(max_steps):\n","    t0 = time.time()\n","    last_step = (step == max_steps - 1)\n","\n","    # once in a while evaluate our validation loss\n","    if step % 250 == 0 or last_step:\n","        model.eval() # we want to use model. we dont want to train it.\n","        val_loader.reset()\n","        with torch.no_grad():\n","            val_loss_accum = 0.0\n","            val_loss_steps = 20\n","            for _ in range(val_loss_steps):\n","                x, y = val_loader.next_batch()\n","                x, y = x.to(device), y.to(device)\n","                with torch.autocast(device_type=device_type, dtype=torch.bfloat16): # do something with ram to make it efficient and sp\n","                    logits, loss = model(x, y)\n","                loss = loss / val_loss_steps\n","                val_loss_accum += loss.detach()\n","        if ddp:\n","            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n","        if master_process:\n","            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n","            with open(log_file, \"a\") as f:\n","                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n","            if step > 0 and (step % 5000 == 0 or last_step):\n","                # optionally write model checkpoints\n","                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'config': raw_model.config,\n","                    'step': step,\n","                    'val_loss': val_loss_accum.item()\n","                }\n","                # you might also want to add optimizer.state_dict() and\n","                # rng seeds etc., if you wanted to more exactly resume training\n","                torch.save(checkpoint, checkpoint_path)\n","\n","    # once in a while evaluate hellaswag\n","    # if (step % 250 == 0 or last_step) and (not use_compile):\n","    #     num_correct_norm = 0\n","    #     num_total = 0 # ToDo i commented the following for loop since it relates to Hellasweg\n","    #     # for i, example in enumerate(iterate_examples(\"val\")):\n","    #     #     # only process examples where i % ddp_world_size == ddp_rank\n","    #     #     if i % ddp_world_size != ddp_rank:\n","    #     #         continue\n","    #     #     # render the example into tokens and labels\n","    #     #     _, tokens, mask, label = render_example(example)\n","    #     #     tokens = tokens.to(device)\n","    #     #     mask = mask.to(device)\n","    #     #     # get the logits\n","    #     #     with torch.no_grad():\n","    #     #         with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n","    #     #             logits, loss = model(tokens)\n","    #     #         pred_norm = get_most_likely_row(tokens, mask, logits)\n","    #     #     num_total += 1\n","    #     #     num_correct_norm += int(pred_norm == label)\n","    #     # reduce the stats across all processes\n","    #     if ddp:\n","    #         num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n","    #         num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n","    #         dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n","    #         dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n","    #         num_total = num_total.item()\n","    #         print('num_total', num_total)\n","    #         num_correct_norm = num_correct_norm.item()\n","    #         print('num_correct_norm', num_correct_norm)\n","    #     print(\"ddp not available\")\n","    #     acc_norm = num_correct_norm / num_total #if num_total != 0 else 0\n","    #     if master_process:\n","    #         print(f\"Dataset accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n","    #         with open(log_file, \"a\") as f:\n","    #             f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n","\n","    # once in a while generate from the model (except step 0, which is noise)\n","    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n","        phrase = \"دردم از یار است و درمان نیز هم\"\n","        generate_poem(model, device, device_type, ddp_rank, phrase, 4)\n","    # do one step of the optimization\n","    print(\"start training the model\")\n","    model.train()\n","    optimizer.zero_grad()\n","    loss_accum = 0.0\n","    for micro_step in range(grad_accum_steps):\n","        print(f\"step {micro_step} of training\")\n","        x, y = train_loader.next_batch()\n","        x, y = x.to(device), y.to(device)\n","        # added after video, this field is also used by the forward pass.\n","        if ddp:\n","            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n","        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n","            logits, loss = model(x, y)\n","        # we have to scale the loss to account for gradient accumulation,\n","        # because the gradients just add on each successive backward().\n","        # addition of gradients corresponds to a SUM in the objective, but\n","        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n","        loss = loss / grad_accum_steps\n","        loss_accum += loss.detach()\n","        loss.backward()\n","    if ddp:\n","        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n","    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    # determine and set the learning rate for this iteration\n","    lr = get_lr(step)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    optimizer.step()\n","    if device_type == \"cuda\":\n","        torch.cuda.synchronize() # wait for the GPU to finish work\n","    t1 = time.time()\n","    dt = t1 - t0 # time difference in seconds\n","    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n","    tokens_per_sec = tokens_processed / dt\n","    if master_process:\n","        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n","        with open(log_file, \"a\") as f:\n","            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n","\n","if ddp:\n","    destroy_process_group()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"QZUvwnNg7wZN","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1723580195847,"user_tz":-210,"elapsed":12,"user":{"displayName":"GenAI","userId":"05659125202882086658"}},"outputId":"0c549692-a372-43d8-d57c-551bd4531560"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-01cbf5be8ad2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading the Model and give your poems :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/saved_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#replacewith model path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"توانا بود هرکه دانا بود\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerated_poems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddp_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["#Loading the Model and give your poems :)\n","model.load_state_dict(torch.load(data_root + '/saved_model.pth'))  #replacewith model path\n","phrase = \"توانا بود هرکه دانا بود\"\n","generated_poems=generate_poem(model, device, device_type, ddp_rank, phrase, 1)\n","\n","result_file_path = data_root + '/result.txt'\n","with open(result_file_path, 'w', encoding='utf-8') as f:\n","    for poem in generated_poems:\n","        f.write(poem + '\\n')\n","\n","print(f\"Generated poems saved to {result_file_path}\")\n","\n","\n","# Flatten the special tokens into a list\n","tokens_to_remove = [special_tokens[\"cls_token\"], special_tokens[\"eos_token\"]] + special_tokens[\"additional_special_tokens\"]\n","#TODO\n","\n","# Function to remove special tokens from a line\n","def remove_special_tokens(line):\n","    for token in tokens_to_remove:\n","        if token == 'BOM':\n","            line = line.replace(token, \"\\n\")\n","            continue\n","        line = line.replace(token, \"\")\n","    return line\n","\n","# Open the input file for reading and the output file for writing\n","input_file_path = data_root + \"/result.txt\"  # Change this to your input file path\n","output_file_path = data_root + '/' + 'final_output.txt'  # Change this to your desired output file path\n","\n","with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n","    for line in infile:\n","        # Remove special tokens from the current line\n","        cleaned_line = remove_special_tokens(line)\n","        # Write the cleaned line to the output file\n","        print(cleaned_line)\n","        outfile.write(cleaned_line)\n","\n","print(\"Special tokens removed and output saved to\", output_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFpQzTpVw201"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}