{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax4CMTRg3EBB",
        "outputId": "5b6fb4a2-b5ad-415b-c794-a383cfefbd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/soleimani101/Persian-poems.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"/kaggle/working/train.txt\"\n",
        "\n",
        "with open(file_path, \"w\") as file:\n",
        "    pass\n",
        "\n",
        "print(f\"File {file_path} created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokeniz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary to map filenames to Persian equivalents\n",
        "\n",
        "name_mapping = {\n",
        "    \"abbadi\": \"عبادی\",\n",
        "    \"abdullah\": \"عبداللهی\",\n",
        "    \"abolhasan\": \"ابوالحسن\",\n",
        "    \"abusaeed\": \"ابوسعید\",\n",
        "    \"adib\": \"ادیب\",\n",
        "    \"afsar\": \"افسر\",\n",
        "    \"ahli\": \"اهلی\",\n",
        "    \"akhsikati\": \"اخسیکتی\",\n",
        "    \"amagh\": \"اماغ\",\n",
        "    \"amir\": \"امیر\",\n",
        "    \"anvari\": \"انوری\",\n",
        "    \"aref\": \"عارف\",\n",
        "    \"aroozi\": \"عروضی\",\n",
        "    \"asad\": \"اسد\",\n",
        "    \"asadi\": \"اسدی\",\n",
        "    \"ashofte\": \"آشفتی\",\n",
        "    \"asiri\": \"عصیری\",\n",
        "    \"asjadi\": \"اسجدی\",\n",
        "    \"asrar\": \"اسرار\",\n",
        "    \"attar\": \"عطار\",\n",
        "    \"ayyooghi\": \"ایویی\",\n",
        "    \"azar\": \"آذر\",\n",
        "    \"azraghi\": \"ازراغی\",\n",
        "    \"babaafzal\": \"باباافضل\",\n",
        "    \"babataher\": \"باباطاهر\",\n",
        "    \"baha\": \"بها\",\n",
        "    \"bahaee\": \"بهایی\",\n",
        "    \"bahar\": \"بهار\",\n",
        "    \"bahoo\": \"بهاو\",\n",
        "    \"beyhaghi\": \"بیهقی\",\n",
        "    \"beylaghani\": \"بیلاغانی\",\n",
        "    \"bidel\": \"بیدل\",\n",
        "    \"bolandeghbal\": \"بلندقبال\",\n",
        "    \"daghighi\": \"دقیق\",\n",
        "    \"daye\": \"دایه\",\n",
        "    \"ebneemad\": \"ابنعماد\",\n",
        "    \"ebnehesam\": \"ابنحسام\",\n",
        "    \"ebneyamin\": \"ابن‌یامین\",\n",
        "    \"elhami\": \"الهامی\",\n",
        "    \"emami\": \"امامی\",\n",
        "    \"eraghi\": \"اراغی\",\n",
        "    \"eshghi\": \"عشقی\",\n",
        "    \"eynolghozat\": \"عین‌القصات\",\n",
        "    \"faghani\": \"فغان\",\n",
        "    \"falaki\": \"فلاکی\",\n",
        "    \"fani\": \"فانی\",\n",
        "    \"farrokhi\": \"فرروخی\",\n",
        "    \"fasihi\": \"فصیحی\",\n",
        "    \"fayez\": \"فایز\",\n",
        "    \"fayyaz\": \"فایز\",\n",
        "    \"ferdousi\": \"فردوسی\",\n",
        "    \"feyz\": \"فیض\",\n",
        "    \"forooghi\": \"فروغی\",\n",
        "    \"fozooli\": \"فضولی\",\n",
        "    \"ghaani\": \"غانی\",\n",
        "    \"ghaemmagham\": \"قائم‌مقام\",\n",
        "    \"ghaleb\": \"غالب\",\n",
        "    \"ghari\": \"قری\",\n",
        "    \"ghasem\": \"قاسم\",\n",
        "    \"ghassab\": \"قصاب\",\n",
        "    \"ghatran\": \"قطران\",\n",
        "    \"ghavami\": \"قوامی\",\n",
        "    \"ghazzali\": \"غزالی\",\n",
        "    \"ghobar\": \"غبار\",\n",
        "    \"ghodsi\": \"قدسی\",\n",
        "    \"ghomri\": \"قمری\",\n",
        "    \"gilani\": \"گیلانی\",\n",
        "    \"habib\": \"حبیب\",\n",
        "    \"hafez\": \"حافظ\",\n",
        "    \"hajeb\": \"حاجب\",\n",
        "    \"hamedani\": \"همدانی\",\n",
        "    \"hamgar\": \"همگار\",\n",
        "    \"hamid\": \"حمید\",\n",
        "    \"hasan\": \"حسن\",\n",
        "    \"hatef\": \"حاطف\",\n",
        "    \"hazin\": \"حزین\",\n",
        "    \"helali\": \"هلالی\",\n",
        "    \"heydar\": \"حیدر\",\n",
        "    \"hojviri\": \"حجوری\",\n",
        "    \"homam\": \"هما\",\n",
        "    \"hoseyn\": \"حسین\",\n",
        "    \"hoseyni\": \"حسینی\",\n",
        "    \"iqbal\": \"اقبال\",\n",
        "    \"iraj\": \"ایراج\",\n",
        "    \"iranshan\": \"ایران‌شاه\",\n",
        "    \"jabali\": \"جبالی\",\n",
        "    \"jahan\": \"جهان\",\n",
        "    \"jalal\": \"جلال\",\n",
        "    \"jamal\": \"جمال\",\n",
        "    \"jami\": \"جامی\",\n",
        "    \"jarooni\": \"جورونی\",\n",
        "    \"jeyhoon\": \"جیحون\",\n",
        "    \"jooya\": \"جویای\",\n",
        "    \"kalim\": \"کلیم\",\n",
        "    \"kamal\": \"کمال\",\n",
        "    \"keikavus\": \"کیقباد\",\n",
        "    \"kermani\": \"کرمانی\",\n",
        "    \"kesayee\": \"کسایی\",\n",
        "    \"khaghani\": \"خاقانی\",\n",
        "    \"khajenasir\": \"خاج‌نسیر\",\n",
        "    \"khajoo\": \"خاجو\",\n",
        "    \"khaled\": \"خالد\",\n",
        "    \"khalili\": \"خلیلی\",\n",
        "    \"khayyam\": \"خیام\",\n",
        "    \"khiali\": \"خیالی\",\n",
        "    \"khojandi\": \"خوجندی\",\n",
        "    \"khosro\": \"خسرو\",\n",
        "    \"koohi\": \"کوهی\",\n",
        "    \"koosaj\": \"کوساج\",\n",
        "    \"labibi\": \"لابیبی\",\n",
        "    \"maghrebi\": \"مغربی\",\n",
        "    \"mahabadi\": \"محبوبی\",\n",
        "    \"mahsati\": \"محسنی\",\n",
        "    \"manoochehri\": \"منوچهری\",\n",
        "    \"masih\": \"مسحی\",\n",
        "    \"masood\": \"مسعود\",\n",
        "    \"meybodi\": \"میبودی\",\n",
        "    \"meyli\": \"میلی\",\n",
        "    \"mir\": \"میر\",\n",
        "    \"mmon\": \"من\",\n",
        "    \"moftagher\": \"مفتخر\",\n",
        "    \"mohit\": \"محیط\",\n",
        "    \"mohtasham\": \"مختشم\",\n",
        "    \"mokhtari\": \"مختاری\",\n",
        "    \"monshi\": \"منشی\",\n",
        "    \"moshtagh\": \"مشطی\",\n",
        "    \"moulavi\": \"مولوی\",\n",
        "    \"naraghi\": \"نارقی\",\n",
        "    \"nasafi\": \"نسفی\",\n",
        "    \"naserkhosro\": \"ناصرخسرو\",\n",
        "    \"nashenas\": \"ناشناس\",\n",
        "    \"nasimi\": \"نسیمی\",\n",
        "    \"nayyer\": \"نایر\",\n",
        "    \"naziri\": \"ناظری\",\n",
        "    \"neshat\": \"نشاط\",\n",
        "    \"nezami\": \"نظامی\",\n",
        "    \"nezari\": \"نظری\",\n",
        "    \"noorali\": \"نورعلی\",\n",
        "    \"nouee\": \"نویی\",\n",
        "    \"obeyd\": \"عبید\",\n",
        "    \"omman\": \"عمان\",\n",
        "    \"onsori\": \"انساری\",\n",
        "    \"orfi\": \"عرفی\",\n",
        "    \"osmani\": \"اسماعی\",\n",
        "    \"ouhad\": \"احد\",\n",
        "    \"ouhadi\": \"احدی\",\n",
        "    \"parvin\": \"پروین\",\n",
        "    \"pazevari\": \"پازواری\",\n",
        "    \"rafigh\": \"رافیق\",\n",
        "    \"rahi\": \"راهی\",\n",
        "    \"rashhe\": \"راشی\",\n",
        "    \"razi\": \"رازی\",\n",
        "    \"rhedayat\": \"رهایی\",\n",
        "    \"roodaki\": \"رودکی\",\n",
        "    \"rooni\": \"رونی\",\n",
        "    \"saadi\": \"سعدی\",\n",
        "    \"saber\": \"صابر\",\n",
        "    \"saeb\": \"صائب\",\n",
        "    \"saeeda\": \"سعیده\",\n",
        "    \"safa\": \"صفا\",\n",
        "    \"safayee\": \"صفایی\",\n",
        "    \"safi\": \"صافی\",\n",
        "    \"saghir\": \"صغیر\",\n",
        "    \"sahab\": \"سحاب\",\n",
        "    \"salim\": \"سالم\",\n",
        "    \"salman\": \"سلمان\",\n",
        "    \"samet\": \"صامت\",\n",
        "    \"sanaee\": \"سناعی\",\n",
        "    \"seyf\": \"سیف\",\n",
        "    \"shabestari\": \"شبستری\",\n",
        "    \"shahedi\": \"شهیدی\",\n",
        "    \"shahi\": \"شاهی\",\n",
        "    \"shahnematollah\": \"شاه‌نعمت‌الله\",\n",
        "    \"shahrestani\": \"شهرستانی\",\n",
        "    \"shahriar\": \"شهریار\",\n",
        "    \"shater\": \"شاطر\",\n",
        "    \"shooshtari\": \"شوشتری\",\n",
        "    \"soofi\": \"صوفی\",\n",
        "    \"soozani\": \"سوزانی\",\n",
        "    \"tabib\": \"طبیب\",\n",
        "    \"toghra\": \"تغرا\",\n",
        "    \"toghrol\": \"تغریول\",\n",
        "    \"torki\": \"ترکی\",\n",
        "    \"vaez\": \"واعظ\",\n",
        "    \"vahdat\": \"وحدت\",\n",
        "    \"vahid\": \"وحید\",\n",
        "    \"vahshi\": \"وحشی\",\n",
        "    \"valad\": \"ولاد\",\n",
        "    \"varavini\": \"وراوینی\",\n",
        "    \"vatvat\": \"واتوات\",\n",
        "    \"yaghma\": \"یغما\",\n",
        "    \"yazdi\": \"یزدی\",\n",
        "    \"zahir\": \"ظاهر\",\n",
        "    \"zahiri\": \"ظاهری\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# Directory containing the .txt files\n",
        "directory = \"/kaggle/working/Persian-poems/database/\"\n",
        "\n",
        "# Output file\n",
        "output_file = \"/kaggle/working/train.txt\"\n",
        "\n",
        "i = 1 \n",
        "# Open the output file in write mode\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    # Iterate over all the files in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            # Extract the Persian name from the filename (assuming the filename is in Persian)\n",
        "\n",
        "            base_name = filename.replace(\".txt\", \"\")\n",
        "            persian_name = name_mapping.get(base_name, base_name) \n",
        "            poem = \"\"\n",
        "            # Open the current .txt file and read its content line by line\n",
        "            print(\"Start : \"+persian_name)\n",
        "            with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as infile:\n",
        "                for line in infile:\n",
        "                    # Remove special characters enclosed within <[]>\n",
        "                    cleaned_line = re.sub(r'<\\[[^\\]]*\\]>', '', line)\n",
        "                    cleaned_line = cleaned_line.replace(\"\\n\", \"<sep>\")\n",
        "\n",
        "                    # Skip writing multiple consecutive <sep> lines\n",
        "                    if cleaned_line == \"<sep>\":\n",
        "                        continue\n",
        "                    poem += cleaned_line\n",
        "\n",
        "            # Write the poem to the output file\n",
        "            print(\"Finish : \"+persian_name)\n",
        "            outfile.write(persian_name + \"<|startoftext|>\" + poem)\n",
        "            if i == 1:\n",
        "                print(persian_name + \"<|startoftext|>\" + poem)\n",
        "            i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelWithLMHead\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name_or_path = \"HooshvareLab/gpt2-fa\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token='<s>', \n",
        "    eos_token='</s>', \n",
        "    pad_token='<pad>',\n",
        "    unk_token='<unk>'\n",
        ")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"bos_token\": '</s>',\n",
        "    \"eos_token\": '</s>', \n",
        "    \"pad_token\": '<pad>',\n",
        "    \"unk_token\": '<unk>'\n",
        "})\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0], \n",
        "    eos_token_id=tokenizer(\"</s>\")[\"input_ids\"][0], \n",
        "    pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n",
        "    unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0],\n",
        ")\n",
        "\n",
        "tokenizer.save_pretrained(\"/kaggle/working/gpt/\")\n",
        "config.save_pretrained(\"/kaggle/working/gpt/\")\n",
        "\n",
        "!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/pytorch_model.bin\" -P /content/gpt2/\n",
        "!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/tokenizer.json\" -P /content/gpt2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/kaggle/working/gpt\",\n",
        "    bos_token='<s>', \n",
        "    eos_token='</s>', \n",
        "    pad_token='<pad>'\n",
        ")\n",
        "\n",
        "print(tokenizer.encode(\"اَلا یا اَیُّهَا السّاقی اَدِرْ کَأسَاً و ناوِلْها  که عشق آسان نمود اوّل ولی افتاد مشکل‌ها\"))\n",
        "print(tokenizer.encode(\"<s>\"))\n",
        "print(tokenizer.encode(\"</s>\"))\n",
        "print(tokenizer.encode(\"<pad>\"))\n",
        "print(tokenizer.encode(\"<|startoftext|>\"))\n",
        "print(tokenizer.encode(\"<sep>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_file, 'r') as file:\n",
        "    texts = file.readlines() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset  # this is the pytorch class import\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "class MTGDataset(Dataset):\n",
        "\n",
        "    def __init__(self, txt_list, tokenizer, max_length=1024):\n",
        "\n",
        "        self.tokenizer = tokenizer  # the gpt2 tokenizer we instantiated\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for txt in txt_list:\n",
        "            \"\"\"\n",
        "            This loop will iterate through each entry in the flavour text corpus.\n",
        "            For each bit of text it will prepend it with the start of text token,\n",
        "            then append the end of text token and pad to the maximum length with the \n",
        "            pad token. \n",
        "            \"\"\"\n",
        "\n",
        "            encodings_dict = tokenizer('<s>' + txt + '</s>',\n",
        "                                       truncation=True,\n",
        "                                       max_length=max_length,\n",
        "                                       padding=\"max_length\")\n",
        "\n",
        "            \"\"\"\n",
        "            Each iteration then appends either the encoded tensor to a list,\n",
        "            or the attention mask for that encoding to a list. The attention mask is\n",
        "            a binary list of 1's or 0's which determine whether the langauge model\n",
        "            should take that token into consideration or not. \n",
        "            \"\"\"\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "dataset = MTGDataset(texts, tokenizer, max_length=max_seq)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "f'There are {len(train_dataset)} samples for training, and {len(val_dataset)} samples for validation testing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train_dataset[0][0][:100])\n",
        "print(tokenizer.decode(train_dataset[0][0][:100]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=RandomSampler(train_dataset),\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    sampler=SequentialSampler(val_dataset),\n",
        "    batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMpfbSKubaJa"
      },
      "source": [
        "![Attention is all you need. High quality image :)](Capture.JPG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3UKd5mq33VFb"
      },
      "outputs": [],
      "source": [
        "# Defining Transformer model. This cell implements what the above image explains. source: <Attention is all you need>\n",
        "class CausalSelfAttention(nn.Module): # Implements self-attention mechanism\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1 # standard deviation grows inside the residual stream. This line controls it.\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) # self.ln_1(x) noramlization --> attention think about all tokens together. aggregated function\n",
        "        x = x + self.mlp(self.ln_2(x)) # self.ln_2(x) --> multi linear perc(?) # think individually about tokens\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int =  25005 # tokenizer.vocab_size number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 6 # number of layers\n",
        "    n_head: int = 6 # number of heads\n",
        "    n_embd: int = 384 # embedding dimension\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd), # token encoding the first box of the picture\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd), # position encoding\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # total block in the picture of attention is all you need\n",
        "            ln_f = nn.LayerNorm(config.n_embd), # linear part of picture\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):# if a layer is linear use normal distribution std is different\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None: # if we have target in data we calculate loss as follows\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # flatten data by targets.view(-1)\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type): # loading wieghts. this is a constructor or class method\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        if master_process:\n",
        "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        # use_fused = fused_available and device_type == \"mps\"\n",
        "        if master_process:\n",
        "            print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FBF-J9ke3cVc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_tokens(filename):\n",
        "    npt = np.load(filename)\n",
        "    npt = npt.astype(np.int32) # added after video\n",
        "    ptt = torch.tensor(npt, dtype=torch.long)\n",
        "    return ptt\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, process_rank, num_processes, split):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "        data_root = \"/content/drive/MyDrive/poems with nano gpt/input/\"\n",
        "        # get the shard filenames\n",
        "        shards = os.listdir(data_root)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_root,  s) for s in shards]\n",
        "        print(shards)\n",
        "        self.shards = shards\n",
        "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
        "\n",
        "        if master_process:\n",
        "            print(f\"found {len(shards)} shards for split {split}\")\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # state, init at shard zero\n",
        "        self.current_shard = 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next batch would be out of bounds, advance to next shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "            self.current_position = B * T * self.process_rank\n",
        "        return x, y\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# helper function for HellaSwag eval\n",
        "# takes tokens, mask, and logits, returns the index of the completion with the lowest loss\n",
        "\n",
        "def get_most_likely_row(tokens, mask, logits):\n",
        "    # evaluate the autoregressive loss at all positions\n",
        "    shift_logits = (logits[..., :-1, :]).contiguous()\n",
        "    shift_tokens = (tokens[..., 1:]).contiguous()\n",
        "    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
        "    flat_shift_tokens = shift_tokens.view(-1)\n",
        "    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
        "    shift_losses = shift_losses.view(tokens.size(0), -1)\n",
        "    # now get the average loss just for the completion region (where mask == 1), in each row\n",
        "    shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n",
        "    masked_shift_losses = shift_losses * shift_mask\n",
        "    # sum and divide by the number of 1s in the mask\n",
        "    sum_loss = masked_shift_losses.sum(dim=1)\n",
        "    avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
        "    # now we have a loss for each of the 4 completions\n",
        "    # the one with the lowest loss should be the most likely\n",
        "    pred_norm = avg_loss.argmin().item()\n",
        "    return pred_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hziv-34h2oyU"
      },
      "outputs": [],
      "source": [
        "# This cell generates poem and saves the trained model. Generated poems will be saved into a file.\n",
        "def generate_poem(model, device, device_type, ddp_rank, phrase, num_return_sequences):\n",
        "    result = 'result.txt'\n",
        "    model.eval()\n",
        "    max_length = 32\n",
        "    tokens = enc.encode(phrase)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "    xgen = tokens.to(device)\n",
        "    sample_rng = torch.Generator(device=device)\n",
        "    sample_rng.manual_seed(42 + ddp_rank)\n",
        "\n",
        "    generated_poems = []  # List to store generated poems\n",
        "\n",
        "    while xgen.size(1) < max_length:\n",
        "        # forward the model to get the logits\n",
        "        with torch.no_grad():\n",
        "            with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "            # take the logits at the last position\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "            # get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # do top-k sampling of 50 (huggingface pipeline default)\n",
        "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # return top k high probability\n",
        "            # select a token from the top-k probabilities\n",
        "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "            # gather the corresponding indices\n",
        "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "            # append to the sequence\n",
        "            xgen = torch.cat((xgen, xcol), dim=1)\n",
        "\n",
        "    # collect and save the generated text\n",
        "    for i in range(num_return_sequences):\n",
        "        tokens = xgen[i, :max_length].tolist()\n",
        "        decoded = enc.decode(tokens)\n",
        "        print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
        "        generated_poems.append(decoded)\n",
        "\n",
        "    # Save the model state (optional)\n",
        "    torch.save(model.state_dict(), data_root + '/saved_model.pth')\n",
        "\n",
        "    # Save the generated poems to a file\n",
        "    output_file = data_root + \"/\" + result\n",
        "    with open(output_file, \"a\") as out_f:\n",
        "        for i, poem in enumerate(generated_poems):\n",
        "            out_f.write(f\"sample {i}: {poem}\\n\")\n",
        "\n",
        "    return generated_poems  # Return the list of generated poems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "Mt0h7QuU3kKy",
        "outputId": "51d8bfea-a2d4-48b8-9dfb-c4a030d2622f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False DDP\n",
            "using device: cuda\n",
            "total desired batch size: 8192\n",
            "=> calculated gradient accumulation steps: 1\n",
            "['/content/drive/MyDrive/poems with nano gpt/input/train.npy']\n",
            "found 1 shards for split train\n",
            "['/content/drive/MyDrive/poems with nano gpt/input/val.npy']\n",
            "found 1 shards for split val\n",
            "data is ready....\n",
            "num decayed parameter tensors: 18, with 1,984,740 parameters\n",
            "num non-decayed parameter tensors: 34, with 2,052 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Cannot forward sequence of length 1024, block size is only 102",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2c9d53f97e0b>\u001b[0m in \u001b[0;36m<cell line: 139>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# do something with ram to make it efficient and sp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mval_loss_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mval_loss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-52888127c25a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# idx is of shape (B, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;31m# forward the token and posisition embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape (T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Cannot forward sequence of length 1024, block size is only 102"
          ]
        }
      ],
      "source": [
        "# Loading data, split data for train and validation, Training model\n",
        "\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "\n",
        "# set up DDP (distributed data parallel).\n",
        "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "print(ddp, \"DDP\")\n",
        "if ddp:\n",
        "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "else:\n",
        "    # vanilla, non-DDP run\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    # attempt to autodetect device\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_batch_size = 8192 #524288 # 2**19, ~0.5M, in number of tokens\n",
        "B = 8 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "    print(f\"total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# all_files = [os.path.join(data_root, f) for f in os.listdir(data_root) if f.endswith('.txt')]\n",
        "# random.shuffle(all_files)  # Shuffle to ensure random split\n",
        "\n",
        "# split_index = int(0.9 * len(all_files))  # 90% for training, 10% for validation\n",
        "# train_files = all_files[:split_index]\n",
        "# val_files = all_files[split_index:]\n",
        "\n",
        "# train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=train_files)\n",
        "# val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=val_files)\n",
        "# new method\n",
        "\n",
        "\n",
        "aggregate_data_files_and_add_special_tokens()\n",
        "\n",
        "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
        "val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
        "\n",
        "\n",
        "# # Shuffle lines to ensure randomness\n",
        "# random.shuffle(all_lines)\n",
        "\n",
        "# # Split the lines into training and validation sets\n",
        "# split_index = int(0.9 * len(all_lines))  # 90% for training, 10% for validation\n",
        "# train_lines = all_lines[:split_index]\n",
        "# val_lines = all_lines[split_index:]\n",
        "# with open(os.path.join(data_root, 'train.txt'), 'w', encoding='utf-8') as train_file:\n",
        "#     train_file.writelines(train_lines)\n",
        "\n",
        "# with open(os.path.join(data_root, 'val.txt'), 'w', encoding='utf-8') as val_file:\n",
        "#     val_file.writelines(val_lines)\n",
        "\n",
        "# train_file_path = os.path.join(data_root, 'train.txt')\n",
        "# val_file_path = os.path.join(data_root, 'val.txt')\n",
        "# print(train_file_path)\n",
        "# print(val_file_path)\n",
        "# train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=[train_file_path])\n",
        "# val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=[val_file_path])\n",
        "\n",
        "print(\"data is ready....\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IgdTXEiGX9H",
        "outputId": "0664e7e8-fe3c-4e77-8c82-5300ce32f97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 30,326,784 parameters\n",
            "num non-decayed parameter tensors: 50, with 30,720 parameters\n",
            "using fused AdamW: True\n",
            "validation loss: 10.9154\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     0 | loss: 10.897057 | lr 8.5714e-07 | norm: 20.0484 | dt: 7293.09ms | tok/sec: 1123.26\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     1 | loss: 10.893776 | lr 1.7143e-06 | norm: 20.1853 | dt: 726.05ms | tok/sec: 11282.90\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     2 | loss: 10.819504 | lr 2.5714e-06 | norm: 19.2437 | dt: 749.34ms | tok/sec: 10932.31\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     3 | loss: 10.762650 | lr 3.4286e-06 | norm: 19.1763 | dt: 737.27ms | tok/sec: 11111.30\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     4 | loss: 10.678307 | lr 4.2857e-06 | norm: 17.0302 | dt: 740.53ms | tok/sec: 11062.32\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     5 | loss: 10.586082 | lr 5.1429e-06 | norm: 16.1951 | dt: 744.31ms | tok/sec: 11006.16\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     6 | loss: 10.452667 | lr 6.0000e-06 | norm: 14.7673 | dt: 748.74ms | tok/sec: 10941.00\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     7 | loss: 10.360371 | lr 6.8571e-06 | norm: 12.9539 | dt: 738.55ms | tok/sec: 11091.98\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     8 | loss: 10.242287 | lr 7.7143e-06 | norm: 11.9450 | dt: 745.68ms | tok/sec: 10985.97\n",
            "start training the model\n",
            "step 0 of training\n",
            "step     9 | loss: 10.150414 | lr 8.5714e-06 | norm: 10.5706 | dt: 741.88ms | tok/sec: 11042.21\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    10 | loss: 10.034996 | lr 9.4286e-06 | norm: 9.7616 | dt: 745.51ms | tok/sec: 10988.42\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    11 | loss: 9.885136 | lr 1.0286e-05 | norm: 9.3433 | dt: 743.36ms | tok/sec: 11020.25\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    12 | loss: 9.819645 | lr 1.1143e-05 | norm: 8.1375 | dt: 748.36ms | tok/sec: 10946.64\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    13 | loss: 9.704044 | lr 1.2000e-05 | norm: 7.6983 | dt: 746.88ms | tok/sec: 10968.36\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    14 | loss: 9.658443 | lr 1.2857e-05 | norm: 6.9815 | dt: 756.52ms | tok/sec: 10828.59\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    15 | loss: 9.637909 | lr 1.3714e-05 | norm: 6.1376 | dt: 749.48ms | tok/sec: 10930.27\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    16 | loss: 9.540699 | lr 1.4571e-05 | norm: 6.0655 | dt: 751.89ms | tok/sec: 10895.15\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    17 | loss: 9.497799 | lr 1.5429e-05 | norm: 5.1816 | dt: 745.50ms | tok/sec: 10988.65\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    18 | loss: 9.361958 | lr 1.6286e-05 | norm: 5.1501 | dt: 753.11ms | tok/sec: 10877.49\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    19 | loss: 9.357979 | lr 1.7143e-05 | norm: 4.6962 | dt: 753.32ms | tok/sec: 10874.54\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    20 | loss: 9.324545 | lr 1.8000e-05 | norm: 4.4880 | dt: 752.12ms | tok/sec: 10891.84\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    21 | loss: 9.252846 | lr 1.8857e-05 | norm: 4.4345 | dt: 751.29ms | tok/sec: 10903.84\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    22 | loss: 9.263607 | lr 1.9714e-05 | norm: 4.1755 | dt: 750.06ms | tok/sec: 10921.75\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    23 | loss: 9.208000 | lr 2.0571e-05 | norm: 4.1312 | dt: 750.25ms | tok/sec: 10919.02\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    24 | loss: 9.171520 | lr 2.1429e-05 | norm: 4.0736 | dt: 754.05ms | tok/sec: 10863.95\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    25 | loss: 9.159334 | lr 2.2286e-05 | norm: 4.0074 | dt: 750.04ms | tok/sec: 10922.02\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    26 | loss: 9.133551 | lr 2.3143e-05 | norm: 3.9855 | dt: 753.89ms | tok/sec: 10866.30\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    27 | loss: 9.124271 | lr 2.4000e-05 | norm: 3.9022 | dt: 746.54ms | tok/sec: 10973.32\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    28 | loss: 9.116674 | lr 2.4857e-05 | norm: 3.8693 | dt: 755.61ms | tok/sec: 10841.59\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    29 | loss: 9.048019 | lr 2.5714e-05 | norm: 3.9815 | dt: 753.18ms | tok/sec: 10876.62\n",
            "start training the model\n",
            "step 0 of training\n",
            "step    30 | loss: 8.965069 | lr 2.6571e-05 | norm: 4.1964 | dt: 757.74ms | tok/sec: 10811.07\n",
            "start training the model\n",
            "step 0 of training\n"
          ]
        }
      ],
      "source": [
        "\n",
        "torch.set_float32_matmul_precision('high') # this  is for gpu tunning\n",
        "\n",
        "# create model\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n",
        "model.to(device)\n",
        "use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 700\n",
        "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimize!\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
        "\n",
        "# create the log directory we will write checkpoints to and log to\n",
        "log_dir = data_root + \"/log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
        "    pass\n",
        "output_file = data_root + \"/output.txt\"\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    # once in a while evaluate our validation loss\n",
        "    if step % 250 == 0 or last_step:\n",
        "        model.eval() # we want to use model. we dont want to train it.\n",
        "        val_loader.reset()\n",
        "        with torch.no_grad():\n",
        "            val_loss_accum = 0.0\n",
        "            val_loss_steps = 20\n",
        "            for _ in range(val_loss_steps):\n",
        "                x, y = val_loader.next_batch()\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16): # do something with ram to make it efficient and sp\n",
        "                    logits, loss = model(x, y)\n",
        "                loss = loss / val_loss_steps\n",
        "                val_loss_accum += loss.detach()\n",
        "        if ddp:\n",
        "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
        "        if master_process:\n",
        "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
        "            with open(log_file, \"a\") as f:\n",
        "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
        "            if step > 0 and (step % 5000 == 0 or last_step):\n",
        "                # optionally write model checkpoints\n",
        "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'config': raw_model.config,\n",
        "                    'step': step,\n",
        "                    'val_loss': val_loss_accum.item()\n",
        "                }\n",
        "                # you might also want to add optimizer.state_dict() and\n",
        "                # rng seeds etc., if you wanted to more exactly resume training\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # once in a while evaluate hellaswag\n",
        "    # if (step % 250 == 0 or last_step) and (not use_compile):\n",
        "    #     num_correct_norm = 0\n",
        "    #     num_total = 0 # ToDo i commented the following for loop since it relates to Hellasweg\n",
        "    #     # for i, example in enumerate(iterate_examples(\"val\")):\n",
        "    #     #     # only process examples where i % ddp_world_size == ddp_rank\n",
        "    #     #     if i % ddp_world_size != ddp_rank:\n",
        "    #     #         continue\n",
        "    #     #     # render the example into tokens and labels\n",
        "    #     #     _, tokens, mask, label = render_example(example)\n",
        "    #     #     tokens = tokens.to(device)\n",
        "    #     #     mask = mask.to(device)\n",
        "    #     #     # get the logits\n",
        "    #     #     with torch.no_grad():\n",
        "    #     #         with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "    #     #             logits, loss = model(tokens)\n",
        "    #     #         pred_norm = get_most_likely_row(tokens, mask, logits)\n",
        "    #     #     num_total += 1\n",
        "    #     #     num_correct_norm += int(pred_norm == label)\n",
        "    #     # reduce the stats across all processes\n",
        "    #     if ddp:\n",
        "    #         num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n",
        "    #         num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n",
        "    #         dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n",
        "    #         dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n",
        "    #         num_total = num_total.item()\n",
        "    #         print('num_total', num_total)\n",
        "    #         num_correct_norm = num_correct_norm.item()\n",
        "    #         print('num_correct_norm', num_correct_norm)\n",
        "    #     print(\"ddp not available\")\n",
        "    #     acc_norm = num_correct_norm / num_total #if num_total != 0 else 0\n",
        "    #     if master_process:\n",
        "    #         print(f\"Dataset accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n",
        "    #         with open(log_file, \"a\") as f:\n",
        "    #             f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n",
        "\n",
        "    # once in a while generate from the model (except step 0, which is noise)\n",
        "    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n",
        "        phrase = \"دردم از یار است و درمان نیز هم\"\n",
        "        generate_poem(model, device, device_type, ddp_rank, phrase, 4)\n",
        "    # do one step of the optimization\n",
        "    print(\"start training the model\")\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        print(f\"step {micro_step} of training\")\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        # added after video, this field is also used by the forward pass.\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    if ddp:\n",
        "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if master_process:\n",
        "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZUvwnNg7wZN"
      },
      "outputs": [],
      "source": [
        "#Loading the Model and give your poems :)\n",
        "model.load_state_dict(torch.load(data_root + '/saved_model.pth'))  #replacewith model path\n",
        "phrase = \"توانا بود هرکه دانا بود\"\n",
        "generated_poems=generate_poem(model, device, device_type, ddp_rank, phrase, 1)\n",
        "\n",
        "result_file_path = data_root + '/result.txt'\n",
        "with open(result_file_path, 'w', encoding='utf-8') as f:\n",
        "    for poem in generated_poems:\n",
        "        f.write(poem + '\\n')\n",
        "\n",
        "print(f\"Generated poems saved to {result_file_path}\")\n",
        "\n",
        "\n",
        "# Flatten the special tokens into a list\n",
        "tokens_to_remove = [special_tokens[\"cls_token\"], special_tokens[\"eos_token\"]] + special_tokens[\"additional_special_tokens\"]\n",
        "#TODO\n",
        "\n",
        "# Function to remove special tokens from a line\n",
        "def remove_special_tokens(line):\n",
        "    for token in tokens_to_remove:\n",
        "        if token == 'BOM':\n",
        "            line = line.replace(token, \"\\n\")\n",
        "            continue\n",
        "        line = line.replace(token, \"\")\n",
        "    return line\n",
        "\n",
        "# Open the input file for reading and the output file for writing\n",
        "input_file_path = data_root + \"/result.txt\"  # Change this to your input file path\n",
        "output_file_path = data_root + '/' + 'final_output.txt'  # Change this to your desired output file path\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        # Remove special tokens from the current line\n",
        "        cleaned_line = remove_special_tokens(line)\n",
        "        # Write the cleaned line to the output file\n",
        "        print(cleaned_line)\n",
        "        outfile.write(cleaned_line)\n",
        "\n",
        "print(\"Special tokens removed and output saved to\", output_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFpQzTpVw201"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
