{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25543,"status":"ok","timestamp":1723579775928,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"Ax4CMTRg3EBB","outputId":"a9043180-b23f-4fc6-fb9d-41bd1b5dc4e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9719,"status":"ok","timestamp":1723579870736,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"41ZHB0qDWqeJ","outputId":"6d210111-dbd3-4125-dded-b40704751a66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting hazm\n","  Downloading hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\n","Collecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n","  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting flashtext<3.0,>=2.7 (from hazm)\n","  Downloading flashtext-2.7.tar.gz (14 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.3)\n","Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n","Collecting numpy==1.24.3 (from hazm)\n","  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Collecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n","  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.3.2)\n","Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm)\n","  Downloading pybind11-2.13.3-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (71.0.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.5)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.16.0)\n","Downloading hazm-0.10.0-py3-none-any.whl (892 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybind11-2.13.3-py3-none-any.whl (240 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.5/240.5 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: flashtext\n","  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9297 sha256=86ef724bbb2188c8a47656ea41300fab1493c2c8625b6a39a4e3f165cfed32b9\n","  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n","Successfully built flashtext\n","Installing collected packages: python-crfsuite, flashtext, pybind11, numpy, fasttext-wheel, hazm\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n","albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n","albumentations 1.4.13 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n","pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 hazm-0.10.0 numpy-1.24.3 pybind11-2.13.3 python-crfsuite-0.9.10\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"32d696f85c6f416486d3c8c45b3e6320","pip_warning":{"packages":["numpy"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install hazm"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2736,"status":"ok","timestamp":1723579908912,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"VTP7QzghXe6z"},"outputs":[],"source":["# import libraries\n","import os\n","import math\n","import time\n","import inspect\n","from dataclasses import dataclass\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from transformers import AutoTokenizer\n","import hazm"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":301,"referenced_widgets":["39ec92bf4ddb4aaa9876704dc7d82b21","78d400e5fcd04e5dbc1db33e27b89773","c21a2002a0b942c19ace265187839446","143b9eeaacfa4c2d9937983c25d1d914","748c3ab6f50c4311a4a4a735bd500c29","861f4d22fcc74ab0bdf5bf7a9baa47ee","a7013ea169d747afa1502ec9ae1691ee","a7ebc8cded0c4845b8524be11613f6d9","091f011f45ea49d3a1b99a8d73774300","94b807f17d1a41b88173be958ed9407c","fd3c2b70d2224e91a9a153f92c87014c","b715226a8e74438ab57ab420a68032f1","cbb726e184254d628185851b1b005586","1e6441edbdff4ec498fc546cc835c766","da640495e7ff4d1ab86569e03c4bbdb5","0dc66a613b4443ac94c4ec3cd2cd0e85","c354236f0895488e9ac33c3051104251","62d656cd521049ac98b939e2fa691496","848840303de7472a99fca57043010b05","a5742a3f281148679bf065f6af796156","bc3753cb8ae44ab8806e8965110f227b","d9750b9b4815455496cb3fb5b7e64234","2572f68c632241ed9ffa2e3cf078b69c","66b1a8eee64142c2be7768e0e495bcd2","259d42a307004923b8b9c6881d0d4589","43122ecf63e64a09ae9387fe21f492bc","4e6a79772d24433ca166a768ea48cef3","a6d03e93718a43c3a205e4de64280e6a","314a9b57acb24987bde6634e3d9813fd","db6a04628ccf4261965f85c8264311e3","4f12abf8010b4615adeee50809ae5db1","963f94a5f31e4debb36567d7550abb29","37054404433d4e74903c8e641e29df3b","2752fbd5fa7847f0a925eecde894c638","a8d494aa933b458592b08ca2d58eb4ea","112b5b9127b24720b591c00ec5f40ac2","d0f01efaaa834febb9ab57e98f67eb0a","1b9bdb0c89f0404a89be212dca956cab","24f0759ee6d440d2963e8fa824b4c06a","f5d6188818d24770ab57a2800d3c8267","87a87380ca1142f49b5c1f05dd9e1723","6a8449369f1042ea9bd8fadfce84fc53","caa2f51656804341884c1a7145745348","da19ee19a49c4f2dba0e9b247c028093","a13180afe7fb45ef93f4aabed7cc01c7","1fb1fd94f3f14d4f90df3ccff0634e49","50929f9ad4874450a9e9d45103866742","345905204ef9408caf934d446fd880fd","f616126382fc40848f72887b1250852c","8b6b17861c714a579e2a739e00953701","7f005153801945bda21cba706486478c","69797c101e7c403e94adf4fc62716633","3ef31c0fadbb4302b89333d26bdd464d","fc2ea1199ef0413ba1f099f722b7022a","f8b65c2838c3422ca966b8bdeaa9c9ca"]},"executionInfo":{"elapsed":6978,"status":"ok","timestamp":1723579915888,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"3reIGFswbaJZ","outputId":"f2069f6e-b6df-4b15-b6df-c605cf4900f7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39ec92bf4ddb4aaa9876704dc7d82b21","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b715226a8e74438ab57ab420a68032f1","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2572f68c632241ed9ffa2e3cf078b69c","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/537k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2752fbd5fa7847f0a925eecde894c638","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.13M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a13180afe7fb45ef93f4aabed7cc01c7","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Defining tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n","special_tokens = {\"cls_token\": \"[CLS]\", \"additional_special_tokens\":[\"[BOM]\",\"[BOB]\",\"[SEP]\",\"<unk>\"],\"eos_token\":\"[EOS]\"}\n","tokenizer.add_special_tokens(special_tokens)\n","tokenizer.cls_token = \"[CLS]\"\n","tokenizer.bob_token =\"[BOB]\"\n","tokenizer.bom_token = \"[BOM]\"\n","tokenizer.eos_token =\"[EOS]\"\n","enc = tokenizer"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10085,"status":"ok","timestamp":1723580911004,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"v1d_K_bTXe60","outputId":"df749e4c-371a-4624-a8a3-4dad89d544ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/gpt/cleanning\n"]}],"source":["# Special tokens will be added, input txt files will be aggregated to one and data will be tokenized...\n","data_root = \"/content/drive/MyDrive/gpt\" # a path for input, log and output files\n","poems_file_name = \"aggregated_data_file.txt\" # aggregated data\n","cls_token = '[CLS]'\n","bob_token = '[BOB]'\n","bom_token = '[BOM]'\n","eos_token = '[EOS]'\n","\n","%cd /content/drive/MyDrive/gpt/cleaning  # pth to custom functions\n","\n","from main_clean import clean #custom function for cleaning data\n","import check    #custom function for checking and saving\n","def aggregate_data_files_and_add_special_tokens():\n","    _data_root = data_root + \"/input/\"\n","    all_files = [os.path.join(_data_root + f) for f in os.listdir(_data_root ) if f.endswith('.txt')]\n","    aggregated_text = ''\n","\n","# # /**************************************\n","    #cleaning\n","    for file in all_files:\n","      with open(file, 'r', encoding='utf-8') as f:\n","        texts = f.readlines()\n","        list_clean = clean(texts)\n","        print(list_clean[0:20])\n","        f=check.save(f,list_clean)\n","\n","\n","\n","\n","#     import check\n","#     print(\"check english char: \", check.check_english_char(list_clean))\n","#     print(\"check number: \", check.check_numbers(list_clean))\n","\n","#     character = \"[-=!﷼٪×*٫\\٬)\\(]}{؛«ٰ»ٰٓ‌ٔء><}٪]\"\n","#     print(\"check special char: \", check.check_special_charecter(list_clean, character)) # except ،\n","#     print(list_clean)\n","\n","\n","\n","\n","\n","\n","# /**************************************\n","    for file in all_files:\n","\n","        with open(file, 'r', encoding='utf-8') as f:\n","            text = f.readlines()[2:] # TODO\n","            is_beit = True\n","            poem_str = f'\\n{cls_token}'\n","\n","            for line in text:\n","                if line.strip() == '':\n","                    continue\n","                poem_str += f\"{bob_token + bom_token if is_beit else bom_token}{line}\"\n","                is_beit = not is_beit\n","        poem_str += f'{eos_token}'\n","\n","        aggregated_text += poem_str.strip()\n","\n","    output_file_path = data_root + \"/input/\" + poems_file_name # Set the path for the output file\n","    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n","        output_file.write(aggregated_text)\n","\n","    all_tokens = tokenizer.encode(aggregated_text, add_special_tokens=True)\n","    tokens_np = np.array(all_tokens, dtype=np.uint16)\n","    _data_root = data_root + '/input/'\n","    poems_file_path = os.path.join(_data_root + 'poems.npy')\n","    np.save(poems_file_path, tokens_np)\n","\n","    n = int(0.9 * len(all_tokens))\n","    train_data = tokens_np[:n]\n","    val_data = tokens_np[n:]\n","\n","\n","    train_file = os.path.join(_data_root, 'train.npy')\n","\n","    np.save(train_file, train_data)\n","    val_file = os.path.join(_data_root, 'val.npy')\n","    np.save(val_file, val_data)\n","\n","    return"]},{"cell_type":"markdown","metadata":{"id":"XMpfbSKubaJa"},"source":["![Attention is all you need. High quality image :)](Capture.JPG)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":562,"status":"ok","timestamp":1723580923065,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"3UKd5mq33VFb"},"outputs":[],"source":["# Defining Transformer model. This cell implements what the above image explains. source: <Attention is all you need>\n","class CausalSelfAttention(nn.Module): # Implements self-attention mechanism\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1 # standard deviation grows inside the residual stream. This line controls it.\n","        # regularization\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","\n","    def forward(self, x):\n","        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n","        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n","        qkv = self.c_attn(x)\n","        q, k, v = qkv.split(self.n_embd, dim=2)\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n","        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n","        # output projection\n","        y = self.c_proj(y)\n","        return y\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n","        self.gelu    = nn.GELU(approximate='tanh')\n","        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        return x\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embd)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embd)\n","        self.mlp = MLP(config)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x)) # self.ln_1(x) noramlization --> attention think about all tokens together. aggregated function\n","        x = x + self.mlp(self.ln_2(x)) # self.ln_2(x) --> multi linear perc(?) # think individually about tokens\n","        return x\n","\n","@dataclass\n","class GPTConfig:\n","    block_size: int = 1024 # max sequence length\n","    vocab_size: int =  25005 # tokenizer.vocab_size number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n","    n_layer: int = 6 # number of layers\n","    n_head: int = 6 # number of heads\n","    n_embd: int = 384 # embedding dimension\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embd), # token encoding the first box of the picture\n","            wpe = nn.Embedding(config.block_size, config.n_embd), # position encoding\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # total block in the picture of attention is all you need\n","            ln_f = nn.LayerNorm(config.n_embd), # linear part of picture\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # weight sharing scheme\n","        self.transformer.wte.weight = self.lm_head.weight\n","\n","        # init params\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):# if a layer is linear use normal distribution std is different\n","            std = 0.02\n","            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n","                std *= (2 * self.config.n_layer) ** -0.5\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        # idx is of shape (B, T)\n","        B, T = idx.size()\n","        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n","        # forward the token and posisition embeddings\n","        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n","        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n","        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n","        x = tok_emb + pos_emb\n","        # forward the blocks of the transformer\n","        for block in self.transformer.h:\n","            x = block(x)\n","        # forward the final layernorm and the classifier\n","        x = self.transformer.ln_f(x)\n","        logits = self.lm_head(x) # (B, T, vocab_size)\n","        loss = None\n","        if targets is not None: # if we have target in data we calculate loss as follows\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # flatten data by targets.view(-1)\n","        return logits, loss\n","\n","    @classmethod\n","    def from_pretrained(cls, model_type): # loading wieghts. this is a constructor or class method\n","        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n","        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n","        from transformers import GPT2LMHeadModel\n","        print(\"loading weights from pretrained gpt: %s\" % model_type)\n","\n","        # n_layer, n_head and n_embd are determined from model_type\n","        config_args = {\n","            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n","            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n","            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n","            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n","        }[model_type]\n","        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n","        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n","        # create a from-scratch initialized minGPT model\n","        config = GPTConfig(**config_args)\n","        model = GPT(config)\n","        sd = model.state_dict()\n","        sd_keys = sd.keys()\n","        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n","\n","        # init a huggingface/transformers model\n","        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n","        sd_hf = model_hf.state_dict()\n","\n","        # copy while ensuring all of the parameters are aligned and match in names and shapes\n","        sd_keys_hf = sd_hf.keys()\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n","        # this means that we have to transpose these weights when we import them\n","        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n","        for k in sd_keys_hf:\n","            if any(k.endswith(w) for w in transposed):\n","                # special treatment for the Conv1D weights we need to transpose\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t())\n","            else:\n","                # vanilla copy over the other parameters\n","                assert sd_hf[k].shape == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k])\n","\n","        return model\n","\n","    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n","        # start with all of the candidate parameters (that require grad)\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n","        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n","        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n","        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n","        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n","        optim_groups = [\n","            {'params': decay_params, 'weight_decay': weight_decay},\n","            {'params': nodecay_params, 'weight_decay': 0.0}\n","        ]\n","        num_decay_params = sum(p.numel() for p in decay_params)\n","        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n","\n","        if master_process:\n","            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n","            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n","        # Create AdamW optimizer and use the fused version if it is available\n","        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n","        use_fused = fused_available and device_type == \"cuda\"\n","        # use_fused = fused_available and device_type == \"mps\"\n","        if master_process:\n","            print(f\"using fused AdamW: {use_fused}\")\n","        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n","        return optimizer"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1723580926631,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"FBF-J9ke3cVc"},"outputs":[],"source":["\n","def load_tokens(filename):\n","    npt = np.load(filename)\n","    npt = npt.astype(np.int32) # added after video\n","    ptt = torch.tensor(npt, dtype=torch.long)\n","    return ptt\n","\n","class DataLoaderLite:\n","    def __init__(self, B, T, process_rank, num_processes, split):\n","        self.B = B\n","        self.T = T\n","        self.process_rank = process_rank\n","        self.num_processes = num_processes\n","        assert split in {'train', 'val'}\n","        data_root = \"/content/drive/MyDrive/gpt/input/\"\n","        # get the shard filenames\n","        shards = os.listdir(data_root)\n","        shards = [s for s in shards if split in s]\n","        shards = sorted(shards)\n","        shards = [os.path.join(data_root,  s) for s in shards]\n","        print(shards)\n","        self.shards = shards\n","        assert len(shards) > 0, f\"no shards found for split {split}\"\n","\n","        if master_process:\n","            print(f\"found {len(shards)} shards for split {split}\")\n","        self.reset()\n","\n","    def reset(self):\n","        # state, init at shard zero\n","        self.current_shard = 0\n","        self.tokens = load_tokens(self.shards[self.current_shard])\n","        self.current_position = self.B * self.T * self.process_rank\n","\n","    def next_batch(self):\n","        B, T = self.B, self.T\n","        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n","        x = (buf[:-1]).view(B, T) # inputs\n","        y = (buf[1:]).view(B, T) # targets\n","        # advance the position in the tensor\n","        self.current_position += B * T * self.num_processes\n","        # if loading the next batch would be out of bounds, advance to next shard\n","        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n","            self.current_shard = (self.current_shard + 1) % len(self.shards)\n","            self.tokens = load_tokens(self.shards[self.current_shard])\n","            self.current_position = B * T * self.process_rank\n","        return x, y\n","\n","# -----------------------------------------------------------------------------\n","# helper function for HellaSwag eval\n","# takes tokens, mask, and logits, returns the index of the completion with the lowest loss\n","\n","def get_most_likely_row(tokens, mask, logits):\n","    # evaluate the autoregressive loss at all positions\n","    shift_logits = (logits[..., :-1, :]).contiguous()\n","    shift_tokens = (tokens[..., 1:]).contiguous()\n","    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n","    flat_shift_tokens = shift_tokens.view(-1)\n","    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n","    shift_losses = shift_losses.view(tokens.size(0), -1)\n","    # now get the average loss just for the completion region (where mask == 1), in each row\n","    shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n","    masked_shift_losses = shift_losses * shift_mask\n","    # sum and divide by the number of 1s in the mask\n","    sum_loss = masked_shift_losses.sum(dim=1)\n","    avg_loss = sum_loss / shift_mask.sum(dim=1)\n","    # now we have a loss for each of the 4 completions\n","    # the one with the lowest loss should be the most likely\n","    pred_norm = avg_loss.argmin().item()\n","    return pred_norm"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":1027,"status":"ok","timestamp":1723580930247,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"Hziv-34h2oyU"},"outputs":[],"source":["# This cell generates poem and saves the trained model. Generated poems will be saved into a file.\n","def generate_poem(model, device, device_type, ddp_rank, phrase, num_return_sequences):\n","    result = 'result.txt'\n","    model.eval()\n","    max_length = 32\n","    tokens = enc.encode(phrase)\n","    tokens = torch.tensor(tokens, dtype=torch.long)\n","    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n","    xgen = tokens.to(device)\n","    sample_rng = torch.Generator(device=device)\n","    sample_rng.manual_seed(42 + ddp_rank)\n","\n","    generated_poems = []  # List to store generated poems\n","\n","    while xgen.size(1) < max_length:\n","        # forward the model to get the logits\n","        with torch.no_grad():\n","            with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n","                logits, loss = model(xgen) # (B, T, vocab_size)\n","            # take the logits at the last position\n","            logits = logits[:, -1, :] # (B, vocab_size)\n","            # get the probabilities\n","            probs = F.softmax(logits, dim=-1)\n","            # do top-k sampling of 50 (huggingface pipeline default)\n","            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # return top k high probability\n","            # select a token from the top-k probabilities\n","            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n","            # gather the corresponding indices\n","            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n","            # append to the sequence\n","            xgen = torch.cat((xgen, xcol), dim=1)\n","\n","    # collect and save the generated text\n","    for i in range(num_return_sequences):\n","        tokens = xgen[i, :max_length].tolist()\n","        decoded = enc.decode(tokens)\n","        print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n","        generated_poems.append(decoded)\n","\n","    # Save the model state (optional)\n","    torch.save(model.state_dict(), data_root + '/saved_model.pth')\n","\n","    # Save the generated poems to a file\n","    output_file = data_root + \"/\" + result\n","    with open(output_file, \"a\") as out_f:\n","        for i, poem in enumerate(generated_poems):\n","            out_f.write(f\"sample {i}: {poem}\\n\")\n","\n","    return generated_poems  # Return the list of generated poems\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"elapsed":12,"status":"error","timestamp":1723580933379,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"Mt0h7QuU3kKy","outputId":"2d04271d-08a2-4593-f892-ddc7feec48d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["False DDP\n","using device: cuda\n","total desired batch size: 8192\n","=> calculated gradient accumulation steps: 1\n"]},{"ename":"AttributeError","evalue":"'str' object has no attribute 'readlines'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-3ea744b69d84>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0maggregate_data_files_and_add_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoaderLite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddp_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddp_world_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-f27b1ea32048>\u001b[0m in \u001b[0;36maggregate_data_files_and_add_special_tokens\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;31m# fp = open('/content/drive/MyDrive/gpt/input/abdullah.txt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mlist_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'readlines'"]}],"source":["# Loading data, split data for train and validation, Training model\n","\n","from torch.distributed import init_process_group, destroy_process_group\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","import torch.distributed as dist\n","\n","# set up DDP (distributed data parallel).\n","# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n","ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n","print(ddp, \"DDP\")\n","if ddp:\n","    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n","    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n","    init_process_group(backend='nccl')\n","    ddp_rank = int(os.environ['RANK'])\n","    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n","    ddp_world_size = int(os.environ['WORLD_SIZE'])\n","    device = f'cuda:{ddp_local_rank}'\n","    torch.cuda.set_device(device)\n","    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n","else:\n","    # vanilla, non-DDP run\n","    ddp_rank = 0\n","    ddp_local_rank = 0\n","    ddp_world_size = 1\n","    master_process = True\n","    # attempt to autodetect device\n","    device = \"cpu\"\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","        device = \"mps\"\n","    print(f\"using device: {device}\")\n","\n","# added after video, pytorch can be serious about it's device vs. device_type distinction\n","device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(1337)\n","\n","# enc = tiktoken.get_encoding(\"gpt2\")\n","\n","total_batch_size = 8192 #524288 # 2**19, ~0.5M, in number of tokens\n","B = 8 # micro batch size\n","T = 1024 # sequence length\n","assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n","grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n","if master_process:\n","    print(f\"total desired batch size: {total_batch_size}\")\n","    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n","\n","\n","import random\n","\n","# all_files = [os.path.join(data_root, f) for f in os.listdir(data_root) if f.endswith('.txt')]\n","# random.shuffle(all_files)  # Shuffle to ensure random split\n","\n","# split_index = int(0.9 * len(all_files))  # 90% for training, 10% for validation\n","# train_files = all_files[:split_index]\n","# val_files = all_files[split_index:]\n","\n","# train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=train_files)\n","# val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=val_files)\n","# new method\n","\n","\n","aggregate_data_files_and_add_special_tokens()\n","\n","train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n","val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n","\n","\n","# # Shuffle lines to ensure randomness\n","# random.shuffle(all_lines)\n","\n","# # Split the lines into training and validation sets\n","# split_index = int(0.9 * len(all_lines))  # 90% for training, 10% for validation\n","# train_lines = all_lines[:split_index]\n","# val_lines = all_lines[split_index:]\n","# with open(os.path.join(data_root, 'train.txt'), 'w', encoding='utf-8') as train_file:\n","#     train_file.writelines(train_lines)\n","\n","# with open(os.path.join(data_root, 'val.txt'), 'w', encoding='utf-8') as val_file:\n","#     val_file.writelines(val_lines)\n","\n","# train_file_path = os.path.join(data_root, 'train.txt')\n","# val_file_path = os.path.join(data_root, 'val.txt')\n","# print(train_file_path)\n","# print(val_file_path)\n","# train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=[train_file_path])\n","# val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, files=[val_file_path])\n","\n","print(\"data is ready....\")\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":16905,"status":"error","timestamp":1723580619883,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"8IgdTXEiGX9H","outputId":"e3b1c5dd-ca06-4566-b8c0-afa84959a8de"},"outputs":[{"name":"stdout","output_type":"stream","text":["num decayed parameter tensors: 26, with 30,326,784 parameters\n","num non-decayed parameter tensors: 50, with 30,720 parameters\n","using fused AdamW: True\n","validation loss: 10.8948\n","start training the model\n","step 0 of training\n","step     0 | loss: 10.888702 | lr 8.5714e-07 | norm: 19.9505 | dt: 2040.50ms | tok/sec: 4014.70\n","start training the model\n","step 0 of training\n","step     1 | loss: 10.850624 | lr 1.7143e-06 | norm: 19.5964 | dt: 93.51ms | tok/sec: 87601.00\n","start training the model\n","step 0 of training\n","step     2 | loss: 10.801247 | lr 2.5714e-06 | norm: 18.1673 | dt: 95.35ms | tok/sec: 85911.59\n","start training the model\n","step 0 of training\n","step     3 | loss: 10.722000 | lr 3.4286e-06 | norm: 18.3235 | dt: 95.89ms | tok/sec: 85431.40\n","start training the model\n","step 0 of training\n","step     4 | loss: 10.628532 | lr 4.2857e-06 | norm: 17.9640 | dt: 95.65ms | tok/sec: 85646.05\n","start training the model\n","step 0 of training\n","step     5 | loss: 10.517639 | lr 5.1429e-06 | norm: 17.1147 | dt: 95.99ms | tok/sec: 85345.88\n","start training the model\n","step 0 of training\n","step     6 | loss: 10.342644 | lr 6.0000e-06 | norm: 16.5837 | dt: 95.06ms | tok/sec: 86173.16\n","start training the model\n","step 0 of training\n","step     7 | loss: 10.205658 | lr 6.8571e-06 | norm: 15.1656 | dt: 95.67ms | tok/sec: 85624.70\n","start training the model\n","step 0 of training\n","step     8 | loss: 10.054558 | lr 7.7143e-06 | norm: 13.6157 | dt: 96.01ms | tok/sec: 85328.07\n","start training the model\n","step 0 of training\n","step     9 | loss: 9.883080 | lr 8.5714e-06 | norm: 12.6194 | dt: 95.57ms | tok/sec: 85717.41\n","start training the model\n","step 0 of training\n","step    10 | loss: 9.771255 | lr 9.4286e-06 | norm: 11.3184 | dt: 95.59ms | tok/sec: 85698.17\n","start training the model\n","step 0 of training\n","step    11 | loss: 9.588974 | lr 1.0286e-05 | norm: 10.8152 | dt: 97.49ms | tok/sec: 84030.51\n","start training the model\n","step 0 of training\n","step    12 | loss: 9.505188 | lr 1.1143e-05 | norm: 9.8868 | dt: 95.73ms | tok/sec: 85576.08\n","start training the model\n","step 0 of training\n","step    13 | loss: 9.343662 | lr 1.2000e-05 | norm: 9.6932 | dt: 96.04ms | tok/sec: 85297.15\n","start training the model\n","step 0 of training\n","step    14 | loss: 9.235435 | lr 1.2857e-05 | norm: 9.2567 | dt: 96.01ms | tok/sec: 85327.44\n","start training the model\n","step 0 of training\n","step    15 | loss: 9.173470 | lr 1.3714e-05 | norm: 8.4680 | dt: 95.67ms | tok/sec: 85625.56\n","start training the model\n","step 0 of training\n","step    16 | loss: 9.004826 | lr 1.4571e-05 | norm: 8.6015 | dt: 96.40ms | tok/sec: 84977.34\n","start training the model\n","step 0 of training\n","step    17 | loss: 8.855156 | lr 1.5429e-05 | norm: 8.6578 | dt: 95.10ms | tok/sec: 86141.40\n","start training the model\n","step 0 of training\n","step    18 | loss: 8.719368 | lr 1.6286e-05 | norm: 8.7891 | dt: 95.27ms | tok/sec: 85983.61\n","start training the model\n","step 0 of training\n","step    19 | loss: 8.593918 | lr 1.7143e-05 | norm: 8.7803 | dt: 94.81ms | tok/sec: 86400.90\n","start training the model\n","step 0 of training\n","step    20 | loss: 8.533123 | lr 1.8000e-05 | norm: 8.1769 | dt: 95.29ms | tok/sec: 85973.28\n","start training the model\n","step 0 of training\n","step    21 | loss: 8.334530 | lr 1.8857e-05 | norm: 8.0171 | dt: 96.41ms | tok/sec: 84971.04\n","start training the model\n","step 0 of training\n","step    22 | loss: 8.184574 | lr 1.9714e-05 | norm: 7.5191 | dt: 95.06ms | tok/sec: 86180.94\n","start training the model\n","step 0 of training\n","step    23 | loss: 8.180260 | lr 2.0571e-05 | norm: 7.1905 | dt: 96.03ms | tok/sec: 85307.95\n","start training the model\n","step 0 of training\n","step    24 | loss: 7.970585 | lr 2.1429e-05 | norm: 6.9223 | dt: 95.47ms | tok/sec: 85807.75\n","start training the model\n","step 0 of training\n","step    25 | loss: 7.841366 | lr 2.2286e-05 | norm: 6.6453 | dt: 96.01ms | tok/sec: 85324.05\n","start training the model\n","step 0 of training\n","step    26 | loss: 7.710419 | lr 2.3143e-05 | norm: 6.3519 | dt: 96.61ms | tok/sec: 84796.36\n","start training the model\n","step 0 of training\n","step    27 | loss: 7.593498 | lr 2.4000e-05 | norm: 6.0215 | dt: 95.58ms | tok/sec: 85708.00\n","start training the model\n","step 0 of training\n","step    28 | loss: 7.400288 | lr 2.4857e-05 | norm: 5.7514 | dt: 96.39ms | tok/sec: 84989.53\n","start training the model\n","step 0 of training\n","step    29 | loss: 7.136898 | lr 2.5714e-05 | norm: 6.1960 | dt: 95.90ms | tok/sec: 85420.56\n","start training the model\n","step 0 of training\n","step    30 | loss: 6.918375 | lr 2.6571e-05 | norm: 6.0304 | dt: 95.67ms | tok/sec: 85626.41\n","start training the model\n","step 0 of training\n","step    31 | loss: 6.866043 | lr 2.7429e-05 | norm: 5.5347 | dt: 96.80ms | tok/sec: 84630.11\n","start training the model\n","step 0 of training\n","step    32 | loss: 6.686033 | lr 2.8286e-05 | norm: 5.3236 | dt: 96.16ms | tok/sec: 85193.10\n","start training the model\n","step 0 of training\n","step    33 | loss: 6.668140 | lr 2.9143e-05 | norm: 5.0880 | dt: 96.33ms | tok/sec: 85038.96\n","start training the model\n","step 0 of training\n","step    34 | loss: 6.542692 | lr 3.0000e-05 | norm: 5.0756 | dt: 95.93ms | tok/sec: 85392.54\n","start training the model\n","step 0 of training\n","step    35 | loss: 6.609699 | lr 3.0857e-05 | norm: 4.8304 | dt: 95.31ms | tok/sec: 85952.42\n","start training the model\n","step 0 of training\n","step    36 | loss: 6.375385 | lr 3.1714e-05 | norm: 4.7577 | dt: 95.84ms | tok/sec: 85473.05\n","start training the model\n","step 0 of training\n","step    37 | loss: 6.303125 | lr 3.2571e-05 | norm: 4.6953 | dt: 95.41ms | tok/sec: 85861.35\n","start training the model\n","step 0 of training\n","step    38 | loss: 6.326174 | lr 3.3429e-05 | norm: 4.5003 | dt: 95.32ms | tok/sec: 85940.60\n","start training the model\n","step 0 of training\n","step    39 | loss: 6.108980 | lr 3.4286e-05 | norm: 4.5718 | dt: 94.65ms | tok/sec: 86548.46\n","start training the model\n","step 0 of training\n","step    40 | loss: 6.032434 | lr 3.5143e-05 | norm: 4.6255 | dt: 95.24ms | tok/sec: 86016.33\n","start training the model\n","step 0 of training\n","step    41 | loss: 6.050760 | lr 3.6000e-05 | norm: 4.6295 | dt: 96.26ms | tok/sec: 85106.79\n","start training the model\n","step 0 of training\n","step    42 | loss: 6.377705 | lr 3.6857e-05 | norm: 4.3839 | dt: 95.72ms | tok/sec: 85586.95\n","start training the model\n","step 0 of training\n","step    43 | loss: 5.898123 | lr 3.7714e-05 | norm: 4.4923 | dt: 96.02ms | tok/sec: 85318.33\n","start training the model\n","step 0 of training\n","step    44 | loss: 5.891005 | lr 3.8571e-05 | norm: 4.4139 | dt: 96.56ms | tok/sec: 84839.49\n","start training the model\n","step 0 of training\n","step    45 | loss: 5.884171 | lr 3.9429e-05 | norm: 4.3023 | dt: 95.08ms | tok/sec: 86157.82\n","start training the model\n","step 0 of training\n","step    46 | loss: 5.808159 | lr 4.0286e-05 | norm: 4.3183 | dt: 94.94ms | tok/sec: 86287.64\n","start training the model\n","step 0 of training\n","step    47 | loss: 5.482121 | lr 4.1143e-05 | norm: 4.5746 | dt: 94.70ms | tok/sec: 86501.18\n","start training the model\n","step 0 of training\n","step    48 | loss: 5.591040 | lr 4.2000e-05 | norm: 4.4340 | dt: 95.77ms | tok/sec: 85537.10\n","start training the model\n","step 0 of training\n","step    49 | loss: 5.725475 | lr 4.2857e-05 | norm: 4.2244 | dt: 95.48ms | tok/sec: 85802.39\n","start training the model\n","step 0 of training\n","step    50 | loss: 7.314577 | lr 4.3714e-05 | norm: 3.3848 | dt: 95.54ms | tok/sec: 85745.43\n","start training the model\n","step 0 of training\n","step    51 | loss: 9.570864 | lr 4.4571e-05 | norm: 3.5690 | dt: 95.77ms | tok/sec: 85541.14\n","start training the model\n","step 0 of training\n","step    52 | loss: 6.356337 | lr 4.5429e-05 | norm: 4.0536 | dt: 95.89ms | tok/sec: 85433.73\n","start training the model\n","step 0 of training\n","step    53 | loss: 5.416306 | lr 4.6286e-05 | norm: 5.1484 | dt: 96.05ms | tok/sec: 85288.47\n","start training the model\n","step 0 of training\n","step    54 | loss: 5.410357 | lr 4.7143e-05 | norm: 4.6887 | dt: 96.60ms | tok/sec: 84802.22\n","start training the model\n","step 0 of training\n","step    55 | loss: 5.425003 | lr 4.8000e-05 | norm: 4.4326 | dt: 96.20ms | tok/sec: 85159.10\n","start training the model\n","step 0 of training\n","step    56 | loss: 5.902370 | lr 4.8857e-05 | norm: 4.2789 | dt: 96.23ms | tok/sec: 85127.45\n","start training the model\n","step 0 of training\n","step    57 | loss: 6.523794 | lr 4.9714e-05 | norm: 3.5937 | dt: 96.08ms | tok/sec: 85264.34\n","start training the model\n","step 0 of training\n","step    58 | loss: 5.620329 | lr 5.0571e-05 | norm: 4.2771 | dt: 96.97ms | tok/sec: 84483.61\n","start training the model\n","step 0 of training\n","step    59 | loss: 5.540260 | lr 5.1429e-05 | norm: 4.2171 | dt: 96.37ms | tok/sec: 85009.72\n","start training the model\n","step 0 of training\n","step    60 | loss: 6.880676 | lr 5.2286e-05 | norm: 3.3106 | dt: 96.35ms | tok/sec: 85021.29\n","start training the model\n","step 0 of training\n","step    61 | loss: 7.068624 | lr 5.3143e-05 | norm: 3.1672 | dt: 96.52ms | tok/sec: 84869.46\n","start training the model\n","step 0 of training\n","step    62 | loss: 6.500349 | lr 5.4000e-05 | norm: 3.4347 | dt: 96.32ms | tok/sec: 85047.17\n","start training the model\n","step 0 of training\n","step    63 | loss: 5.153265 | lr 5.4857e-05 | norm: 4.6893 | dt: 96.91ms | tok/sec: 84534.33\n","start training the model\n","step 0 of training\n","step    64 | loss: 5.134302 | lr 5.5714e-05 | norm: 4.3839 | dt: 96.44ms | tok/sec: 84940.58\n","start training the model\n","step 0 of training\n","step    65 | loss: 5.166126 | lr 5.6571e-05 | norm: 4.5591 | dt: 95.16ms | tok/sec: 86084.43\n","start training the model\n","step 0 of training\n","step    66 | loss: 5.064743 | lr 5.7429e-05 | norm: 4.4476 | dt: 94.84ms | tok/sec: 86373.75\n","start training the model\n","step 0 of training\n","step    67 | loss: 5.021784 | lr 5.8286e-05 | norm: 4.3501 | dt: 94.52ms | tok/sec: 86670.72\n","start training the model\n","step 0 of training\n","step    68 | loss: 5.022200 | lr 5.9143e-05 | norm: 4.3411 | dt: 95.21ms | tok/sec: 86045.41\n","start training the model\n","step 0 of training\n","step    69 | loss: 5.280054 | lr 6.0000e-05 | norm: 4.0374 | dt: 95.94ms | tok/sec: 85386.39\n","start training the model\n","step 0 of training\n","step    70 | loss: 5.570908 | lr 6.0857e-05 | norm: 3.9298 | dt: 96.07ms | tok/sec: 85274.50\n","start training the model\n","step 0 of training\n","step    71 | loss: 5.520029 | lr 6.1714e-05 | norm: 3.9033 | dt: 96.20ms | tok/sec: 85151.71\n","start training the model\n","step 0 of training\n","step    72 | loss: 5.194542 | lr 6.2571e-05 | norm: 3.9668 | dt: 96.20ms | tok/sec: 85152.14\n","start training the model\n","step 0 of training\n","step    73 | loss: 4.890081 | lr 6.3429e-05 | norm: 4.2635 | dt: 96.71ms | tok/sec: 84706.05\n","start training the model\n","step 0 of training\n","step    74 | loss: 4.873219 | lr 6.4286e-05 | norm: 4.1805 | dt: 95.92ms | tok/sec: 85400.61\n","start training the model\n","step 0 of training\n","step    75 | loss: 4.857726 | lr 6.5143e-05 | norm: 4.1683 | dt: 95.95ms | tok/sec: 85381.51\n","start training the model\n","step 0 of training\n","step    76 | loss: 4.803244 | lr 6.6000e-05 | norm: 4.1639 | dt: 96.73ms | tok/sec: 84689.56\n","start training the model\n","step 0 of training\n","step    77 | loss: 4.799715 | lr 6.6857e-05 | norm: 4.1100 | dt: 96.14ms | tok/sec: 85208.10\n","start training the model\n","step 0 of training\n","step    78 | loss: 4.745065 | lr 6.7714e-05 | norm: 4.1413 | dt: 96.67ms | tok/sec: 84739.89\n","start training the model\n","step 0 of training\n","step    79 | loss: 5.165569 | lr 6.8571e-05 | norm: 3.7518 | dt: 95.73ms | tok/sec: 85575.44\n","start training the model\n","step 0 of training\n","step    80 | loss: 5.141985 | lr 6.9429e-05 | norm: 3.7814 | dt: 96.40ms | tok/sec: 84976.08\n","start training the model\n","step 0 of training\n","step    81 | loss: 5.321776 | lr 7.0286e-05 | norm: 3.6448 | dt: 96.62ms | tok/sec: 84782.76\n","start training the model\n","step 0 of training\n","step    82 | loss: 5.484835 | lr 7.1143e-05 | norm: 3.4524 | dt: 95.37ms | tok/sec: 85899.99\n","start training the model\n","step 0 of training\n","step    83 | loss: 6.156672 | lr 7.2000e-05 | norm: 2.9921 | dt: 95.88ms | tok/sec: 85441.59\n","start training the model\n","step 0 of training\n","step    84 | loss: 5.938152 | lr 7.2857e-05 | norm: 3.0797 | dt: 94.75ms | tok/sec: 86460.04\n","start training the model\n","step 0 of training\n","step    85 | loss: 5.868031 | lr 7.3714e-05 | norm: 3.1328 | dt: 94.64ms | tok/sec: 86561.11\n","start training the model\n","step 0 of training\n","step    86 | loss: 5.797818 | lr 7.4571e-05 | norm: 3.1331 | dt: 95.58ms | tok/sec: 85709.50\n","start training the model\n","step 0 of training\n","step    87 | loss: 5.224742 | lr 7.5429e-05 | norm: 3.5055 | dt: 96.02ms | tok/sec: 85319.81\n","start training the model\n","step 0 of training\n","step    88 | loss: 4.452028 | lr 7.6286e-05 | norm: 4.1581 | dt: 97.23ms | tok/sec: 84258.00\n","start training the model\n","step 0 of training\n","step    89 | loss: 4.451362 | lr 7.7143e-05 | norm: 4.0634 | dt: 95.79ms | tok/sec: 85524.32\n","start training the model\n","step 0 of training\n","step    90 | loss: 4.399499 | lr 7.8000e-05 | norm: 4.0278 | dt: 96.40ms | tok/sec: 84980.91\n","start training the model\n","step 0 of training\n","step    91 | loss: 4.428303 | lr 7.8857e-05 | norm: 4.1433 | dt: 95.78ms | tok/sec: 85528.79\n","start training the model\n","step 0 of training\n","step    92 | loss: 4.360065 | lr 7.9714e-05 | norm: 3.9906 | dt: 96.06ms | tok/sec: 85281.06\n","start training the model\n","step 0 of training\n","step    93 | loss: 4.414047 | lr 8.0571e-05 | norm: 3.8905 | dt: 96.39ms | tok/sec: 84991.85\n","start training the model\n","step 0 of training\n","step    94 | loss: 5.386795 | lr 8.1429e-05 | norm: 3.4216 | dt: 96.05ms | tok/sec: 85292.49\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-91f64adaeb53>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaster_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{step} train {loss_accum.item():.6f}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","torch.set_float32_matmul_precision('high') # this  is for gpu tunning\n","\n","# create model\n","model = GPT(GPTConfig(vocab_size=50304))\n","# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n","model.to(device)\n","use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n","if use_compile:\n","    model = torch.compile(model)\n","if ddp:\n","    model = DDP(model, device_ids=[ddp_local_rank])\n","raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n","\n","max_lr = 6e-4\n","min_lr = max_lr * 0.1\n","warmup_steps = 700\n","max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n","def get_lr(it):\n","    # 1) linear warmup for warmup_iters steps\n","    if it < warmup_steps:\n","        return max_lr * (it+1) / warmup_steps\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > max_steps:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n","    return min_lr + coeff * (max_lr - min_lr)\n","\n","# optimize!\n","optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n","\n","# create the log directory we will write checkpoints to and log to\n","log_dir = data_root + \"/log\"\n","os.makedirs(log_dir, exist_ok=True)\n","log_file = os.path.join(log_dir, f\"log.txt\")\n","with open(log_file, \"w\") as f: # open for writing to clear the file\n","    pass\n","output_file = data_root + \"/output.txt\"\n","for step in range(max_steps):\n","    t0 = time.time()\n","    last_step = (step == max_steps - 1)\n","\n","    # once in a while evaluate our validation loss\n","    if step % 250 == 0 or last_step:\n","        model.eval() # we want to use model. we dont want to train it.\n","        val_loader.reset()\n","        with torch.no_grad():\n","            val_loss_accum = 0.0\n","            val_loss_steps = 20\n","            for _ in range(val_loss_steps):\n","                x, y = val_loader.next_batch()\n","                x, y = x.to(device), y.to(device)\n","                with torch.autocast(device_type=device_type, dtype=torch.bfloat16): # do something with ram to make it efficient and sp\n","                    logits, loss = model(x, y)\n","                loss = loss / val_loss_steps\n","                val_loss_accum += loss.detach()\n","        if ddp:\n","            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n","        if master_process:\n","            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n","            with open(log_file, \"a\") as f:\n","                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n","            if step > 0 and (step % 5000 == 0 or last_step):\n","                # optionally write model checkpoints\n","                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'config': raw_model.config,\n","                    'step': step,\n","                    'val_loss': val_loss_accum.item()\n","                }\n","                # you might also want to add optimizer.state_dict() and\n","                # rng seeds etc., if you wanted to more exactly resume training\n","                torch.save(checkpoint, checkpoint_path)\n","\n","    # once in a while evaluate hellaswag\n","    # if (step % 250 == 0 or last_step) and (not use_compile):\n","    #     num_correct_norm = 0\n","    #     num_total = 0 # ToDo i commented the following for loop since it relates to Hellasweg\n","    #     # for i, example in enumerate(iterate_examples(\"val\")):\n","    #     #     # only process examples where i % ddp_world_size == ddp_rank\n","    #     #     if i % ddp_world_size != ddp_rank:\n","    #     #         continue\n","    #     #     # render the example into tokens and labels\n","    #     #     _, tokens, mask, label = render_example(example)\n","    #     #     tokens = tokens.to(device)\n","    #     #     mask = mask.to(device)\n","    #     #     # get the logits\n","    #     #     with torch.no_grad():\n","    #     #         with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n","    #     #             logits, loss = model(tokens)\n","    #     #         pred_norm = get_most_likely_row(tokens, mask, logits)\n","    #     #     num_total += 1\n","    #     #     num_correct_norm += int(pred_norm == label)\n","    #     # reduce the stats across all processes\n","    #     if ddp:\n","    #         num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n","    #         num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n","    #         dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n","    #         dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n","    #         num_total = num_total.item()\n","    #         print('num_total', num_total)\n","    #         num_correct_norm = num_correct_norm.item()\n","    #         print('num_correct_norm', num_correct_norm)\n","    #     print(\"ddp not available\")\n","    #     acc_norm = num_correct_norm / num_total #if num_total != 0 else 0\n","    #     if master_process:\n","    #         print(f\"Dataset accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n","    #         with open(log_file, \"a\") as f:\n","    #             f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n","\n","    # once in a while generate from the model (except step 0, which is noise)\n","    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n","        phrase = \"دردم از یار است و درمان نیز هم\"\n","        generate_poem(model, device, device_type, ddp_rank, phrase, 4)\n","    # do one step of the optimization\n","    print(\"start training the model\")\n","    model.train()\n","    optimizer.zero_grad()\n","    loss_accum = 0.0\n","    for micro_step in range(grad_accum_steps):\n","        print(f\"step {micro_step} of training\")\n","        x, y = train_loader.next_batch()\n","        x, y = x.to(device), y.to(device)\n","        # added after video, this field is also used by the forward pass.\n","        if ddp:\n","            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n","        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n","            logits, loss = model(x, y)\n","        # we have to scale the loss to account for gradient accumulation,\n","        # because the gradients just add on each successive backward().\n","        # addition of gradients corresponds to a SUM in the objective, but\n","        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n","        loss = loss / grad_accum_steps\n","        loss_accum += loss.detach()\n","        loss.backward()\n","    if ddp:\n","        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n","    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    # determine and set the learning rate for this iteration\n","    lr = get_lr(step)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    optimizer.step()\n","    if device_type == \"cuda\":\n","        torch.cuda.synchronize() # wait for the GPU to finish work\n","    t1 = time.time()\n","    dt = t1 - t0 # time difference in seconds\n","    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n","    tokens_per_sec = tokens_processed / dt\n","    if master_process:\n","        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n","        with open(log_file, \"a\") as f:\n","            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n","\n","if ddp:\n","    destroy_process_group()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":12,"status":"error","timestamp":1723580195847,"user":{"displayName":"GenAI","userId":"05659125202882086658"},"user_tz":-210},"id":"QZUvwnNg7wZN","outputId":"0c549692-a372-43d8-d57c-551bd4531560"},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-01cbf5be8ad2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading the Model and give your poems :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/saved_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#replacewith model path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"توانا بود هرکه دانا بود\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerated_poems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddp_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["#Loading the Model and give your poems :)\n","model.load_state_dict(torch.load(data_root + '/saved_model.pth'))  #replacewith model path\n","phrase = \"توانا بود هرکه دانا بود\"\n","generated_poems=generate_poem(model, device, device_type, ddp_rank, phrase, 1)\n","\n","result_file_path = data_root + '/result.txt'\n","with open(result_file_path, 'w', encoding='utf-8') as f:\n","    for poem in generated_poems:\n","        f.write(poem + '\\n')\n","\n","print(f\"Generated poems saved to {result_file_path}\")\n","\n","\n","# Flatten the special tokens into a list\n","tokens_to_remove = [special_tokens[\"cls_token\"], special_tokens[\"eos_token\"]] + special_tokens[\"additional_special_tokens\"]\n","#TODO\n","\n","# Function to remove special tokens from a line\n","def remove_special_tokens(line):\n","    for token in tokens_to_remove:\n","        if token == 'BOM':\n","            line = line.replace(token, \"\\n\")\n","            continue\n","        line = line.replace(token, \"\")\n","    return line\n","\n","# Open the input file for reading and the output file for writing\n","input_file_path = data_root + \"/result.txt\"  # Change this to your input file path\n","output_file_path = data_root + '/' + 'final_output.txt'  # Change this to your desired output file path\n","\n","with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n","    for line in infile:\n","        # Remove special tokens from the current line\n","        cleaned_line = remove_special_tokens(line)\n","        # Write the cleaned line to the output file\n","        print(cleaned_line)\n","        outfile.write(cleaned_line)\n","\n","print(\"Special tokens removed and output saved to\", output_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFpQzTpVw201"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"091f011f45ea49d3a1b99a8d73774300":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0dc66a613b4443ac94c4ec3cd2cd0e85":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"112b5b9127b24720b591c00ec5f40ac2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_87a87380ca1142f49b5c1f05dd9e1723","max":1127358,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a8449369f1042ea9bd8fadfce84fc53","value":1127358}},"143b9eeaacfa4c2d9937983c25d1d914":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94b807f17d1a41b88173be958ed9407c","placeholder":"​","style":"IPY_MODEL_fd3c2b70d2224e91a9a153f92c87014c","value":" 350/350 [00:00&lt;00:00, 28.8kB/s]"}},"1b9bdb0c89f0404a89be212dca956cab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e6441edbdff4ec498fc546cc835c766":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_848840303de7472a99fca57043010b05","max":1331,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5742a3f281148679bf065f6af796156","value":1331}},"1fb1fd94f3f14d4f90df3ccff0634e49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b6b17861c714a579e2a739e00953701","placeholder":"​","style":"IPY_MODEL_7f005153801945bda21cba706486478c","value":"special_tokens_map.json: 100%"}},"24f0759ee6d440d2963e8fa824b4c06a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2572f68c632241ed9ffa2e3cf078b69c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66b1a8eee64142c2be7768e0e495bcd2","IPY_MODEL_259d42a307004923b8b9c6881d0d4589","IPY_MODEL_43122ecf63e64a09ae9387fe21f492bc"],"layout":"IPY_MODEL_4e6a79772d24433ca166a768ea48cef3"}},"259d42a307004923b8b9c6881d0d4589":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db6a04628ccf4261965f85c8264311e3","max":537052,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f12abf8010b4615adeee50809ae5db1","value":537052}},"2752fbd5fa7847f0a925eecde894c638":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8d494aa933b458592b08ca2d58eb4ea","IPY_MODEL_112b5b9127b24720b591c00ec5f40ac2","IPY_MODEL_d0f01efaaa834febb9ab57e98f67eb0a"],"layout":"IPY_MODEL_1b9bdb0c89f0404a89be212dca956cab"}},"314a9b57acb24987bde6634e3d9813fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"345905204ef9408caf934d446fd880fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc2ea1199ef0413ba1f099f722b7022a","placeholder":"​","style":"IPY_MODEL_f8b65c2838c3422ca966b8bdeaa9c9ca","value":" 399/399 [00:00&lt;00:00, 27.9kB/s]"}},"37054404433d4e74903c8e641e29df3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39ec92bf4ddb4aaa9876704dc7d82b21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78d400e5fcd04e5dbc1db33e27b89773","IPY_MODEL_c21a2002a0b942c19ace265187839446","IPY_MODEL_143b9eeaacfa4c2d9937983c25d1d914"],"layout":"IPY_MODEL_748c3ab6f50c4311a4a4a735bd500c29"}},"3ef31c0fadbb4302b89333d26bdd464d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43122ecf63e64a09ae9387fe21f492bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_963f94a5f31e4debb36567d7550abb29","placeholder":"​","style":"IPY_MODEL_37054404433d4e74903c8e641e29df3b","value":" 537k/537k [00:00&lt;00:00, 1.26MB/s]"}},"4e6a79772d24433ca166a768ea48cef3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f12abf8010b4615adeee50809ae5db1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50929f9ad4874450a9e9d45103866742":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69797c101e7c403e94adf4fc62716633","max":399,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ef31c0fadbb4302b89333d26bdd464d","value":399}},"62d656cd521049ac98b939e2fa691496":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66b1a8eee64142c2be7768e0e495bcd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6d03e93718a43c3a205e4de64280e6a","placeholder":"​","style":"IPY_MODEL_314a9b57acb24987bde6634e3d9813fd","value":"spiece.model: 100%"}},"69797c101e7c403e94adf4fc62716633":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a8449369f1042ea9bd8fadfce84fc53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"748c3ab6f50c4311a4a4a735bd500c29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78d400e5fcd04e5dbc1db33e27b89773":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_861f4d22fcc74ab0bdf5bf7a9baa47ee","placeholder":"​","style":"IPY_MODEL_a7013ea169d747afa1502ec9ae1691ee","value":"tokenizer_config.json: 100%"}},"7f005153801945bda21cba706486478c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"848840303de7472a99fca57043010b05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"861f4d22fcc74ab0bdf5bf7a9baa47ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87a87380ca1142f49b5c1f05dd9e1723":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b6b17861c714a579e2a739e00953701":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94b807f17d1a41b88173be958ed9407c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"963f94a5f31e4debb36567d7550abb29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a13180afe7fb45ef93f4aabed7cc01c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1fb1fd94f3f14d4f90df3ccff0634e49","IPY_MODEL_50929f9ad4874450a9e9d45103866742","IPY_MODEL_345905204ef9408caf934d446fd880fd"],"layout":"IPY_MODEL_f616126382fc40848f72887b1250852c"}},"a5742a3f281148679bf065f6af796156":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6d03e93718a43c3a205e4de64280e6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7013ea169d747afa1502ec9ae1691ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7ebc8cded0c4845b8524be11613f6d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d494aa933b458592b08ca2d58eb4ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24f0759ee6d440d2963e8fa824b4c06a","placeholder":"​","style":"IPY_MODEL_f5d6188818d24770ab57a2800d3c8267","value":"tokenizer.json: 100%"}},"b715226a8e74438ab57ab420a68032f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbb726e184254d628185851b1b005586","IPY_MODEL_1e6441edbdff4ec498fc546cc835c766","IPY_MODEL_da640495e7ff4d1ab86569e03c4bbdb5"],"layout":"IPY_MODEL_0dc66a613b4443ac94c4ec3cd2cd0e85"}},"bc3753cb8ae44ab8806e8965110f227b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c21a2002a0b942c19ace265187839446":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7ebc8cded0c4845b8524be11613f6d9","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_091f011f45ea49d3a1b99a8d73774300","value":350}},"c354236f0895488e9ac33c3051104251":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caa2f51656804341884c1a7145745348":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbb726e184254d628185851b1b005586":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c354236f0895488e9ac33c3051104251","placeholder":"​","style":"IPY_MODEL_62d656cd521049ac98b939e2fa691496","value":"config.json: 100%"}},"d0f01efaaa834febb9ab57e98f67eb0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_caa2f51656804341884c1a7145745348","placeholder":"​","style":"IPY_MODEL_da19ee19a49c4f2dba0e9b247c028093","value":" 1.13M/1.13M [00:00&lt;00:00, 1.73MB/s]"}},"d9750b9b4815455496cb3fb5b7e64234":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da19ee19a49c4f2dba0e9b247c028093":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da640495e7ff4d1ab86569e03c4bbdb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc3753cb8ae44ab8806e8965110f227b","placeholder":"​","style":"IPY_MODEL_d9750b9b4815455496cb3fb5b7e64234","value":" 1.33k/1.33k [00:00&lt;00:00, 105kB/s]"}},"db6a04628ccf4261965f85c8264311e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5d6188818d24770ab57a2800d3c8267":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f616126382fc40848f72887b1250852c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8b65c2838c3422ca966b8bdeaa9c9ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc2ea1199ef0413ba1f099f722b7022a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd3c2b70d2224e91a9a153f92c87014c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
